#! /bin/sh

##
# Copyright © 2021 Splunk Inc.
# SPLUNK CONFIDENTIAL – Use or disclosure of this material in whole or in part
# without a valid written license from Splunk Inc. is PROHIBITED
##

VERSion="21.06.014"

#	uba_health_check.sh
#
#	This is a never ending work in progress to capture metrics and interpret UBA health

WHOIAM=`whoami 2>/dev/null`

case "$WHOIAM" in
        caspida);;
        *)      echo "script must be run by user 'caspida'"
                exit;;
esac

CASPIDA_HOME=~caspida/

JAVA_HOME=`echo '. /opt/caspida/bin/CaspidaCommonEnv.sh >/dev/null 2>&1; echo $JAVA_HOME' | bash `

CLASSPATH=/opt/caspida/lib/CaspidaSecurity.jar

DOCKERcidr="172.17.0.0/16"

export SPLUNK_HOME=/opt/splunk

export JAVA_HOME CLASSPATH

EMAILto=${1-noBody}

PROGname="`basename $0`"
PROGdir="`dirname $0`"
PROGpwd="`pwd`"
BASEname="`basename $PROGname .sh`"

HOSTname=`hostname -f 2>/dev/null`
if test "$HOSTname" = ""
then
	HOSTname=`hostnamectl --static 2>/dev/null`
fi

case "$PROGdir" in
	.) PROGdir="$PROGpwd";;			# must be absolute
	.*) PROGdir="$PROGpwd/$PROGdir";;	# must be absolute
esac

GETmd5=false
GETfile=false
GOTfile=false

IOPSmin=720

JEpoch="-610192560"
UBAepoch="1436400000"

NOW=`date '+%s'`
TIMEnow=`date "--date=@$NOW" "+%F_%R:%S"`
OUTnow=`date "--date=@$NOW" '+%y%m%d_%H%M'`

TODAY=`date "--date=@$NOW" '+%Y-%m-%d'`
DayOfSeconds=`expr 3600 \* 24`

TOMORROW=`expr $NOW + $DayOfSeconds`
YESTERDAY=`expr $NOW - $DayOfSeconds`
DAYBEFORE=`expr $YESTERDAY - $DayOfSeconds`

TODAYmmmdd=`date "--date=@$NOW" '+%b %d'`
YESTERDAYmmmdd=`date "--date=@$YESTERDAY" '+%b %d'`
DAYBEFOREmmmdd=`date "--date=@$DAYBEFORE" '+%b %d'`

TODAYmmm_d=`date "--date=@$NOW" '+%b %e'`
YESTERDAYmmm_d=`date "--date=@$YESTERDAY" '+%b %e'`
DAYBEFOREmmm_d=`date "--date=@$DAYBEFORE" '+%b %e'`

TOMORROW=`date "--date=@$TOMORROW" '+%Y-%m-%d'`
YESTERDAY=`date "--date=@$YESTERDAY" '+%Y-%m-%d'`
DAYBEFORE=`date "--date=@$DAYBEFORE" '+%Y-%m-%d'`

export LEADERnode=false
export MEMBERnode=false
export MEMBERlist=""

CASPIDA_PROPERTIES_EXIST=false				# not 'true', until 'setup' has been run

RHELrelease=false                # limits.conf is RHEL/CentOS requirement
RHELrelease72=false

SENDemail=false
EMAILrecipients=""

CASPIDAdir=/opt/caspida
ZOOKEEPERdir=/var/lib/zookeeper

reqDISKs=2		# 2 disks are required
minDISK1r=50		# at least 50 GB
minDISK2r=500		# at least 500 GB
minDISK1f=45		# GB about the formatted size of a 50 GB drive
minDISK2f=488		# GB about the formatted size of a 500 GB drive
minSPEED=1000		# Mb/s
minAVAILdisk1=20	# GB space available for /opt/caspida
minAVAILdisk2=125	# GB space available for /var/vcap

PARTvcap=""

SIGNIFICANTdifference=20	

SAMPLElimit=3

INDENT="                              "		# 30 spaces
#      "123456789012345678901234567890"

BOUNDARY="UBA_/Splunk_a1b2c3d4e5.94303caspida"


uba32version="3.2"			# new features with dubai
uba32feature=false

uba321version="3.2.1"			# new features with dubai
uba321feature=false

uba33version="3.3"			# new features with edinburgh
uba33feature=false

uba34version="3.4"			# new features with florence
uba34feature=false

uba40version="4.0"			# new features with florence
uba40feature=false

uba4001version="4.0.0.1"			# new features with florence
uba4001feature=false

uba4002version="4.0.0.2"			# new features with florence
uba4002feature=false

uba41version="4.1"			# new features with geneva
uba41feature=false

uba411version="4.1.1"			# new features with geneva
uba411feature=false

uba412version="4.1.2"			# new features with geneva
uba412feature=false

uba413version="4.1.3"			# new features with geneva
uba413feature=false

uba42version="4.2"			# new features with hong kong
uba42feature=false

uba421version="4.2.1"			# new features with hong kong
uba421feature=false

uba422version="4.2.2"			# new features with hong kong
uba422feature=false

uba423version="4.2.3"			# new features with hong kong
uba423feature=false

uba43version="4.3"			# new features with indianapolis
uba43feature=false

uba431version="4.3.1"			# new features with indianapolis
uba431feature=false

uba4311version="4.3.1.1"		# new features with indianapolis
uba4311feature=false

uba432version="4.3.2"			# new features with indianapolis
uba432feature=false

uba433version="4.3.3"			# new features with indianapolis
uba433feature=false

uba434version="4.3.4"			# new features with indianapolis
uba434feature=false

uba50version="5.0"			# new features with jakarta
uba50feature=false

uba501version="5.0.1"                   # new features with jakarta
uba501feature=false

uba502version="5.0.2"                   # new features with jakarta
uba502feature=false

uba503version="5.0.3"                   # new features with jakarta
uba503feature=false

uba504version="5.0.4"                   # new features with jakarta
uba504feature=false

uba5041version="5.0.4.1"                   # new features with jakarta
uba5041feature=false

uba505version="5.0.5"                   # new features with jakarta
uba505feature=false

uba506version="5.0.6"                   # new features with jakarta
uba506feature=false

uba51version="5.1"                       # new features with key west
uba51feature=false

EXPECTEDparsers=0

TMP=/var/vcap/sys/tmp     # UBA tmp
if touch $TMP/uhc >/dev/null 2>&1
then
	rm $TMP/uhc
else
	TMP="/tmp"
fi

TMP1file=$TMP/uhc.check1.$$
TMP2file=$TMP/uhc.check2.$$
TMP3file=$TMP/uhc.check3.$$
TMP4file=$TMP/uhc.check4.$$
TMP5file=$TMP/uhc.check5.$$
MAXETLfile=$TMP/uhc.maxetl.$$
MAXEPSfile=$TMP/uhc.maxeps.$$
EXCref=$TMP/uhc.except_ref.$$
EXCfile=$TMP/uhc.except_count.$$
EXLfile=$TMP/uhc.except_list.$$
OBLfile=$TMP/uhc.observation_list.$$
SUMMfile="$CASPIDA_HOME/${BASEname}_summary.out"
MAILfile="$CASPIDA_HOME/${BASEname}_mail.txt"
AWKfile=$TMP/uhc.awk.$$
OUTfile=$TMP/uhc.output.$$
LICENSEfile=$TMP/uhc.license.$$
BGjobs=$TMP/uhc.jobs.$$
LOADfile=$TMP/uhc.load.$$
IOPSfile_vcap2=/var/vcap2/sys/tmp/uhc.iops.data
IOPSfile_vcap=/var/vcap/sys/tmp/uhc.iops.data
IOPSfile_tmp=/tmp/uhc.iops.data
BLOATviews=$TMP/uhc.bloat.$$.sql
SEQUENTIALsubnets=$TMP/uhc.seq_sub.$$
KAFKAingest=$TMP/uhc.kafka.$$
CONTRIBlines="$TMP/uhc.contrib.$$"
INTPUBfile=$TMP/uhc.intpub.$$
INTPUBawk=$TMP/uhc.intpub.awk.$$

KAFKAdelay=$TMP/uhc.delay.awk.$$

RESTawk=$TMP/uhc.rest.awk.$$
RESTapps=$TMP/uhc.rest.apps.$$

SNODEfile=$TMP/uhc.super.$$
SUPERmem=96000
SUPERcpu=56

HEADERfile=$TMP/uhc.$$.header
BODYfile=$TMP/uhc.$$.body
ATTACHfile=$TMP/uhc.$$.attachment

CPUTEST="$CASPIDA_HOME/bin/cputest"
DSH="$CASPIDA_HOME/bin/dsh"
DSINFO="$CASPIDA_HOME/uhc.$$.dsinfo"
IDINFO="$CASPIDA_HOME/uhc.$$.idinfo"
RSH="$CASPIDA_HOME/bin/rsh"
REDIScli="$CASPIDA_HOME/uhc.$$.redis_cli"

PSQLfile=$TMP/uhc.psql.$$

MEMBERout=$TMP/uhc.nodeout.$$_
MEMBERwait=$TMP/uhc.wait.$$

UBAfile=$TMP/uhc.uba_release.$$

REPLtype=$TMP/uhc.repl.$$
SWAPawk=$TMP/uhc.swapawk.$$

PYscript=$TMP/uhc.py.$$
PYsmtp="$CASPIDA_HOME/uhc.smtp.txt"

OCout=$TMP/uhc.oc.$$
SEARCHmessages=$TMP/uhc.sm.$$

ENDpoints=$TMP/uhc.ep.$$

RHEL_OS_file=$TMP/uhc.rhel.$$ 

FWfile="$CASPIDA_HOME/uhc.fw.txt"

trap "rm -f $FWfile $RHEL_OS_file $KAFKAdelay $ENDpoints $RESTawk $RESTapps $PSQLfile $SEARCHmessages $OCout $PYsmtp $PYscript $INTPUBfile $INTPUBawk $CONTRIBlines $SWAPawk $REPLtype $LOADfile $TMP1file ${TMP1file}e $TMP2file ${TMP2file}e $TMP3file $TMP4file ${TMP4file}e  $TMP5file $LICENSEfile $SNODEfile ${MEMBERout}* $MEMBERwait $AWKfile $BLOATviews $MAXETLfile $MAXEPSfile $EXCfile $EXCref $EXLfile $OBLfile $OUTfile $BGjobs $HEADERfile $BODYfile $ATTACHfile $IOPSfile_tmp $IOPSfile_vcap $IOPSfile_vcap2 $REDIScli $DSINFO $IDINFO $SEQUENTIALsubnets $UBAfile $KAFKAingest; exit" 0 2 15		# clean up temporary files

echo 0 > $EXCfile		# store count of exceptions

# rm -f $CPUTEST >/dev/null 2>&1
if ! test -x $CPUTEST >/dev/null 2>&1
then
base64 -d > $CPUTEST 2>/dev/null <<EndOfData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EndOfData
chmod 500 $CPUTEST 2>/dev/null 
fi

# rm -f $DSH >/dev/null 2>&1
if ! test -x $DSH >/dev/null 2>&1
then
base64 -d > $DSH  2>/dev/null <<EndOfData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EndOfData
chmod 500 $DSH 2>/dev/null 
fi

# rm -f $RSH >/dev/null 2>&1
if ! test -x $RSH >/dev/null 2>&1
then
base64 -d > $RSH 2>/dev/null  <<EndOfData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EndOfData
chmod 500 $RSH 2>/dev/null 
fi

cat > $DSINFO 2>/dev/null  <<EndOfData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EndOfData
chmod 400 $DSINFO 2>/dev/null 

cat > $IDINFO 2>/dev/null  <<EndOfData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EndOfData
chmod 400 $IDINFO 2>/dev/null 

cat > $REDIScli  2>/dev/null <<EndOfData
U2FsdGVkX1+KFzeoNqWaowl56nmtCGlHbl+ZODxy/Qxj/cYp8eZEfbLgbfv9O5Df
2M0n4ZPec0o1X48XqLy7OQ7m5HyLlNIFqOS+Tac3G0o6z+2KSxkZ70o8yFtO3dUv
n/PmMp3jqpyz/OWD7ENLVsJ2rsWXEnSIkJUjJs6mhGozodkNAsF0u3GeBQxtF4gC
7ASFXTCdmFFy2HRsQyd8l4o91B3OoiD5FZitxQfSXYmeaDm5tskx46qTdiLTnGqL
y+NfjUr8uN4ptRxXV+4Z4BZJhR/jW3zIja5pU9BzdWVjMQVjqm3QpJ6UR7gpYnHR
Dg0PopColCinS+m91eXWIPMI0p1tdubT9O/H3eb+Ccsd7fXkE9rMU0PchjCW+9D3
wbhM47Yudl3qS57TimP5Q+YdHLLHspJGpnw4cEfofpbLl8GXhfCK8o6GJEpOkgCD
inJg+nF3VFdnwjDiSoltijOLrhQZYXJY3qO1+y1YeG3c0Zayu86SRMrxXBO7cUsD
e1CBkdpUwu71byuZutDfPSPC7IEbygOzeKKrrkB+6VmsFqukoE+HKnH4pNnR0EfS
qT8/cilkWWC/xMj6aEB7l5S2ZNp5IpSTJWOfSTzTQwc+Om5EjUpZHfLrxFntGAFi
5OUCu92OSAwiMJwFq3w8iUlwBIvGAdx39jlccYH8fIZRKS0CfFjhAGUMrRqbYCUY
IfBPk3hVbp3iUELv3wIK5dp/JwOTPbY2yIoajJsPsgAeeI1VB3SMzYXLE4Qsaz1n
fqseN3sO4vBM2IjOO3Sq8t4Bi+C8MAFFOttZdBCEMPb/GZptney6LCGggXiQuYbT
iWeuyh27COFrtvgwSPbCXcbuDdPkqZu3+JISiRcNqUvCBpDPpsbU+rqYkHAHBQSX
h+gKeMXzYmLuVEcQOSYuuZv/vlm8FfQdVxSzTv/reNY364NxKRpGq9fTHvTeIqZ0
9jF9INBxCeFYc62W+yZs1DeZ5W6wAQRJKUcdeYZMIp+iwIgH9sffUdytAtBYvOmC
21FrjdsNYqJCchEeMRIq8v9ynYlQx7zYuiTm0bACTqPdxgfORtT/n30rn31FrPlB
Hj+OzCABOaMekLmirRxLwYRXN8lKu1y08X9MTdZGZYYKvSM53kzK1cNd+Xqm85GG
GQvS76RSwAjDvXskAMYW3404RdZGQl0BOg5mXjNt9h9PgVntEyqmtpw+7dBSFHYA
Y9YV/VvjIUFsO+cUX3hnYG19oiHn4aDjAjDnwEiyq53RcSEPQm3rDUac/E3ZXuyu
PPmILAtelMHj41bZx7eV/fRxssajvKbpaCruGEPf+GLUM6lmZ8e/xM7jAwOYFynh
Lq7d0ZcQmKUvKt5rG+SDuaEL5ns3o6zY6FFs0xWQvktWjLrf1M6siqGnVgJ58sRM
HNVgR3/GQAEO+XrKVulJgb54b76jtrQME7bU9mmIttd7ZyVr9KLCGrrV4/UIf290
d/2+ihvuIZD09yfws4vZ/Wb4fF0LxKEllkuJk6+nCp+szOiqQyo7LkrvgkL7gCF3
xgVamIquDywXWIxpQrFqfVKIMu7CpjrPQ9rKIzZhqlKx14k3YvSbsESTMh9XUpay
zSA6GDBG+uUAR4rPSrMW8qOlNG5yx9VuSkgcTEUidOQAP3saXM2owyCHwycAnlLb
2h10xFrVPfNDs+gNzay9Q29yw9Bf5lNvVbjmCFwQ3vMt2ko8CW4O1W8pHA9eLS72
gKYSkg2uQbLJqb88GOCSPhCavfsb2L1DbW7yXG0Z57w=
EndOfData
chmod 400 $REDIScli 2>/dev/null 

RESTsearch="| rest /services/apps/local \
| where in(title, \"SA-UEBA\", \"Splunk_UBA_Monitor\", \"Splunk-UBA-SA-Kafka\", \"Splunk_TA_ueba\")  \
| dedup title version \
| sort title version build \
| table title version build disabled"

cat > $RESTawk <<\EndOfData
BEGIN {
        app_value["title"]=""
        app_value["version"]=""
        app_value["build"]=""
        app_value["disabled"]=""

        printf("+ %-35.35s + %-20.20s + %7.7s + %8.8s + %8.8s\n", "endpoint", "title", "version", "build", "disabled" )
}

/<\/field>/ {
        next
}

/field k=/ {
        split($0, keys, "'")
        app_key=keys[2]
        next
}

/<value/ {
        split($0, tags, ">")
        split(tags[3], tag, "<")
        app_value[app_key]=tag[1]
        next
}

/<\/result>/ {
        printf("  %-35.35s | %-20.20s | %7.7s | %8.8s | %8.8s\n", ENDpoint, app_value["title"], app_value["version"], app_value["build"], app_value["disabled"])
        app_value["title"]=""
        app_value["version"]=""
        app_value["build"]=""
        app_value["disabled"]=""
        next
}
EndOfData
export RESTawk RESTsearch RESTapps

cat > $KAFKAdelay <<\EndOfData
BEGIN {
	checkTime=0
        minDelay=1000
	maxDelay=0
	logEntry=""
}

/.*KafkaUnbufferedSink: Posted.*/ {
	if (checkTime == 1) {
		split($0,statCounts, "(")
		split(statCounts[4], statCount, ")")
		if (statCount[1] > maxDelay) {
			maxDelay=statCount[1]
			logEntry=$0
		}
	} else
		checkTime=1
	fi

	next
}

{
	checkTime=0
}

END {
	WARNing=""
	if (maxDelay > 0) {
		if (maxDelay > minDelay)
			WARNing="<= longer than expected; check disk and Kafka performance"
		printf("%s   %s\n", logEntry, WARNing)
	}
}
EndOfData

cat > $INTPUBawk <<\EndOfData
function write_when(num) {
     	i=0	
	printf ("WHEN ( ");
	while (i < num) {
		printf (" networkid::inet << inet '%s' ", cidrs[i]) 	
		i++
		if (i < num)
			printf " OR "
	}
	printf " ) THEN 'intpub'   \n";
	count=0
}

BEGIN {
	split("", cidrs);
	count=0;
}

count==5 {
	write_when(count)
}

{
	cidrs[count++]=$0
}

END {
	if (count != 0)
		write_when(count) ;
	exit;
}
EndOfData

cat >$SWAPawk <<\EndOfData
BEGIN {
	VMSWAP="";
}

{
	gsub(/\s\s*/, " ");
}

/^VmSwap:/ {
	if ($2 < 512 ) {
		next;
	}
	VMSWAP=$0
	next;
}
/^Uid:/ { UID=$0; next; }
/^Name:/ { NAME=$0; next; }
/^Pid:/ { PID=$0; next; }

END {
	if (VMSWAP != "")
		printf "%-10.10s | %-29.29s | %-18.18s | %s\n", PID, UID, VMSWAP, NAME;
}
EndOfData

cat >$AWKfile <<\EndOfData
BEGIN {
	FS=":"
	displayName=""
	printLine("displayName", "type", "enabled")
	dashes="------------------------------------------------------------------------------------------------"
	printf "%-57.57s-+-%-9.9s-+-%-6.6s\n", dashes, dashes, dashes
} 
function printLine(f1,f2,f3) {
	if (f1 != "")
		printf "%-57.57s | %-9.9s | %-6.6s\n", f1, f2, f3
}
/displayName/ {
		printLine(displayName, type, enabled)
		displayName=$2
		next
} 
/type/ {
		type=$2
		next
} 
/enabled/ {
		enabled=$2
		next
} 
END {
	printLine(displayName, type, enabled)
}
EndOfData

cat > $PYscript <<\EndOfData
import shlex, subprocess
import sys

endPoint=sys.argv[1]

cmd = '''curl --connect-timeout 15 -k ''' + endPoint + '/'

args = shlex.split(cmd)
process = subprocess.Popen(args, shell=False, universal_newlines=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

stdout, stderr = process.communicate()

genString = stdout.partition("<generator ")[2].partition("/>")[0]
print (genString)
EndOfData
export PYscript

cat > $PYsmtp <<EndOfData
import shlex, subprocess
import smtplib
import sys

if len(sys.argv) != 4 :
    print("missing args")
    sys.exit(1)

hostName=sys.argv[1]

mailHost=sys.argv[2]
try:
    mailPort=int(sys.argv[3])
except:
    mailPort=25

print ("%s, %s:%d - " % (hostName, mailHost, mailPort),  end='')

if mailPort == 465 :
    try:
        smtp=smtplib.SMTP_SSL(mailHost, mailPort, timeout=10)
        print("ssl connection successful")
    except:
        print("ssl connection failed")
        sys.exit(2)
else:
    try:
        smtp=smtplib.SMTP(mailHost, mailPort, timeout=10)
        print("connection successful")
    except:
        print("connection failed")
        sys.exit(3)

try:
    smtp.ehlo(name=hostName)
    esmtp=smtp.does_esmtp
    resp=smtp.ehlo_resp
    respString=resp.decode()
    print(respString.splitlines()[0])
    auth=smtp.has_extn('AUTH')
    vrfy=smtp.has_extn('VRFY')
    tls=smtp.has_extn('STARTTLS')
    if vrfy :
        print("vrfy")
    if tls :
        print ("%s: %s" % ('starttls', smtp.esmtp_features.get('starttls', 'not an option')))
    if auth and tls:
        print ("%s: %s" % ('auth w/tls', smtp.esmtp_features.get('auth', 'not an option')))
    if tls and not auth:
        smtp.starttls()
        smtp.ehlo(name=hostName)
        vrfy=smtp.has_extn('VRFY')
        if vrfy :
            print("vrfy")
    print ("%s: %s" % ('auth', smtp.esmtp_features.get('auth', 'not an option')))
    smtp.quit()
except:
    print("handshake failed")
    sys.exit(99)
EndOfData

cat > $BLOATviews 2>/dev/null  <<EndOfData
------------------------------------------------------------------------------------------------------------------------------
-- Following view provides storage size and bloating factor of the tables                                                   --
-- and indexes. Which are summarized by v_tables_health view for further                                                    --
-- use in v_pg_stats                                                                                                        --
------------------------------------------------------------------------------------------------------------------------------
CREATE VIEW v_storage_stats 
  AS SELECT
    current_database(), schemaname, tablename, /*reltuples::bigint, relpages::bigint, otta,*/
    ROUND((CASE WHEN otta=0 THEN 0.0 ELSE sml.relpages::FLOAT/otta END)::NUMERIC,1) AS tbloat,
    CASE WHEN relpages < otta THEN 0 ELSE bs*(sml.relpages-otta)::BIGINT END AS wastedbytes,
    iname, /*ituples::bigint, ipages::bigint, iotta,*/
    ROUND((CASE WHEN iotta=0 OR ipages=0 THEN 0.0 ELSE ipages::FLOAT/iotta END)::NUMERIC,1) AS ibloat,
    CASE WHEN ipages < iotta THEN 0 ELSE bs*(ipages-iotta) END AS wastedibytes
  FROM (
    SELECT
      schemaname, tablename, cc.reltuples, cc.relpages, bs,
      CEIL((cc.reltuples*((datahdr+ma-
        (CASE WHEN datahdr%ma=0 THEN ma ELSE datahdr%ma END))+nullhdr2+4))/(bs-20::FLOAT)) AS otta,
      COALESCE(c2.relname,'?') AS iname, COALESCE(c2.reltuples,0) AS ituples, COALESCE(c2.relpages,0) AS ipages,
      COALESCE(CEIL((c2.reltuples*(datahdr-12))/(bs-20::FLOAT)),0) AS iotta -- very rough approximation, assumes all cols
    FROM (
      SELECT
        ma,bs,schemaname,tablename,
        (datawidth+(hdr+ma-(CASE WHEN hdr%ma=0 THEN ma ELSE hdr%ma END)))::NUMERIC AS datahdr,
        (maxfracsum*(nullhdr+ma-(CASE WHEN nullhdr%ma=0 THEN ma ELSE nullhdr%ma END))) AS nullhdr2
      FROM (
        SELECT
          schemaname, tablename, hdr, ma, bs,
          SUM((1-null_frac)*avg_width) AS datawidth,
          MAX(null_frac) AS maxfracsum,
          hdr+(
            SELECT 1+COUNT(*)/8
            FROM pg_stats s2
            WHERE null_frac<>0 AND s2.schemaname = s.schemaname AND s2.tablename = s.tablename
          ) AS nullhdr
        FROM pg_stats s, (
          SELECT
            (SELECT current_setting('block_size')::NUMERIC) AS bs,
            CASE WHEN SUBSTRING(v,12,3) IN ('8.0','8.1','8.2') THEN 27 ELSE 23 END AS hdr,
            CASE WHEN v ~ 'mingw32' THEN 8 ELSE 4 END AS ma
          FROM (SELECT version() AS v) AS foo
        ) AS constants
        GROUP BY 1,2,3,4,5
      ) AS foo
    ) AS rs
    JOIN pg_class cc ON cc.relname = rs.tablename
    JOIN pg_namespace nn ON cc.relnamespace = nn.oid AND nn.nspname = rs.schemaname AND nn.nspname <> 'information_schema'
    LEFT JOIN pg_index i ON indrelid = cc.oid
    LEFT JOIN pg_class c2 ON c2.oid = i.indexrelid
  ) AS sml
  ORDER BY wastedbytes DESC;

------------------------------------------------------------------------------------------------------------------------------
-- Summary view derived from v_storage_stats, that contains total wasted bytes and bloat factor                             --
------------------------------------------------------------------------------------------------------------------------------
CREATE VIEW v_tables_health 
  AS SELECT 
    tablename,round(avg(wastedbytes) + sum(wastedibytes))::bigint AS wastebytes,
    ROUND( (round(avg(tbloat),1) + round(avg(ibloat),1))/2,1) AS bloatfactor  
  FROM   v_storage_stats 
  GROUP BY 1 ;

------------------------------------------------------------------------------------------------------------------------------
-- View v_pg_stats used by UI to display tables statistics                                                                  --
------------------------------------------------------------------------------------------------------------------------------
CREATE VIEW v_pg_stats_bloat AS 
  SELECT table_name, row_count,rows_updated,rows_deleted,
    pg_size_pretty(table_bytes) AS table_size,
    pg_size_pretty(index_bytes) AS index_size,
    pg_size_pretty(total_bytes) AS total_size,
    pg_size_pretty(coalesce(b.wastebytes,0)) AS wastebytes, 
    coalesce(b.bloatfactor,0) as bloatfactor,
    coalesce(b.wastebytes,0) > 1073741824 AND coalesce(bloatfactor,0) > 1  AS rebuild_required 
  FROM (
    SELECT *, total_bytes-index_bytes-COALESCE(toast_bytes,0) AS table_bytes 
    FROM (
      SELECT 
        c.oid,nspname AS table_schema, c.relname AS TABLE_NAME,
        c.reltuples AS row_count,
        d.n_tup_upd AS rows_updated,
        d.n_tup_del AS rows_deleted,
        pg_total_relation_size(c.oid) AS total_bytes,
        pg_indexes_size(c.oid) AS index_bytes,
        pg_total_relation_size(reltoastrelid) AS toast_bytes
      FROM pg_class c
        LEFT JOIN pg_namespace n ON n.oid = c.relnamespace
        LEFT JOIN pg_stat_user_tables d ON c.relname = d.relname
      WHERE relkind = 'r' AND nspname = 'public'
    ) a
  ) a LEFT JOIN v_tables_health b ON a.table_name = b.tablename 
  ORDER BY total_bytes desc;
EndOfData

#
#	get UBA version info
#

UBA_VERSION () {

	UBA_DEFAULT_PROPERTIES=$CASPIDAdir/conf/caspida-default.properties	# defaults
	UBA_ENV_PROPERTIES="$CASPIDAdir/conf/caspida-site.properties"		# blueprint deployment in 2.4
	UBA_SITE_PROPERTIES=$CASPIDAdir/conf/caspida-site.properties		# customization
	UBA_PROPERTIES="$UBA_DEFAULT_PROPERTIES $UBA_SITE_PROPERTIES"

        if test -f $CASPIDAdir/conf/version.properties
        then
                echo
                UBAversion=`grep release-version $CASPIDAdir/conf/version.properties | cut -d "=" -f 2`
		echo "$UBAversion" > $UBAfile
                reqDB="`echo $UBAversion | sed -n 's/^\([0-9\.]*\)-[0-9]*\.*$/\1/p'`"
                reqDB="`echo $reqDB | sed -e 's/\.0$//'`"
        
		UBAvers=`echo $UBAversion | sed -e 's/\-.*$//' -e 's/\.0$//'`		# remove for version checking
		if test -n "$UBAvers"
		then
			case $UBAvers in
				$uba32version)
					uba32feature=true;;
				$uba321version)
					uba32feature=true;
					uba321feature=true;;
				$uba33version)
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;;
				$uba34version)
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;;
				$uba40version)
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;;
				$uba4001version)
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;
					uba4001feature=true;;
				$uba4002version)
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;
					uba4001feature=true;
					uba4002feature=true;;
				$uba41version)
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;
					uba4001feature=true;
					uba4002feature=true;
					uba41feature=true;
					uba411feature=true;
					uba412feature=true;;
				$uba411version)
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;
					uba4001feature=true;
					uba4002feature=true;
					uba41feature=true;
					uba411feature=true;;
				$uba412version)
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;
					uba4001feature=true;
					uba4002feature=true;
					uba41feature=true;
					uba411feature=true;
					uba412feature=true;;
				$uba413version)
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;
					uba4001feature=true;
					uba4002feature=true;
					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;;
				$uba42version)
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;
					uba4001feature=true;
					uba4002feature=true;
					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
					uba42feature=true;;
				$uba421version)
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;
					uba4001feature=true;
					uba4002feature=true;
					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
					uba42feature=true;
					uba421feature=true;;
				$uba422version)
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;
					uba4001feature=true;
					uba4002feature=true;
					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
					uba42feature=true;
					uba421feature=true;
					uba422feature=true;;
				$uba423version)
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;
					uba4001feature=true;
					uba4002feature=true;
					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
					uba42feature=true;
					uba421feature=true;
					uba422feature=true;
					uba423feature=true;;
				$uba43version)
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;
					uba4001feature=true;
					uba4002feature=true;
					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
					uba42feature=true;
					uba421feature=true;
					uba42f2eature=true;
					uba42f3eature=true;
					uba43feature=true;;
				$uba431version)
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;
					uba4001feature=true;
					uba4002feature=true;
					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
					uba42feature=true;
					uba421feature=true;
					uba42f2eature=true;
					uba42f3eature=true;
					uba43feature=true;
					uba431feature=true;;
				$uba4311version)
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;
					uba4001feature=true;
					uba4002feature=true;
					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
					uba42feature=true;
					uba421feature=true;
					uba42f2eature=true;
					uba42f3eature=true;
					uba43feature=true;
					uba431feature=true;
					uba4311feature=true;;
				$uba432version)
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;
					uba4001feature=true;
					uba4002feature=true;
					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
					uba42feature=true;
					uba421feature=true;
					uba42f2eature=true;
					uba42f3eature=true;
					uba43feature=true;
					uba431feature=true;
					uba4311feature=true;
					uba432feature=true;;
				$uba433version)
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;
					uba4001feature=true;
					uba4002feature=true;
					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
					uba42feature=true;
					uba421feature=true;
					uba42f2eature=true;
					uba42f3eature=true;
					uba43feature=true;
					uba431feature=true;
					uba4311feature=true;
					uba432feature=true;
					uba433feature=true;;
				$uba434version)
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;
					uba4001feature=true;
					uba4002feature=true;
					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
					uba42feature=true;
					uba421feature=true;
					uba42f2eature=true;
					uba42f3eature=true;
					uba43feature=true;
					uba431feature=true;
					uba4311feature=true;
					uba432feature=true;
					uba433feature=true;
					uba434feature=true;;
				$uba501version)
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;
					uba4001feature=true;
					uba4002feature=true;
					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
					uba42feature=true;
					uba421feature=true;
					uba42f2eature=true;
					uba42f3eature=true;
					uba43feature=true;
					uba431feature=true;
					uba432feature=true;
					uba433feature=true;
					uba434feature=true;
					uba50feature=true;
					uba501feature=true;;
				$uba502version)
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;
					uba4001feature=true;
					uba4002feature=true;
					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
					uba42feature=true;
					uba421feature=true;
					uba42f2eature=true;
					uba42f3eature=true;
					uba43feature=true;
					uba431feature=true;
					uba432feature=true;
					uba433feature=true;
					uba434feature=true;
					uba50feature=true;
					uba501feature=true;
					uba502feature=true;;
				$uba503version)
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;
					uba4001feature=true;
					uba4002feature=true;
					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
					uba42feature=true;
					uba421feature=true;
					uba42f2eature=true;
					uba42f3eature=true;
					uba43feature=true;
					uba431feature=true;
					uba432feature=true;
					uba433feature=true;
					uba434feature=true;
					uba50feature=true;
					uba501feature=true;
					uba502feature=true;
					uba503feature=true;;
				$uba504version)
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;
					uba4001feature=true;
					uba4002feature=true;
					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
					uba42feature=true;
					uba421feature=true;
					uba42f2eature=true;
					uba42f3eature=true;
					uba43feature=true;
					uba431feature=true;
					uba432feature=true;
					uba433feature=true;
					uba434feature=true;
					uba50feature=true;
					uba501feature=true;
					uba502feature=true;
					uba503feature=true;
					uba504feature=true;;
				$uba5041version)
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;
					uba4001feature=true;
					uba4002feature=true;
					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
					uba42feature=true;
					uba421feature=true;
					uba42f2eature=true;
					uba42f3eature=true;
					uba43feature=true;
					uba431feature=true;
					uba432feature=true;
					uba433feature=true;
					uba434feature=true;
					uba50feature=true;
					uba501feature=true;
					uba502feature=true;
					uba503feature=true;
					uba504feature=true;
					uba5041feature=true;;
				$uba505version)
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;
					uba4001feature=true;
					uba4002feature=true;
					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
					uba42feature=true;
					uba421feature=true;
					uba42f2eature=true;
					uba42f3eature=true;
					uba43feature=true;
					uba431feature=true;
					uba432feature=true;
					uba433feature=true;
					uba434feature=true;
					uba50feature=true;
					uba501feature=true;
					uba502feature=true;
					uba503feature=true;
					uba504feature=true;
					uba5041feature=true;
					uba505feature=true;;
				$uba506version)
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;
					uba4001feature=true;
					uba4002feature=true;
					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
					uba42feature=true;
					uba421feature=true;
					uba42f2eature=true;
					uba42f3eature=true;
					uba43feature=true;
					uba431feature=true;
					uba432feature=true;
					uba433feature=true;
					uba434feature=true;
					uba50feature=true;
					uba501feature=true;
					uba502feature=true;
					uba503feature=true;
					uba504feature=true;
					uba5041feature=true;
					uba505feature=true;
					uba506feature=true;;
				$uba51version)
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;
					uba4001feature=true;
					uba4002feature=true;
					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
					uba42feature=true;
					uba421feature=true;
					uba42f2eature=true;
					uba42f3eature=true;
					uba43feature=true;
					uba431feature=true;
					uba432feature=true;
					uba433feature=true;
					uba434feature=true;
					uba50feature=true;
					uba501feature=true;
					uba502feature=true;
					uba503feature=true;
					uba504feature=true;
					uba5041feature=true;
					uba505feature=true;
					uba506feature=true;
					uba51feature=true;;
				$uba50version|*)
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;
					uba4001feature=true;
					uba4002feature=true;
					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
					uba42feature=true;
					uba421feature=true;
					uba42f2eature=true;
					uba42f3eature=true;
					uba43feature=true;
					uba431feature=true;
					uba432feature=true;
					uba433feature=true;
					uba434feature=true;
					uba50feature=true;;
			esac	
		fi

                if $uba41feature
                then
                        UBA_TUNING_PROPERTIES=/etc/caspida/local/conf/deployment/uba-tuning.properties
                else
                        UBA_TUNING_PROPERTIES=""
		fi

		if $uba32feature
		then
			UBA_DEFAULT_PROPERTIES=$CASPIDAdir/conf/uba-default.properties	# defaults
			UBA_ENV_PROPERTIES=$CASPIDAdir/conf/uba-env.properties			# blueprint deployment in 2.4
			UBA_SITE_PROPERTIES=/etc/caspida/local/conf/uba-site.properties		# customization
			UBA_PROPERTIES="$UBA_DEFAULT_PROPERTIES $UBA_ENV_PROPERTIES $UBA_TUNING_PROPERTIES $UBA_SITE_PROPERTIES"
		fi
		SITE_PROPERTIES=`basename $UBA_SITE_PROPERTIES`
	fi

}

#
#	display outputconnector configuration
#

DISPLAY_OUTPUT_CONNECTOR () {
	OC_ID=$1

	export IOd="outputconnectors"

	OC_NAME=`psql -d caspidadb -t -c "select name from outputconnectors where id='$OC_ID';" 2>/dev/null | sed -e 's/^\s\s*//' -e 's/\s\s*$//' -e '/^$/d'`

	export ID_NAME="$OC_NAME"
	
	if which openssl >/dev/null 2>&1
	then
		( exec $DSH $IDINFO $OC_ID ) > $TMP3file 2>&1	# get datasource query

		OCusername=`grep '"username"' $TMP3file |  cut -d '"' -f 4`
		OCusername=`TRANSLATE "$OCusername"`

		OCrecipients=`grep '"recipients"' $TMP3file |  cut -d '"' -f 4`
		OCrecipients=`TRANSLATE "$OCrecipients"`

		sed -e '/"username"/ s/": "[^"][^"]*/": "notDisplayable/' -e '/"recipients"/ s/": "[^"][^"]*/": "notDisplayable/' \
			-e "/username/ s/notDisplayable/$OCusername/" -e "/recipients/ s/notDisplayable/$OCrecipients/" < $TMP3file > $TMP4file
	else
		echo "   <= openssl not installed" > $TMP4file
	fi

	if ! test -s $TMP4file
	then
		echo "   <= connector was not retrieved; could be blocked" > $TMP4file
	fi
	echo
	NECHO "${OC_NAME} conf:" "`head -1 $TMP4file`"; echo
	ALL_BUT_FIRST $TMP4file
	> $TMP3file
}

#
#	display datasource configuration
#

DISPLAY_DATASOURCE () {
	DS_ID=$1

	export IOd="datasources"

	DS_NAME=`psql -d caspidadb -t -c "select name from datasources where id='$DS_ID';" 2>/dev/null | sed -e 's/^\s\s*//' -e 's/\s\s*$//' -e '/^$/d'`

	export ID_NAME="$DS_NAME"

	if which openssl >/dev/null 2>&1
	then
		( exec $DSH $IDINFO $DS_ID ) > $TMP3file 2>&1	# get datasource query

		DSusername=`grep '"userName"' $TMP3file |  cut -d '"' -f 4`
		DSusername=`TRANSLATE "$DSusername"`

		DSrecipients=`grep '"recipients"' $TMP3file |  cut -d '"' -f 4`
		DSrecipients=`TRANSLATE "$DSrecipients"`

		sed -e '/"userName"/ s/": "[^"][^"]*/": "notDisplayable/' -e '/"recipients"/ s/": "[^"][^"]*/": "notDisplayable/' \
			-e '/"query":/ s/y": ".*",$/y": "seeFormattedBelow",/' \
			-e '/"query":/ s/y": ".*"$/y": "seeFormattedBelow"/' \
			-e "/userName/ s/notDisplayable/$DSusername/" -e "/recipients/ s/notDisplayable/$DSrecipients/" < $TMP3file > $TMP4file
	else
		echo "   <= openssl not installed" > $TMP4file
	fi

	if ! test -s $TMP4file
	then
		echo "   <= datasource was not retrieved; could be blocked" > $TMP4file
	else
		if grep -q '"splunkSendToKafka": true' $TMP4file
		then
			echo "$DS_NAME" >> $KAFKAingest
		fi
		
		GET_CASPIDA_PROPERTY splunk.live.micro.batching
                if test "$PROPERTYvalue" = "true"
		then
			if grep -q '"endDate": "rt",' $TMP4file
			then
				:
			else
				sed -ie '/"endDate"/ s/$/   <= unexpected value with micro-batching/' $TMP4file
			fi
		fi
	fi
	echo
	NECHO "${DS_NAME} conf:" "`head -1 $TMP4file`"; echo
	ALL_BUT_FIRST $TMP4file

	if grep -q '"cimCompliant": true,' $TMP4file
	then
		CIMcompliant=true
	else
		CIMcompliant=false
	fi

	grep '"query":' < $TMP3file  | head -1 | sed -e 's/\\"/"/g' -e 's/\\n/\n/g' -e 's/\\\\/\\/g' -e 's/^\s\s*"query": "//' -e 's/"$//' -e 's/,\([^\s]\)/,        \1/g'|  fold -s -w 120 | sed -e 's/,        /,/g' -e 's/^\s\s*//' | sed -e '$ s/",$//' > $TMP4file
	if test -s $TMP4file
	then
		ERROR=""
		if $CIMcompliant
		then
			if egrep -qe 'fields\s' $TMP4file
			then
				:
			else
				ERROR="   <= SPL may not meet UBA CIM-compliant expectations" 
			fi
		fi

		echo
		NECHO "${DS_NAME} query:" "`head -1 $TMP4file`"; echo "$ERROR"
		ALL_BUT_FIRST $TMP4file
		sed -e '/^$/d' -e 's/$/ /' < $TMP4file | sed -e :a -e '$!N; s/\n//; ta' > $TMP3file     # flatten SPL to a single line
		LINE1=`sed -e 's/^\(....[^|]*\)|.*$/\1/' < $TMP3file`
		echo "${DS_NAME}:    $LINE1" >> $CONTRIBlines		# spl used for view conributing events
	fi
	> $TMP3file
}

#
#	is datasource distributed
#

DISPLAY_DATASOURCE_DISTRIBUTED () {

	DS_NAME=$1

	export uba42feature

        if which openssl >/dev/null 2>&1
        then
		( exec $DSH $DSINFO $DS_NAME ) > $TMP3file 2>&1	# get datasource info
		egrep -e "maxHeapMB|parallelism|distributed|remoteAgent" $TMP3file | fmt -s -w 120 >$TMP4file 
        else
		NECHO "${DS_NAME}:" "   <= openssl not installed"; echo
        fi
	if test -s $TMP4file
	then
		if grep -q 'distributed:true' $TMP4file
		then
			echo
			NECHO "${DS_NAME}:" "`head -1 $TMP4file`"; echo
			ALL_BUT_FIRST $TMP4file
			grep 'parallelism' $TMP4file >> $TMP5file
		fi
	fi
	> $TMP3file

}

#
#	sample users with multiple accounts
#

SAMPLE_USERS_ACCOUNTS () {

	COUNT=${1-2}
	LIMIT=${2-$SAMPLElimit}
	psql -d caspidadb -c "$OBSCUREfunction; select pg_temp.obscure(u.employeeid) as id, a.parentid as hrdatausers_key, hraccounttype as type, pg_temp.obscure(loginid) as loginid,   pg_temp.obscure(a.domainloginid) as domainloginid, pg_temp.obscure(displayname) as displayname, pg_temp.obscure(firstname) as firstname, pg_temp.obscure(lastname) as lastname, pg_temp.obscure(email) as email, u.hrstatus as hrstatus, date_trunc('Seconds', u.updated) as updated from hrdatausers u, hrdataaccounts a where a.parentid in (select parentid from hrdataaccounts group by 1 having count(*) = $COUNT limit $LIMIT) AND u.caspidaid=a.parentid order by 1,2;" 2>/dev/null | egrep -v ' rows*\)' > $TMP3file

	NECHO "sample '$COUNT' accounts *:" "`head -1 $TMP3file`"
	if test $LIMIT -eq 1
	then
		USER_KEY=`sed -e '3!d' <$TMP3file| tr -d ' ' | cut -d '|' -f 1`
		if test $COUNT -gt 25
		then
			ERROR="<= only user with this number of accounts"
		else
			ERROR=""
		fi
		echo "user '$USER_KEY' has $COUNT accounts   $ERROR"
	else
		echo
	fi
	ALL_BUT_FIRST $TMP3file
}

#
#       sample users with particular account type
#

SAMPLE_USERS_TYPE () {

	TYPE="${1-Service}"
	LIMIT=${2-$SAMPLElimit}
	psql -d caspidadb -c "$OBSCUREfunction; select pg_temp.obscure(u.employeeid) as id, a.parentid as hrdatausers_key, hraccounttype as type, pg_temp.obscure(loginid) as loginid, pg_temp.obscure(domainloginid) as domainloginid, pg_temp.obscure(displayname) as displayname, pg_temp.obscure(firstname) as firstname, pg_temp.obscure(lastname) as lastname, pg_temp.obscure(email) as email , u.hrstatus as hrstatus, date_trunc('Seconds', u.updated) as updated, case when ((hraccounttype='Service' OR hraccounttype='System') AND (hrstatus='Active' OR hrstatus='InActive')) then '   <= hrstatus not expected for $TYPE' else '' end  as comment from hrdatausers u, hrdataaccounts a where a.parentid in (select parentid  from hrdataaccounts where hraccounttype='$TYPE' limit $LIMIT) AND a.parentid=u.caspidaid;" 2>/dev/null | egrep -v ' rows*\)' > $TMP3file

	NECHO "sample '$TYPE' users *:" "`head -1 $TMP3file`"; echo
	ALL_BUT_FIRST $TMP3file
}

#
# sub-routine to ouput a label and value without a new line
#

NECHO() {
	TAG="$1"
	VALUE="$2"

	echo "$1" | awk '{ printf "    %-25.25s ", $0}'	# note: no newline
	echo -n "$2"		# note: no newline
}

GET_CASPIDA_PROPERTY () {
		PROPERTYarg=${1-missing}
		PROPERTYvalue=`egrep -he "^$PROPERTYarg\s*=" $UBA_PROPERTIES 2>/dev/null |\
		tail -1 | sed -e 's/\s*=\s*/=/' | cut -d '=' -f 2`
}


CHECK_DEPLOYMENT () {
	SERVICE=${1-missing}
	NODE=${2-$THISnode}
	IP=${3-$THISip}

	if test -f $CASPIDAdir/conf/deployment/caspida-deployment.conf
	then 
        	egrep "^$SERVICE\b" $CASPIDAdir/conf/deployment/caspida-deployment.conf > $TMP3file 2>/dev/null
        	if egrep -iq "\b$NODE\b" $TMP3file
        	then
                	FOUND=true
		else
			FOUND=false
			if egrep -iq "\b$IP\b" $TMP3file
			then
				FOUND=true
			fi
        	fi	
		if test -s $TMP3file
		then
			DEPLOYMENTlist=`cut -d '=' -f 2 < $TMP3file`
		else
			DEPLOYMENTlist="localhost"		# use 'localhost' as a default
		fi
	else
		FOUND=false	# without deployment, everything assumed to be false 
		DEPLOYMENTlist=""
	fi

	if ! $CASPIDA_PROPERTIES_EXIST
	then
		# all bets off until 'setup' has been run

		FOUND=false
		DEPLOYMENTlist=""	
	fi
}

#
#	Get 'tunable' values
#

CHECK_TUNABLES() {
	TUNABLEsection=${1-missing}
	TUNABLEarg=${2-missing}
	CLUSTERnodes=${3-$NUMnodes}

	if test "$TUNABLEarg" = "workersHint"
	then
		DELIM=":"
		DELchar='" ,'
	else
		DELIM="="
		DELchar='"'
	fi


	TUNABLEfile=$CASPIDAdir/conf/deployment/recipes/caspida/caspidatunables-${CLUSTERnodes}_node.conf
	if test -r $TUNABLEfile
	then
		sed -ne "/BEGIN $TUNABLEsection/,/END $TUNABLEsection/p" < $TUNABLEfile |\
		egrep "$TUNABLEarg[^\.]" | tr -d "$DELchar" > $TMP3file
		TUNABLEvalue=`sed -e "s/^[^$DELIM]*$DELIM//" < $TMP3file`
	else
		TUNABLEvalue=""
	fi

# 	echo "section: $TUNABLEsection; arg: $TUNABLEarg; nodes: ${CLUSTERnodes}; value: $TUNABLEvalue"
}

#
#	sub-routine to maintain count of exceptions
#
#	stored in a file because sub-shell can not update global variable
#

INCREMENT_EXCEPTIONS () {
	INCREMENT=${1-1}	# by default, increment by one
	REFnum=${2-unknown}
	EXCEPTIONS=`cat $EXCfile`
	EXCEPTIONS=`expr $EXCEPTIONS + $INCREMENT`
	echo "exceptions: incr=$INCREMENT, ref=$REFnum" >> $EXCref 
	echo $EXCEPTIONS > $EXCfile
}

#
#	sub-routine to indent remaining lines of a file
#

ALL_BUT_FIRST () {
	FILEname=$1

	LINEcount=`wc -l < $FILEname`	
	if test $LINEcount -gt 1
	then
		LINEcount=`expr $LINEcount - 1`
		tail -$LINEcount $FILEname | sed -e "s/^/$INDENT/"
	fi
}

#
#	obscure PII
#

SET_SEED() {
	CHARset='#1Aa2Bb3Cc4Dd5Ee6Ff7Gg8Hh9Ii0JjKkLlMmNnOoPpQqRrSsTtUuVvWwXxYyZz'

	if which shuf >/dev/null 2>&1
	then	
		SEED=`shuf -e a b c d e f g h i j k l m n o p q r s t u v w x y z A B C D E F G H I J K L M N O P Q R S T U V W X Y Z 0 1 2 3 4 5 6 7 8 9 '#'  |
 tr -d '\n'`
	else
		SEED='YDPOy5IuKVWRa76h8CFXrq1w4GibkdEfS#cgLQJUxjnNZv30oAt9Mmspelz2TBH'
	fi	

	RANDOM=`echo 'echo $RANDOM' | bash`
        ROTATE=`expr $RANDOM \% 23 + 1`
}

SHUFFLE() {
	direction=$1

	case $direction in
		in) echo "$SEED" | sed -e "s/^\(.\{$ROTATE\}\)\(.*\)/\2\1/g";;
		 *) echo "$SEED";;
	esac
}


TRANSLATE() {
        FROM=$1

        TO=`echo "$FROM" | tr "$CHARset" "$INPUTstring" | tr '[:upper:]' '[:lower:]' | tr "$CHARset" "$OUTPUTstring" | tr '[:upper:]' '[:lower:]' | tr "$CHARset" "$INPUTstring" | tr '[:upper:]' '[:lower:]' `
        echo "$TO"
}

#
#	compute sequential IOPS 
#

IOPS() {
        OPERATION=${1}
        BLOCKsize=${2}
        REPORTfile=${3}
	PART=${4}

        grep copied $REPORTfile |\
	sed -e 's/ bytes.* s, / /' |\
        while read BYTES RATE UNIT
        do
#               echo "bytes: $BYTES; rate: $RATE; unit: $UNIT"
                case "$UNIT" in
                        [mM]*) CONVERSION=1024; UNIT="KB/s";;
                        [gG]*) CONVERSION=`expr 1024 \* 1024`; UNIT="KB/s";;
			[kK]*) CONVERSION=1;;
                esac
                RATE=`echo $RATE $CONVERSION | awk '{print ($1 * $2)}'`
#               echo "bytes: $BYTES; rate: $RATE; unit: $UNIT"

                IOPS=`echo $RATE $BLOCKsize | awk '{printf "%d", ($1 / $2)}'`
                NECHO "iops $PART ${OPERATION}:" "sequential (${BLOCKsize}k): $IOPS"
		if test "$IOPS" -ge  "$IOPSmin"
		then
			echo
		else
			echo "   <= latency on disk $PART may need to be monitored"
			# sed -e "s/^/$INDENT/" < $REPORTfile
		fi
        done
}

CHECK_FOR_UPDATE() {

#
#	Before doing anything more, check to see if there is an updated version of script on Box
#

if ! test "$TTY" = "not a tty"
then
	echo "Checking for latest version" > /dev/tty
        NECHO "version check:" ""
	if sudo which wget >/dev/null 2>&1
	then
		SCRIPTname=$PROGname
	
		case "$SCRIPTname" in
        		uba_pre_check.sh)       FILEurl="https://splunk.box.com/shared/static/pzgpthu7zhucyeb3tcqwmatibrj8te0d.sh"
                       		         	MD5url="https://splunk.box.com/shared/static/myext7xe8qlkzaif90n95pl98sjx0qyf.md5";;
        		uba_health_check.sh)    FILEurl="https://splunk.box.com/shared/static/fkbqty0zkhsfnodf6ucsdjhj6m86fqvs.sh"
                       		         	MD5url="https://splunk.box.com/shared/static/870akdfxy0j3vj0mfgrulcv0tc44s8g6.md5";;
		esac

		mkdir -p ~/bin >/dev/null 2>&1		# probably exists but make sure

		GETmd5=false
		MD5file=$TMP/${BASEname}.md5
		if test -f $MD5file
		then
			INTERVALsecs=900		# don't want to check repeatedly; 15 minutes is compromise
			MD5time=`stat -c %Y $MD5file`
       			THENsecs=`expr $NOW - $INTERVALsecs`
        		if test $THENsecs -gt $MD5time
        		then
                		GETmd5=true
        		fi
		else
			GETmd5=true
		fi

		GETfile=false			# assume that file is up to date

		if $GETmd5
		then
                     if which timeout >/dev/null 2>&1
                        then
                                TIMEOUT="timeout 15"    # ideally, want to timeout DNS queries
                        else    
                                TIMEOUT=""
                        fi

			if $TIMEOUT wget --no-check-certificate -O $MD5file $MD5url >/dev/null 2>&1
                	then
                        	if which md5sum >/dev/null 2>&1
                        	then
					SAVEwd="`pwd`"
					for BINdir in "$PROGdir"
					do
						cd $BINdir
                                		if md5sum -c $MD5file >/dev/null 2>&1
                                		then
                                        		echo "current $BINdir; matches Box"    # nothing to change     
                                		else
                                        		echo -n "update available"
							if test "$BINdir" = "/home/caspida/bin"
							then
								echo "; will attempt download to '$BINdir'"
                                        			GETfile=true
							else
								echo "   <= download latest version of $SCRIPTname from Box to $BINdir"
								 echo "$FILEurl" | sed "s/^/$INDENT    /"
							fi
						fi
					done
					cd "$SAVEwd"
                        	else
                               		echo "failed   <= 'md5sum' not available"
				fi
                	else
                        	echo "failed   ('wget' failed)"
				 echo "$FILEurl" | sed "s/^/$INDENT    /"
                	fi
		else
			echo "current at this time"
		fi
	else
		echo "failed   <= 'wget' not available to check for update"
 		echo "$FILEurl" | sed "s/^/$INDENT    /"
	fi	
	echo
fi 

}

DOWNLOAD_SCRIPT() {

	GOTfile=false

	cd "$BINdir"
        NECHO "downloading script:" ""
        if wget --no-check-certificate -O ${SCRIPTname}.new $FILEurl >/dev/null 2>&1
        then
                GOTfile=true
                NEWmd5=`md5sum ${SCRIPTname}.new | cut -d ' ' -f 1`
                EXPECTEDmd5=`cut -d ' ' -f 1 < $MD5file`
                if test "$NEWmd5" = "$EXPECTEDmd5"
                then
                        echo "successful"
                else
                        echo "md5sum mismatch   <= download $SCRIPTname from Box manually"
                        GOTfile=false
                fi
        else
                echo "wget failed	<= download $SCRIPTname from Box manually using $FILEurl"
        fi

        if ! $GOTfile
        then
                rm -f ${SCRIPTname}.new
        fi

	cd "$SAVEwd"
}

MMDB_BUILD() {
	MMDBfile=${1-noMMDBfile}

	if test -r $MMDBfile
	then
		EPOCHoff=`grep --byte-offset --only-matching --text  build_epoch $MMDBfile 2>/dev/null | cut -d ':' -f 1`
		EPOCHoff=`expr $EPOCHoff + 11 + 2  2>/dev/null`
		if test "$EPOCHoff" != ""
		then
			EPOCHbytes=4
			dd if=$MMDBfile iflag=skip_bytes,count_bytes  skip=$EPOCHoff count=$EPOCHbytes of=$TMP3file >/dev/null 2>&1
        		dd conv=swab if=$TMP3file of=$TMP4file >/dev/null 2>&1
			EPOCHhex=`od -x $TMP4file | head -1`
			EPOCHhex=`echo $EPOCHhex | sed -e 's/^000000//' -e 's/\s2020//g' -e 's/\s//g'`
        		EPOCHsecs=`printf '%d\n' 0x$EPOCHhex 2>/dev/null`
			if test "$EPOCHsecs" -lt $NOW 2>/dev/null
			then
				if test "$EPOCHsecs" -gt $UBAepoch 2>/dev/null
				then
        				EPOCHbuild=`TZ=UTC date -d@$EPOCHsecs 2>/dev/null`
        				NECHO "   build epoch:"  "   (${EPOCHsecs}) - $EPOCHbuild"; echo
				fi
			fi
		fi
	fi
}

#
#	checking to see if UBA has already been configured
#

TTY=`tty 2>&1`


UBA_VERSION			# check for Dubai

if test -f $UBA_DEFAULT_PROPERTIES
then
	if test -f $UBA_ENV_PROPERTIES
	then
		if grep -q '<%=' $UBA_ENV_PROPERTIES
		then
			CASPIDA_PROPERTIES_EXIST=false			# 'setup' has not been run
		else
			CASPIDA_PROPERTIES_EXIST=true			# 'setup' has been run
        		egrep "^caspida.cluster.nodes\b" $CASPIDAdir/conf/deployment/caspida-deployment.conf > $TMP3file 2>/dev/null
			if test -s $TMP3file
			then
				CLUSTER_NODES_list=`cut -d '=' -f 2 < $TMP3file`
			else
				CLUSTER_NODES_list=""
			fi
		fi
	else
		CASPIDA_PROPERTIES_EXIST=false				# capida-site.properties does not exist
	fi
else
	CASPIDA_PROPERTIES_EXIST=false					# caspida-default.properties does not exist
fi

FIRSTnode=`echo "$CLUSTER_NODES_list" | cut -d ',' -f 1 | tr '[:upper:]' '[:lower:]'`	# this is the 'leader' of the cluster

USEip=false
USEfqdn=false

case $FIRSTnode in
	[0-9]*\.[0-9]*\.[0-9]*\.[0-9]*)	THISnode=$THISip
					USEip=true;;
	*\.*) USEfqdn=true;;
esac

if $USEfqdn
then
	THISnode=`hostname -f 2>/dev/null | tr '[:upper:]' '[:lower:]'`
else
	THISnode=`hostname 2>/dev/null | tr '[:upper:]' '[:lower:]' | cut -d '.' -f 1` 
fi

THISip=`hostname -i 2>/dev/null`
if $USEip
then
        THISnode=$THISip
fi

NUMnodes=`echo "$CLUSTER_NODES_list" | tr ',' ' ' | wc -w`

if test "$NUMnodes" -gt "1"
then
	MULTInode=true
else
	MULTInode=false
fi

NUMnodes_unique=`echo "$CLUSTER_NODES_list" | tr ',' ' ' | sed -e 's/\s\s/ /g' | tr ' ' '\n' | sort -fu | wc -l` 
if test "$NUMnodes" -ne "$NUMnodes_unique"
then
	NECHO "node names:" "'$CLUSTER_NODES_list'   <== node names are not unique; re-run 'Setup' with unique names"; echo
	NECHO "" " ... cannot continue as cluster..."; echo
	INCREMENT_EXCEPTIONS
	CASPIDA_PROPERTIES_EXIST=false			# deployment is incorrect
	MULTInode=false					# only report on this node
fi

if test "$THISnode" = "$FIRSTnode" -a "$PROGdir" = "/home/caspida/bin"
then
	# BOX has been deprecated 
	#
	# CHECK_FOR_UPDATE	# see if Box has different version
	#	
        # public Google Drive being investigated

	if $GETfile
	then
		DOWNLOAD_SCRIPT	# updated script should be downloaded
	fi

	if $GOTfile
	then
		OLDstamp=`date '+%Y_%m_%d_%H:%M:%S' -r  $PROGdir/${PROGname}`
		if cp -p $PROGdir/${PROGname} $PROGdir/${PROGname}.$OLDstamp >/dev/null 2>&1
		then
			if cp $PROGdir/${PROGname}.new $PROGdir/${PROGname} >/dev/null 2>&1
			then
				echo 
				NECHO "saved script:"	"prior version saved as $PROGdir/${PROGname}.$OLDstamp"; echo
				echo
				NECHO "updated script:" "re-run new version of $PROGdir/$PROGname"; echo
				echo; echo
			else
				NECHO "update script:" "failed   <== failed to replace script with $PROGdir/${PROGname}.new"; echo
				INCREMENT_EXCEPTIONS 1 8
			fi
		else
			NECHO "save script:" "failed   <== could not save to $PROGdir/${PROGname}.$OLDstamp"; echo
			INCREMENT_EXCEPTIONS 1 9
		fi
	fi
fi | tee -a $OUTfile

if grep -q "re-run new version of" $OUTfile
then
	exit 0
fi

if $MULTInode
then
	if test "$THISnode" = "$FIRSTnode"
	then
		LEADERnode=true
		MEMBERnode=false
		MEMBERlist="`echo "$CLUSTER_NODES_list" | tr ',' ' ' | cut -d ' ' -f 2- | tr '[:upper:]' '[:lower:]'`"

		echo
		echo "parallelization ..."
		# FIRSTnode is responsible for checking other nodes
		for NODE in $MEMBERlist
		do
			echo
			NECHO "verifying version:"  "$NODE ... "	# check the member node
	
			SCRIPTbin="${CASPIDAdir}/bin"
			if $uba32feature
			then
				SCRIPTbin="${SCRIPTbin}/utils"
			fi
			
			if ssh -o ConnectTimeout=15 $NODE grep "^VERSion" "${SCRIPTbin}/$PROGname" > $TMP1file 2>/dev/null
			then
				REMOTEversion=`sed -e 's/^VERSion="//' -e 's/"//' < $TMP1file`
				echo -n "'$REMOTEversion'"
				if test "$VERSion"  != "$REMOTEversion"
				then
					REPLICATE=true		# use the leader's version
				else
					REPLICATE=false		# use the distributed version
				fi
			else
				echo -n "'check failed'"
				REPLICATE=true			# distribution file not found
			fi
	
			if $REPLICATE
			then
				echo "   (replication needed)" | tee /dev/tty
				if ssh -o ConnectTimeout=15 $NODE mkdir -p ~/bin 2>/dev/null
 				then
 					:               # ~/bin/ directory created
				else
					NECHO "replicate:" "   <== mkdir on '$NODE' failed; check will not run on this node"; echo       
					INCREMENT_EXCEPTIONS
					continue
				fi

				if scp "$PROGdir/$PROGname" ${NODE}:~/bin >/dev/null 2>&1
				then
					SCRIPTbin="~/bin"	# script was copied
				else
					NECHO "replicate:" "   <== scp to '$NODE' failed; check will not run on this node"; echo 
					INCREMENT_EXCEPTIONS
					continue                        
				fi
			else
				echo
			fi 

			MEMBERoutfile="${MEMBERout}${NODE}.txt"
			
			echo "$MEMBERoutfile" >> $MEMBERwait

			(NECHO "dispatching script:" "initiated on ${NODE}:${SCRIPTbin} ..."; echo) | tee /dev/tty	
			ssh -o ServerAliveInterval=55 -o ServerAliveCountMax=3 $NODE "sh ${SCRIPTbin}/$PROGname $NODElist" > $MEMBERoutfile 2>&1 &	# run in background for parallelism
			echo $! >> $BGjobs
		done 


		echo
		echo "Checking $FIRSTnode first ..."
		echo
	else
		LEADERnode=false
		MEMBERnode=true
	fi

	DEPLOYnode=$THISnode
else
	LEADERnode=true
	MEMBERnode=false

	if $uba41feature
	then
		DEPLOYnode=$THISnode
	else
		DEPLOYnode="localhost"
	fi
	
	echo
	echo "Checking $FIRSTnode only ..."
fi >> $OUTfile

#
#	deployment.conf is checked for multi-node clusters for purposes of relevancy
#

CHECK_DEPLOYMENT uiServer.host $DEPLOYnode
UISERVER_HOST=$FOUND
UISERVER_HOST_list=$DEPLOYMENTlist

CHECK_DEPLOYMENT database.host $DEPLOYnode
DATABASE_HOST=$FOUND
DATABASE_HOST_list=$DEPLOYMENTlist

CHECK_DEPLOYMENT caspida.cluster.replication.nodes $DEPLOYnode
REPLICATION_NODE=$FOUND
REPLICATION_NODE_list=$DEPLOYMENTlist

CHECK_DEPLOYMENT kafka.brokers $DEPLOYnode
KAFKA_BROKER=$FOUND
	
CHECK_DEPLOYMENT spark.server $DEPLOYnode
SPARK_SERVER=$FOUND
SPARK_SERVER_list=$DEPLOYMENTlist
	
CHECK_DEPLOYMENT spark.master $DEPLOYnode
SPARK_MASTER=$FOUND
SPARK_MASTER_list=$DEPLOYMENTlist
	
CHECK_DEPLOYMENT spark.worker $DEPLOYnode
SPARK_WORKER=$FOUND
SPARK_WORKER_list=$DEPLOYMENTlist
SPARK_WORKER_host_count=`echo $DEPLOYMENTlist | tr ',' ' ' | wc -w`
	
CHECK_DEPLOYMENT impala $DEPLOYnode
IMPALA_HOST=$FOUND

CHECK_DEPLOYMENT storm.ui.host $DEPLOYnode
STORM_UI_HOST=$FOUND
STORM_UI_HOST_list=$DEPLOYMENTlist
	
CHECK_DEPLOYMENT storm.supervisor $DEPLOYnode
STORM_SUPERVISOR=$FOUND
	
CHECK_DEPLOYMENT storm.nimbus.host $DEPLOYnode
STORM_NIMBUS_HOST=$FOUND
STORM_NIMBUS_HOST_list=$DEPLOYMENTlist
	
CHECK_DEPLOYMENT jobmanager.restServer $DEPLOYnode
JOB_MANAGER=$FOUND
JOB_MANAGER_host=`echo $DEPLOYMENTlist | cut -d ':' -f 1`
if test "$JOB_MANAGER_host" = ""
then
	JOB_MANAGER_host="localhost"
fi
export JOB_MANAGER_host

CHECK_DEPLOYMENT jobmanager.agents $DEPLOYnode
JOB_AGENT=$FOUND
JOB_AGENT_list=`echo $DEPLOYMENTlist | tr ','  ' '`

CHECK_DEPLOYMENT analytics.host $DEPLOYnode
ANALYTICS_HOST=$FOUND
ANALYTICS_HOST_list=$DEPLOYMENTlist
	
CHECK_DEPLOYMENT zookeeper.servers $DEPLOYnode
ZOOKEEPER_SERVER=$FOUND
ZOOKEEPER_SERVER_list=$DEPLOYMENTlist

CHECK_DEPLOYMENT hadoop.namenode.host $DEPLOYnode
HADOOP_NAMENODE=$FOUND

CHECK_DEPLOYMENT hadoop.snamenode.host $DEPLOYnode
HADOOP_SNAMENODE=$FOUND

CHECK_DEPLOYMENT hadoop.datanode.host $DEPLOYnode
HADOOP_DATANODE=$FOUND
HADOOP_DATANODE_list=$DEPLOYMENTlist

CHECK_DEPLOYMENT resourcesmonitor.host $DEPLOYnode
RESOURCESMONITOR=$FOUND
RESOURCESMONITOR_list=$DEPLOYMENTlist

CHECK_DEPLOYMENT sysmonitor.host $DEPLOYnode
SYSMONITOR=$FOUND
SYSMONITOR_list=$DEPLOYMENTlist

if $uba32feature
then
	HBASE_MASTER=false
	HBASE_REGIONSERVER=false
else
	CHECK_DEPLOYMENT hbase.master.host $DEPLOYnode
	HBASE_MASTER=$FOUND
	HBASE_MASTER_list=$DEPLOYMENTlist

	CHECK_DEPLOYMENT hbase.regionserver.host $DEPLOYnode
	HBASE_REGIONSERVER=$FOUND
	HBASE_REGIONSERVER_list=$DEPLOYMENTlist
fi

CHECK_DEPLOYMENT hive.host $DEPLOYnode
HIVE_HOST=$FOUND

CHECK_DEPLOYMENT rule.realtime.exec.host  $DEPLOYnode
RULE_REALTIME_HOST=$FOUND

CHECK_DEPLOYMENT rule.offline.exec.host $DEPLOYnode
RULE_OFFLINE_HOST=$FOUND

CHECK_DEPLOYMENT persistence.datastore.tsdb $DEPLOYnode
PERSISTENCE_DATASTORE=$FOUND
PERSISTENCE_DATASTORE_list=$DEPLOYMENTlist

CHECK_DEPLOYMENT persistence.graphdb.server $DEPLOYnode
PERSISTENCE_GRAPHDB_SERVER=$FOUND

CHECK_DEPLOYMENT persistence.redis.server $DEPLOYnode
PERSISTENCE_SERVER=$FOUND
PERSISTENCE_SERVER_list=$DEPLOYMENTlist

CHECK_DEPLOYMENT persistence.redis.irserver $DEPLOYnode
PERSISTENCE_IRSERVER=$FOUND
PERSISTENCE_IRSERVER_list=$DEPLOYMENTlist

if $uba40feature
then
	CHECK_DEPLOYMENT container.worker.host $DEPLOYnode
	CONTAINER_WORKER=$FOUND
	CONTAINER_WORKER_list=$DEPLOYMENTlist

	CHECK_DEPLOYMENT container.master.host $DEPLOYnode
	CONTAINER_MASTER=$FOUND
	CONTAINER_MASTER_list=$DEPLOYMENTlist
fi

if $uba41feature
then
	CHECK_DEPLOYMENT kubernetes.restServer $DEPLOYnode
	KUBERNETES_RESTSERVER=$FOUND
	KUBERNETES_RESTSERVER_list=$DEPLOYMENTlist
fi

if $CASPIDA_PROPERTIES_EXIST
then
	echo
        NECHO "$BASEname:" "version $VERSion"; echo
	NECHO "" "exec'd from $PROGdir"; echo 
	NECHO "" "exec'd by $WHOIAM"; echo 
	NECHO "" "          "; id 
	chage -l $WHOIAM 2>/dev/null | grep -i '^Password expires'> $TMP1file
	if test -s $TMP1file
	then
		NECHO "" "          "; head -1 $TMP1file | sed -e 's/\s\s*:/ :/'
	fi
	if test "$SUDO_USER" != ""
	then 
		NECHO ""  "SUDO_USER: $SUDO_USER; SUDO_UID: $SUDO_UID; SUDO_COMMAND: $SUDO_COMMAND"; echo
	fi
	NECHO "" "UMASK: `umask`"; echo
	NECHO "" "HOME: $HOME"; echo
	NECHO "" "PATH: $PATH"; echo 
	NECHO "" "TTY: $TTY; SSH_TTY: $SSH_TTY"; echo 
	NECHO "" "TEMPdir: $TMP"; echo 
	NECHO "" "HOSTname: $HOSTname"; echo 
        echo

	NECHO "proxy settings:" ""
	env | grep -i "_proxy=" | sort -f > $TMP1file
	if test -s $TMP1file
	then
		head -1 $TMP1file
		ALL_BUT_FIRST $TMP1file

		if $uba41release
                then
                        IPcontainers=",10.96.0.0/12,10.244.0.0/16"
                else
                        IPcontainers=""
                fi
		NOPROXYlist="localhost,127.0.0.1${IPcontainers},$CLUSTER_NODES_list"

		if test -z "$no_proxy"
		then
                        NECHO "no_proxy:" "'$no_proxy'   <== not set;  no_proxy value must include '$NOPROXYlist'"; echo
                        INCREMENT_EXCEPTIONS
		fi
		if test -z "$NO_PROXY"
		then
                        NECHO "NO_PROXY:" "'$NO_PROXY'   <== not set;  NO_PROXY value must include '$NOPROXYlist'"; echo
                        INCREMENT_EXCEPTIONS
		fi
		if test "$no_proxy" != "$NO_PROXY"
		then
			NECHO "proxy values:" "no_proxy and NO_PROXY have different values   <== proxy environment must match"; echo
			INCREMENT_EXCEPTIONS
		fi
	else
		echo "(no proxy)"
		
		grep -Hi "no_proxy" /etc/environment ~/.bash_profile 2>/dev/null |\
		egrep -ve '^#' > $TMP1file 
		if test -s $TMP1file
		then
			echo
			NECHO "_proxy (file):" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		fi 
	fi

	echo 	

	#
	#	This info may be useful for debugging
	#


        if test -f $CASPIDAdir/conf/deployment/caspida-deployment.conf
        then
                DEPLOYMENT=`grep "node deployment" /opt/caspida/conf/deployment/caspida-deployment.conf 2>/dev/null | cut -d ' ' -f 2`
                DEPLOYMENTcount=`echo $DEPLOYMENT | sed -e 's/[^0-9]//g'`
                NECHO "deployment:" "'$DEPLOYMENT'";echo
                NECHO "cluster nodes:" "'$CLUSTER_NODES_list'"; echo            # show modes in cluster 
                echo
        fi

	REPLICATION_PRIMARY=false
	REPLICATION_SECONDARY=false

        GET_CASPIDA_PROPERTY replication.enabled                   # is this a DR cluster?
        REPLICATIONstate=`echo "$PROPERTYvalue" | tr -d ' ' | tr '[:upper:]' '[:lower:]'`
	
	if test "$REPLICATIONstate" = "true"
	then
		THIShost=`echo $THISnode | tr '[:upper:]' '[:lower:]'`

		GET_CASPIDA_PROPERTY replication.primary.host                   
        	REPLICATIONprimary=`echo "$PROPERTYvalue" | tr -d ' ' | tr '[:upper:]' '[:lower:]'`
		if test "$THIShost" = "$REPLICATIONprimary"
		then
			REPLICATION_PRIMARY=true
			echo "Primary" > $REPLtype
		fi

		GET_CASPIDA_PROPERTY replication.secondary.host                   
        	REPLICATIONsecondary=`echo "$PROPERTYvalue" | tr -d ' ' | tr '[:upper:]' '[:lower:]'`
		if test "$THIShost" = "$REPLICATIONsecondary"
		then
			REPLICATION_SECONDARY=true
			echo "Standby" > $REPLtype
		fi
	fi

	for ROLE in UISERVER_HOST DATABASE_HOST KAFKA_BROKER SPARK_SERVER SPARK_MASTER \
	SPARK_WORKER IMPALA_HOST STORM_SUPERVISOR STORM_NIMBUS_HOST STORM_UI_HOST JOB_MANAGER JOB_AGENT ANALYTICS_HOST \
	ZOOKEEPER_SERVER HADOOP_NAMENODE HADOOP_SNAMENODE HADOOP_DATANODE HBASE_MASTER HBASE_REGIONSERVER \
	PERSISTENCE_DATASTORE PERSISTENCE_IRSERVER PERSISTENCE_SERVER HIVE_HOST PERSISTENCE_GRAPHDB_SERVER \
	RULE_REALTIME_HOST RULE_OFFLINE_HOST KUBERNETES_RESTSERVER CONTAINER_MASTER CONTAINER_WORKER SYSMONITOR RESOURCESMONITOR REPLICATION_PRIMARY REPLICATION_SECONDARY
	do
		eval STATE=\$$ROLE
		if $STATE
		then
			echo "$ROLE " | tr '[:upper:]' '[:lower:]'
		fi
	done | sort > $TMP1file

	if $LEADERnode
	then
		echo leadernode
	else
		echo membernode
	fi > $TMP2file

	cat $TMP1file >> $TMP2file
	fmt -w 80 < $TMP2file > $TMP1file
	echo
	NECHO "UBA roles:" "`head -1 $TMP1file`"; echo
	ALL_BUT_FIRST $TMP1file
	echo
fi | tee -a  $OUTfile 

if $LEADERnode
then
	DEPLOYMENTconf="${CASPIDAdir}/conf/deployment/caspida-deployment.conf"
	if test -r $DEPLOYMENTconf
	then
		ls -l $DEPLOYMENTconf > $TMP1file
		echo >> $TMP1file
 		sed -e '/^$/d' -e '/^#/d' < $DEPLOYMENTconf >> $TMP1file
		NECHO "caspida-deployment.conf:" "`head -1 $TMP1file`"; echo
        	ALL_BUT_FIRST $TMP1file
		echo
	fi
fi | tee -a  $OUTfile

if $DATBASE_HOST
then
	SET_SEED

	INPUTstring=`SHUFFLE in`
	OUTPUTstring=`SHUFFLE out`

	OBSCUREfunction="create function pg_temp.obscure(text) returns text as \$\$ select lower(translate(lower(translate(lower(translate(\$1, '$CHARset', '$INPUTstring')), '$CHARset' , '$OUTPUTstring')), '$CHARset', '$INPUTstring'))  \$\$ language sql"
fi

if $IMPALA_HOST
then
        LASTapplied=`impala-shell -d caspida -B --quiet -q "select max(distinct eventtime) from semiaggr_s;" 2>/dev/null`
        LAST_4days=`impala-shell -d caspida -B --quiet -q "select days_sub('$LASTapplied',3);" 2>/dev/null`
	export LASTapplied LAST_4days
fi

if $DATABASE_HOST
then
	GET_CASPIDA_PROPERTY persistence.anomalies.trashed.maintain.days
        MAINTAINdays="$PROPERTYvalue"

	if expr "$MAINTAINdays" >= 90 >/dev/null 2>&1
	then
		:
	else
		MAINTAINdays=90
	fi
	RECENT_days=`psql -d caspidadb -t -c "select current_date - interval '14 days';" 2>/dev/null | sed -e 's/^ //'`
        export RECENT_days
	ANCIENT_days=`psql -d caspidadb -t -c "select current_date - interval '$MAINTAINdays days';" 2>/dev/null | sed -e 's/^ //'`
        export ANCIENT_days MAINTAINdays
fi

#
#	Script is run from the 'leader', the first node in a cluster. 
#
#	When multi-node, script is automatically copied to other nodes
#

(

	CASPIDAhome=`( cd ~caspida/bin; pwd )`
	

        ls -ld /opt/caspida/etc/sudoers.d/ubasudoers_simple /opt/caspida/etc/sudoers.d/ubasudoers /etc/sudoers.d/ubasudoers > $TMP1file 2>/dev/null
        if test -f $TMP1file
        then
                echo
                NECHO "ubasudoers info:" "`head -1 $TMP1file`"; echo
                ALL_BUT_FIRST $TMP1file
        fi

	for U in caspida sudo
	do
		if test $U = "caspida"
		then
			Upath=`printenv PATH 2>/dev/null`
		else
			Upath=`sudo printenv PATH 2>/dev/null`
		fi

		echo
		NECHO "path check (${U}):" "$Upath"

        	ERROR=""
        	ERRdelim=""
		reqSECUREPATH="/sbin:/bin:/usr/sbin:/usr/bin"
        	REQpath=`echo "$reqSECUREPATH" | tr ":" " "` 
        	for REQdir in $REQpath
        	do
                	case ":$Upath:/home" in
                        	*:$REQdir:*);;
                        	*) ERROR="${ERROR}${ERRdelim}$REQdir not in PATH"; ERRdelim=", ";
                	esac
        	done
        	if test "$ERROR" != ""
        	then
                	echo "    ($ERROR; PATH may need to be corrected)"
        	else
                	echo
        	fi
	 
        	RMcmd="curl svc nproc mpstat dmidecode uptime nslookup openssl shuf wget timeout dig ifconfig ip top vmstat pidstat netstat mailx sendmail lsof impala-shell w3m java ntpdate hostnamectl timedatectl zip md5sum ss nc netcat killall script slabtop getenforce sestatus jps"

        	echo
        	NECHO "cmd check (${U})..." ""; echo
        	for CMD in $RMcmd
        	do
			PATH=$Upath which $CMD > $TMP1file 2>/dev/null
                	if grep -q '/' $TMP1file 
                	then
				:
			else
                        	echo "(not in PATH)" > $TMP1file
                	fi

                	NECHO "  ${CMD}:" "`head -1 $TMP1file`"; echo
        	done | sort
        	echo
	
		for PYcmd in python python2 python3
		do 
			PYpath=`PATH=$Upath which $PYcmd 2>/dev/null`
			if test "$PYpath" = ""
			then
				echo "$PYcmd: not in PATH"
			else
				PYsym=`stat -c "%N" $PYpath`
				echo -n "$PYcmd: $PYsym;  "
				$PYpath -c 'import pexpect' >/dev/null 2>&1
				if test $? -eq 0
				then
					echo -n "pexpect: module found;  PS1: "
					echo "$PS1"
				else
					echo "pexpect: module not found"
				fi
			fi
		done > $TMP1file

		NECHO "python check (${U}):" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
		echo
	done

        openssl version > $TMP1file 2>/dev/null
        if test -s $TMP1file
        then
                NECHO "openssl version" "`head -1 $TMP1file`"; echo
                echo
        fi

	hostnamectl status > $TMP1file 2>/dev/null
	if test -s $TMP1file
	then
		NECHO "hostnamectl status:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
		echo
	fi
	
	HOSTip=`hostname -i 2>/dev/null`
	NECHO "hostip:" "$HOSTip"; echo
	
	echo
	egrep -e '^[0-9]+\.' /etc/hosts > $TMP1file
	NECHO "/etc/hosts:" "`head -1 $TMP1file`"; echo
	ALL_BUT_FIRST $TMP1file

        echo
        egrep -v '^#' /etc/resolv.conf > $TMP1file 2>&1
        NECHO "/etc/resolv.conf:" "`head -1 $TMP1file`"
        if ! egrep -qe '^search |^domain' $TMP1file
        then
                echo "   <= without domain or search, nslookup shortname is expected to fail"
        else
                echo 
        fi

        ALL_BUT_FIRST $TMP1file

        LOCALHOSTip=""          # start as empty string
        if $NSLOOKUP
        then
                LOCALHOSTname=`nslookup localhost 2>/dev/null | egrep "^Name" | cut -d ':' -f 2 | sed -e "s/\s\s*//g"`
                if test "$LOCALHOSTname" = ""
                then
                        LOCALHOSTname="localhost"
                fi
                LOCALHOSTip=`nslookup $LOCALHOSTname 2>/dev/null | egrep -e "^Address" | grep -v '#' |cut -d ':' -f 2 | sed -e "s/\s\s*//g"`
                LOOKprog="nslookup"
        else
                if which dig >/dev/null 2>$1
                then
                        DIGanswer=`dig -4 +search localhost | sed -n -e '/^;; ANSWER SECTION/,+1 p' | tail -1 | sed -e 's/\s\s*/ /g' -e 's/\. / /' | cut -d ' ' -f 1,5`
                        LOCALHOSTname=`echo $DIGanswer | cut -d ' ' -f 1`
                        LOCALHOSTip=`echo $DIGanswer | cut -d ' ' -f 2`
                        LOOKprog="dig"
                fi
        fi
        if test "$LOCALHOSTip" = ""
        then
                LOCALHOSTname="localhost"
                LOCALHOSTip=`getent ahosts localhost | egrep '\.' | grep -m 1 "RAW" | sed -e 's/\s\s*/ /g' -e 's/ RAW\s*//'`
                LOOKprog="getent"
        fi
        echo
        NECHO "localhost ($LOOKprog):" "name='$LOCALHOSTname', ip='$LOCALHOSTip'"
        if test "$LOCALHOSTip" != "127.0.0.1"
        then
                if $uba43feature
		then
			FLAG="<="
		else
			FLAG="<=="
			INCREMENT_EXCEPTIONS
		fi
                echo "   $FLAG localhost does not resolve to '127.0.0.1'"
        else
                echo
        fi

	notCertified=false
	CERTIFIED=false

        OSrelease="`lsb_release -ds 2>/dev/null | tr -d '\"'| sed -e 's/ *$//'`"

	case "$OSrelease" in
		"Ubuntu 14.04.2 LTS")						CERTIFIED=true;;
		"Ubuntu 14.04.3 LTS")						CERTIFIED=true;;
		"Ubuntu 14.04.5 LTS")						CERTIFIED=true;;
		"Ubuntu 16.04.3 LTS")						CERTIFIED=true;;
		"Ubuntu 16.04.4 LTS")						CERTIFIED=true;;
		"Ubuntu 16.04.5 LTS")						CERTIFIED=true;;
		"Ubuntu 16.04.6 LTS")						CERTIFIED=true;;
		"Red Hat Enterprise Linux Server release 6.7 (Santiago)")	CERTIFIED=true;;
		"Red Hat Enterprise Linux Server release 7.2 (Maipo)")		RHELrelease72=true; CERTIFIED=true;;
                "Red Hat Enterprise Linux Server release 6.9 (Santiago)")       RHELrelease=true;;
                "Red Hat Enterprise Linux Server release 7.3 (Maipo)")          RHELrelease72=true;;
                "Red Hat Enterprise Linux Server release 7.4 (Maipo)")          RHELrelease72=true;;
                "Red Hat Enterprise Linux Server release 7.5 (Maipo)")          RHELrelease72=true;;
                "Red Hat Enterprise Linux Server release 7.6 (Maipo)")          RHELrelease72=true;;
		"CentOS Linux release 7.2.1511 (Core)") 			RHELrelease72=true; CERTIFIED=true;;
                "CentOS Linux release 7.3.1611 (Core)")                         RHELrelease72=true;;
		"CentOS Linux release 7.4.1708 (Core)")                 	RHELrelease72=true;;
		"CentOS Linux release 7.5.1804 (Core)")                 	RHELrelease72=true;;
		"CentOS Linux release 7.6.1810 (Core)")                 	RHELrelease72=true;;
		Oracle*7.4*)	RHELrelease72=true;;
		Oracle*7.5*)	RHELrelease72=true;;
		Oracle*7.6*)	RHELrelease72=true;;
	esac

	if $uba33feature && ! $uba40feature
	then
        	case "$OSrelease" in
                	"Red Hat Enterprise Linux Server release 6.9 (Santiago)")       CERTIFIED=true;;
                	"Red Hat Enterprise Linux Server release 7.3 (Maipo)")          RHELrelease72=true; CERTIFIED=true;;
                	"CentOS Linux release 7.3.1611 (Core)")                         RHELrelease72=true; CERTIFIED=true;;
        	esac
	fi

	if $uba40feature && ! $uba41feature
	then
		CERTIFIED=false
        	case "$OSrelease" in
			"Ubuntu 16.04.3 LTS")						CERTIFIED=true;;
			"Ubuntu 16.04.4 LTS")						CERTIFIED=true;;
			"Ubuntu 16.04.5 LTS")						CERTIFIED=true;;
			"Ubuntu 16.04.6 LTS")						CERTIFIED=true;;
                	"Red Hat Enterprise Linux Server release 6.9 (Santiago)")       CERTIFIED=true;;
                	"Red Hat Enterprise Linux Server release 7.3 (Maipo)")          RHELrelease72=true; CERTIFIED=true;;
        	esac
	fi

	if $uba41feature
	then
		CERTIFIED=false
        	case "$OSrelease" in
			"Red Hat Enterprise Linux Server release 7.4 (Maipo)")RHELrelease72=true; CERTIFIED=true;;
			"Red Hat Enterprise Linux Server release 7.5 (Maipo)")RHELrelease72=true; CERTIFIED=true;;
			"CentOS Linux release 7.4.1708 (Core)")                 RHELrelease72=true; CERTIFIED=true;;
			"CentOS Linux release 7.5.1804 (Core)")                 RHELrelease72=true; CERTIFIED=true;;
                        "Ubuntu 16.04.3 LTS")                                   CERTIFIED=true;;
                        "Ubuntu 16.04.4 LTS")                                   CERTIFIED=true;;
                        "Ubuntu 16.04.5 LTS")                                   CERTIFIED=true;;
                        "Ubuntu 16.04.6 LTS")                                   CERTIFIED=true;;
			Oracle*7.4*)	RHELrelease72=true; RHELrelease=true;;
			Oracle*7.5*)	RHELrelease72=true; RHELrelease=true;;
        	esac
	fi

	if $uba42feature
	then
		case "$OSrelease" in
			"CentOS Linux release 7.6.1810 (Core)") RHELrelease72=true; CERTIFIED=true;;
			"Red Hat Enterprise Linux Server release 7.6 (Maipo)")RHELrelease72=true; CERTIFIED=true;;
			Oracle*7.5*)	RHELrelease72=true; CERTIFIED=true;;
			Oracle*7.6*)	RHELrelease72=true; CERTIFIED=true;;
		esac
	fi

	if $uba43feature
	then
		CERTIFIED=false
		case "$OSrelease" in
			"CentOS Linux release 7.6.1810 (Core)") RHELrelease72=true; CERTIFIED=true;;
			"Red Hat Enterprise Linux Server release 7.6 (Maipo)")	  RHELrelease72=true; CERTIFIED=true;;
			Oracle*7.6*)						  RHELrelease72=true; CERTIFIED=true;;
                        "Ubuntu 16.04.3 LTS")                                     CERTIFIED=true;;
                        "Ubuntu 16.04.4 LTS")                                     CERTIFIED=true;;
                        "Ubuntu 16.04.5 LTS")                                     CERTIFIED=true;;
                        "Ubuntu 16.04.6 LTS")                                     CERTIFIED=true;;
		esac
	fi

	if $uba50feature
	then
		CERTIFIED=false
		case "$OSrelease" in
			"CentOS Linux release 7.7.1908 (Core)") RHELrelease72=true; CERTIFIED=true;;
			"Red Hat Enterprise Linux Server release 7.7 (Maipo)")	  RHELrelease72=true; CERTIFIED=true;;
			Oracle*7.7*)						  RHELrelease72=true; CERTIFIED=true;;
                        "Ubuntu 16.04.3 LTS")                                     CERTIFIED=true;;
                        "Ubuntu 16.04.4 LTS")                                     CERTIFIED=true;;
                        "Ubuntu 16.04.5 LTS")                                     CERTIFIED=true;;
                        "Ubuntu 16.04.6 LTS")                                     CERTIFIED=true;;
		esac
	fi


	if $uba503feature
	then
		CERTIFIED=false
		case "$OSrelease" in
			"CentOS Linux release 7.7.1908 (Core)") RHELrelease72=true; CERTIFIED=true;;
			"CentOS Linux release 7.8."*) RHELrelease72=true; CERTIFIED=true;;
			"CentOS Linux release 7.9."*) RHELrelease72=true; CERTIFIED=false;;
			"Red Hat Enterprise Linux Server release 7.7 (Maipo)")	  RHELrelease72=true; CERTIFIED=true;;
			"Red Hat Enterprise Linux Server release 7.8 (Maipo)")	  RHELrelease72=true; CERTIFIED=true;;
			"Red Hat Enterprise Linux Server release 7.9 (Maipo)")	  RHELrelease72=true; CERTIFIED=false;;
			Oracle*7.7*)						  RHELrelease72=true; CERTIFIED=true;;
                        "Ubuntu 16.04.3 LTS")                                     CERTIFIED=true;;
                        "Ubuntu 16.04.4 LTS")                                     CERTIFIED=true;;
                        "Ubuntu 16.04.5 LTS")                                     CERTIFIED=true;;
                        "Ubuntu 16.04.6 LTS")                                     CERTIFIED=true;;
		esac
	fi

	if $uba51feature
	then
		CERTIFIED=false
 		case "$OSrelease" in
			"Red Hat Enterprise Linux release 8.2 (Ootpa)") RHELrelease72=true; CERTIFIED=true;;
			"Red Hat Enterprise Linux Server release 7.9 (Maipo)")    RHELrelease72=true; CERTIFIED=true;;
			"CentOS Linux release 7.9."*) RHELrelease72=true; CERTIFIED=true;;
			"CentOS Linux release 8.2."*) RHELrelease72=true; CERTIFIED=true;;
                        "Ubuntu 18.04.5 LTS")                                     CERTIFIED=true;;
		esac
	fi

	case "$OSrelease" in
		CentOS*) RHELrelease=true;;
		Red*) RHELrelease=true;;
		Oracle*) RHELrelease=true;;
		Ubuntu*) RHELrelease=false;;
	esac

	if $RHELrelease
	then
		echo "$OSrelease" > $RHEL_OS_file
	fi

	if $CERTIFIED
	then
		notCertified=false
	else
		notCertified=true
	fi

	echo
        NECHO "release:" "'$OSrelease'" 
	if $notCertified
	then
		UBArelease="`cat $UBAfile`"
		echo "   <= not certified for UBA '$UBArelease'"
	else
		echo
	fi

	SELINUXfile="/etc/selinux/config"

	if test -f "$SELINUXfile"
	then
		echo
		ls -l /etc/sysconfig/selinux "$SELINUXfile" > $TMP1file 2>&1
		if test -s $TMP1file
		then
			NECHO "selinux config:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		fi
		SELINUX=`egrep "^SELINUX=" $SELINUXfile | cut -d "=" -f 2`
		NECHO "${SELINUXfile}:" "SELINUX=${SELINUX}"
		case "$SELINUX" in
			disabled)	echo;;
			permissive)	echo;;
			*)		echo "   <= 'disabled' or 'permissive' expected";;
		esac

		getenforce > $TMP1file 2>/dev/null
		if test -s $TMP1file
		then
			if grep -qi "enforcing" $TMP1file
			then
				ERROR="   <== 'disabled' or 'permissive' expected"
				INCREMENT_EXCEPTIONS
			else
				ERROR=""
			fi
			NECHO "getenforce:" "`head -1 $TMP1file`"; echo "$ERROR"
		fi

		sestatus -v  > $TMP1file 2>/dev/null
		
		if test -s $TMP1file
		then
			STATUSconfig=`egrep "^Mode from config file:" $TMP1file | sed -e 's/^..*:\s\s*//'`
			STATUScurrent=`egrep "^Current mode:" $TMP1file | sed -e 's/^..*:\s\s*//'`
			if test "$STATUSconfig" != "$STATUScurrent"
			then
				sed -i -e '/^Current mode:/ s/$/   <x= does not match config/' $TMP1file
			fi
			NECHO "sestatus:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		fi

		if selinuxenabled >/dev/null 2>&1
		then
			NECHO "selinuxenabled:" "'true'"
			
			BOOTtimeReadable=`who -b 2>/dev/null| sed -e 's/\s\s*/ /g' -e 's/ system boot //'`

			ERROR=""
			if expr 0 + "$BOOTtimeReadable" >/dev/null 2>&1
			then
				BOOTtimeEpoch=`date --date "$BOOTtimeReadable" +%s 2>/dev/null`
				if expr 0 + "$BOOTtimeEpoch"  >/dev/null 2>&1
				then
					SELINUXtimeMod=`stat -c '%Z' $SELINUXfile 2>/dev/null`
					if expr 0 + "$SELINUXtimeMod" >/dev/null 2>&1
					then
						if test "$SELINUXtimeMod" -gt "$BOOTtimeEpoch"
						then
							ERROR="   <= $SELINUXfile modifed since re-boot"
						fi
					
					fi
				fi
			fi
			echo "$ERROR"
		fi
	fi

	if test -r /sys/kernel/mm/transparent_hugepage/enabled
	then
		echo
		NECHO "THP enabled:" ""
		cat /sys/kernel/mm/transparent_hugepage/enabled
	fi

	if test -r /proc/sys/crypto/fips_enabled
	then
		echo
		FIPSval=`cat /proc/sys/crypto/fips_enabled`
		NECHO "fips enabled:" "$FIPSval"; echo
		if test "$FIPSval" = "1"
		then
			export OPENSSL_FIPS=1		# let openssl know
		fi
	fi

	echo
	java -version > $TMP1file 2>&1 
	NECHO "java version:" "`head -1 $TMP1file`";echo
	ALL_BUT_FIRST $TMP1file

	echo
	JAVA_SECURITY="`find -L  $JAVA_HOME -name java.security 2>/dev/null`"
	NECHO "java security:" "$JAVA_SECURITY"; echo
	sed -n '/^jdk.tls.disabledAlgorithms/,/^$/p' $JAVA_SECURITY 2>/dev/null > $TMP1file
	if test -s $TMP1file
	then
		NECHO "" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	fi

	curl --version > $TMP1file 2>&1 
	NECHO "curl version:" "`head -1 $TMP1file`";echo
	ALL_BUT_FIRST $TMP1file

	echo
	sudo -n dmidecode -t system 2>/dev/null | grep 'Manufacturer\|Product' > $TMP1file
        NECHO "Manufacturer:" "`grep "Manufacturer" $TMP1file | sed -e 's/^.*: //'`"; echo
        NECHO "Product:" "`grep "Product" $TMP1file | sed -e 's/^.*: //'`"; echo
        BogoMIPS=`grep -m 1 "^bogomips" /proc/cpuinfo | tr -d ' ' | cut -d ':' -f 2`
        NECHO "bogoMIPS:" "$BogoMIPS"; echo             # may be meaningless but a data point nonetheless
	
	NECHO "cpu/vmem observation:" ""
	( exec $CPUTEST	)				# simple test of cpu and virtual memory

	echo
	IPV6disabled=`cat /proc/sys/net/ipv6/conf/all/disable_ipv6 2>/dev/null`
        NECHO "disable_ipv6:" "'$IPV6disabled'"
	echo
        # if $RHELrelease && $uba41feature
        # then
                # if test "$IPV6disabled" = "0"
                # then
                        # echo
                # else
                        # echo "   <= expecting /proc/sys/net/ipv6/conf/all/disable_ipv6 to be '0'"
                # fi
        # else
                # echo
        # fi

        echo
        IPforward=`cat /proc/sys/net/ipv4/ip_forward 2>/dev/null`
        NECHO "ip_forward:" "/proc/sys/net/ipv4/ip_forward=$IPforward"
        if test "$IPforward" = "1"
        then
                echo
        else
		if $CONTAINER_MASTER || $CONTAINER_WORKER
		then
                	echo "   <= might need to be 1 (enabled) for containerization"
		else
			echo
		fi
        fi
	echo

	if egrep -q "bond0:" /proc/net/dev 2>/dev/null
	then
		INTERFACE="bond0"
	else 
		INTERFACE=`cat /proc/net/dev 2>/dev/null | grep ":" | egrep -ve "lo:|docker0:|flannel|veth|cni" | head -1 | sed -e 's/^\s\s*//' -e 's/:.*$//'`
	fi

	if $uba421feature
	then
		GET_CASPIDA_PROPERTY system.docker.networkcidr
		if !  test "$PROPERTYvalue" = ""
		then 
			DOCKERcidr="$PROPERTYvalue"
		fi
	fi

	if $uba41feature 
	then
		GET_CASPIDA_PROPERTY system.network.interface
                NETWORK_INTERFACE="$PROPERTYvalue"
		NECHO "properties:" "system.network.interface=$NETWORK_INTERFACE"; echo
		if test "$NETWORK_INTERFACE" != ""
		then
			INTERFACE=$NETWORK_INTERFACE
		fi
	fi
		
	if test -z "$INTERFACE"
	then
	       	if which ifconfig >/dev/null 2>&1
	       	then
	                INTERFACE=`sudo -n ifconfig -s  2>/dev/null | grep BMRU |  cut -d ' ' -f 1`
		fi
	fi

	if test -z "$INTERFACE"
	then
		if which ip >/dev/null 2>&1
		then
	       		INTERFACE=`ip addr | grep -v SLAVE | grep UP |  grep -m 1 BROADCAST | tr -d ' ' | cut -d ':' -f 2`
	        fi
	fi
	
	
	NECHO "interface:" "'$INTERFACE'"
	if test -z "$INTERFACE"
	then
		INTERFACE="eth0"	# best guess
		echo "   <= assuming 'eth0'"
	else
		echo
	fi

	SPEED=`cat /sys/class/net/$INTERFACE/speed 2>/dev/null`
	if test $? -eq 0
	then
		NECHO "$INTERFACE:" "$SPEED Mb/s"
		if test "$SPEED" -lt "$minSPEED"
		then
			echo "   <== less than $minSPEED Mb/s"
			INCREMENT_EXCEPTIONS
		else
			echo
		fi
	else
		NECHO "${INTERFACE}:" "indeterminate Mb/s"; echo	# some virtual machines do not report speed
	fi

	ifconfig $INTERFACE > $TMP1file 2>/dev/null
	if test -s $TMP1file
	then
		echo
		NECHO "ifconfig $INTERFACE:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	else
		ip -s link show dev $INTERFACE > $TMP1file 2>/dev/null
        	if test -s $TMP1file
        	then
                	echo
                	NECHO "ip show $INTERFACE:" "`head -1 $TMP1file`"; echo
                	ALL_BUT_FIRST $TMP1file
		fi
	fi
	
 	echo
 	NECHO "/dev/tty:" ""
 	ls -l /dev/tty | tail -1

	#
	#	Check number and size of disks
	#

	if $uba42feature
	then
		minDISK2r=`expr 2 \* $minDISK2r`
		minDISK2f=`expr 2 \* $minDISK2f`
		minAVAILdisk2=`expr 2 \* $minAVAILdisk2`
	fi

	echo

	NUMdisks=`lsblk -ibnd --output NAME,RM,SIZE,TYPE -P 2>/dev/null | grep 'RM="0"' | sort | tee $TMP1file | wc -l`

	if test -z "$NUMdisks"
	then
		NUMdisks=0
	fi

	NECHO "disks:" "$NUMdisks"
	if test "$NUMdisks" -lt "$reqDISKs"
	then
		echo "   <== does not meet $reqDISKs disk requirement"
		INCREMENT_EXCEPTIONS 1 11		
	else
		echo
	fi

	if test "$NUMdisks" -eq "$reqDISKs"
	then
		SIZE="`head -1 $TMP1file 2>/dev/null | sed -e 's/^.*SIZE="//' -e 's/".*$//'`"
		if test -z "$SIZE"
		then
			SIZE="0"
		else
			SIZE=`expr $SIZE / 1024 / 1024 / 1024`
		fi
		NECHO "disk1 size:" "$SIZE GB"
		if test "$SIZE" -lt "$minDISK1r"
		then
			echo "   <== smaller than required $minDISK1r GB"
			INCREMENT_EXCEPTIONS 1 12		
		else
			echo
		fi
		SIZE="`tail -1 $TMP1file 2>/dev/null | sed -e 's/^.*SIZE="//' -e 's/".*$//'`"
		if test -z "$SIZE"
		then
			SIZE="0"
		else
			SIZE=`expr $SIZE / 1024 / 1024 / 1024`
		fi
		NECHO "disk2 size:" "$SIZE GB"
		if test "$SIZE" -lt "$minDISK2r"
		then
			echo "   <== smaller than required $minDISK2r GB"
			INCREMENT_EXCEPTIONS		1 13
		else
			echo
		fi
	else
		lsblk -d --output NAME,RM,SIZE,TYPE 2>/dev/null |\
		grep -v ' 1 ' |\
		sed "s/^/$INDENT    /" 	# simply list the disks 
	fi

	#
	#	Verify partition space
	#
	
	echo
        df -h -x tmpfs -x devtmpfs -P > $TMP1file 2>&1

	GET_CASPIDA_PROPERTY system.disk.usage.high.watermark
        HIGHwater="$PROPERTYvalue"
	if test "$HIGHwater" = ""
	then
		HIGHwater=90
	fi
	GET_CASPIDA_PROPERTY system.disk.usage.low.watermark
        LOWwater="$PROPERTYvalue"
	if test "$LOWwater" = ""
	then
		LOWwater=75
	fi
	MIDwater=`expr \( \( $HIGHwater - $LOWwater \) / 2 \) + $LOWwater`

	df --output=source,pcent /home/caspida /etc/caspida /var/vcap /var/vcap2 2>/dev/null |\
	egrep -e '^/dev/' | sed -e 's/\s\s*/ /g' -e 's/%//' -e 's+/+\.+g' |\
	while read DEV AVAIL
	do 
		if expr "$AVAIL" + 0 >/dev/null 2>/dev/null
		then
			AVAIL=0
		fi
		if test $AVAIL -ge $HIGHwater
		then
			sed -ie "/^$DEV/ s/\$/   <== usage ${HIGHwater}% or greater; space must be recovered/" $TMP1file
			INCREMENT_EXCEPTIONS
		else
			if test $AVAIL -ge $MIDwater
			then
				sed -ie "/^$DEV/ s/\$/   <= usage approaching ${HIGHwater}%; monitor closely/" $TMP1file
			fi
		fi
	done

        NECHO "partitions:" "`head -1 $TMP1file`"; echo
        ALL_BUT_FIRST $TMP1file

	df -x tmpfs -x devtmpfs -i > $TMP1file 2>&1

	df --output=source,ipcent /home/caspida /etc/caspida /var/vcap /var/vcap2 2>/dev/null |\
        egrep -e '^/dev/' | sed -e 's/\s\s*/ /g' -e 's/%//' -e 's+/+\.+g' |\
        while read DEV AVAIL
        do
                if test $AVAIL -ge 75
                then
                        sed -ie "/^$DEV/ s/\$/   <== usage 75% or greater; inodes must be recovered/" $TMP1file
                        INCREMENT_EXCEPTIONS
                else
                        if test $AVAIL -ge 20
                        then
                                sed -ie "/^$DEV/ s/\$/   <= usage 20%; monitor closely/" $TMP1file
                        fi
                fi
        done

	if test -s $TMP1file
	then
		echo
		NECHO "inodes:" "`head -1 $TMP1file`"; echo
        	ALL_BUT_FIRST $TMP1file
	fi

        echo
        mount -t ext4 > $TMP1file 2>&1
        NECHO "disk mounts:" "`head -1 $TMP1file`"; echo
        ALL_BUT_FIRST $TMP1file

        egrep "\sro\b" /proc/mounts > $TMP1file 2>&1
        if test -s $TMP1file
        then
		if grep -q '/var/vcap2' $TMP1file
                then
                        sed -i '/\/var\/vcap2/ s/$/   <== \/var\/vcap2 must not be mounted read-only/' $TMP1file
                        INCREMENT_EXCEPTIONS
                fi
		if grep -q '/var/vcap ' $TMP1file
                then
                        sed -i '/\/var\/vcap / s/$/   <== \/var\/vcap must not be mounted read-only/' $TMP1file
                        INCREMENT_EXCEPTIONS
                fi
                echo

                NECHO "read-only fs:" "`head -1 $TMP1file`"; echo
                ALL_BUT_FIRST $TMP1file
		echo
        fi

	sudo /usr/sbin/sshd -T 2>&1 |\
	tee $TMP2file |\
	egrep -i 'maxsessions|permittty|usedns|strictmodes|usepam|hostbasedauthentication|ignoreuserknownhosts|passwordauthentication|authorizedkeysfile|permituserrc|setenv|pubkeyauthentication' |\
	sort > $TMP1file
	if ! test -s $TMP1file
	then
		echo "(no matches)" > $TMP1file
		cat $TMP2file >> $TMP1file

                echo >> $TMP1file
                ls -l /etc/ssh/sshd_config 2>/dev/null >> $TMP1file
                sed -e '/^#/d' -e '/^\s*$/d' < /etc/ssh/sshd_config 2>&1 >> $TMP1file
	fi
	echo
	NECHO "sshd -T:" "`head -1 $TMP1file`" ; echo
	ALL_BUT_FIRST $TMP1file

	SSHconnections=`ps -elf | grep '[s]shd:' | wc -l | sed -e 's/\s\s*//g'`
	echo
	NECHO "sshd connections:" "'$SSHconnections'" ; echo

        echo
        ls -laR ~/.ssh > $TMP1file 2>/dev/null
        NECHO "~/.ssh:" "`head -1 $TMP1file`"; echo
        ALL_BUT_FIRST $TMP1file

	echo
	ls -l ~/.profile ~/.bash_login ~/.bash_profile ~/.bashrc 2>/dev/null > $TMP1file
	if ! test -s $TMP1file
	then
		echo "(no profile)" > $TMP1file
	fi
	NECHO "profile:" "`head -1 $TMP1file`"; echo
	ALL_BUT_FIRST $TMP1file

	ERROR=""
	grep -H 'no_pass_security' /etc/pam.d/* 2>/dev/null > $TMP1file
	if ! test -s $TMP1file
	then
		echo "   (not found)"  > $TMP1file 
		chage -l caspida 2>/dev/null | grep -i '^Password expires'> $TMP2file
		if grep -qiv never $TMP2file
		then
			ERROR="   <= make sure that 'caspida' password does not expire" 
		fi
	fi
	echo
	NECHO "no_pass_security:" "`head -1 $TMP1file`"; echo "$ERROR"
	ALL_BUT_FIRST $TMP1file

	echo
	PWD="`pwd`"	# save current directory
	
	if $ZOOKEEPER_SERVER
	then
		if cd $ZOOKEEPERdir >/dev/null 2>&1
		then
			ls -ld $ZOOKEEPERdir >$TMP1file 2>/dev/null
			NECHO "${ZOOKEEPERdir}:" "`cat $TMP1file`"
		
			# '.' is the current directory, will follow symbolic link
		
			ls -ld . >$TMP2file 2>/dev/null
			if grep -q " zookeeper zookeeper " $TMP2file >/dev/null 2>&1
			then
				echo
		else
				echo "   <== owner/group are not 'zookeeper'"
				INCREMENT_EXCEPTIONS 1 14
			fi
	
			#
			#	make sure that there's space on /var/lib for zookeeper
			#
		
			AVAIL=`df . | egrep -ve '^Filesystem' | sed -r 's/^(\S)/\n\1/' | sed -n '/^\S/,/^$/p' | sed -e :a -e '$!N; s/\n//; ta' | sed -e 's/  */ /g' | cut -d ' ' -f 4`
			if ! test -z "$AVAIL"
			then
				AVAIL=`expr $AVAIL / 1024 / 1024` # reduce to GB
	
				NECHO "${ZOOKEEPERdir}:" "available: $AVAIL GB"
				if test $AVAIL -lt $minAVAILdisk1
				then
					halfAVAIL=`expr $minAVAILdisk1 / 4`
					if test $AVAIL -lt $halfAVAIL
					then
						echo "   <== ${ZOOKEEPERdir} less that ${minAVAILdisk1} GB"
						INCREMENT_EXCEPTIONS 1 15
					else
						echo "   <= ${ZOOKEEPERdir} less that ${minAVAILdisk1} GB"
					fi
					df -h . | sed  -e "s/^/$INDENT/" 
				else
					echo
				fi
			fi
		else
			NECHO "${ZOOKEEPERdir}:" "$ZOOKEEPERdir   <== does not exist"; echo
			INCREMENT_EXCEPTIONS 1 16
		fi
	fi
	
	if cd $CASPIDAdir >/dev/null 2>&1
	then
		ls -ld $CASPIDAdir >$TMP1file 2>/dev/null
		NECHO "$CASPIDAdir:" "`cat $TMP1file`"
	
		# '.' is the current directory, will follow symbolic link
	
		TIMESTAMP_OptCaspida=`stat --format='%Z' . 2>/dev/null`		# value used later 

		ls -ld . >$TMP2file 2>/dev/null
		if grep -q " caspida caspida " $TMP2file >/dev/null 2>&1
		then
			echo
		else
			echo "   <== owner/group are not 'caspida'"
			INCREMENT_EXCEPTIONS 1 16
		fi
	
		#
		#	make sure that there's space on /opt/caspida
		#
	
		AVAIL=`df . | egrep -ve '^Filesystem' | sed -r 's/^(\S)/\n\1/' | sed -n '/^\S/,/^$/p' | sed -e :a -e '$!N; s/\n//; ta' | sed -e 's/  */ /g' | cut -d ' ' -f 4`
		if ! test -z "$AVAIL"
		then
			AVAIL=`expr $AVAIL / 1024 / 1024` # reduce to GB
	
			NECHO "$CASPIDAdir:" "available: $AVAIL GB"
			if test $AVAIL -lt $minAVAILdisk1
			then
				halfAVAIL=`expr $minAVAILdisk1 / 4`	# 25% actually
                                if test $AVAIL -lt $halfAVAIL
                                then
					echo "   <== $CASPIDAdir less that ${minAVAILdisk1} GB"
					INCREMENT_EXCEPTIONS 1 17
				else
					halfAVAIL=`expr $minAVAILdisk1  + $minAVAILdisk1`  # now 50%
					if test $AVAIL -lt $halfAVAIL
					then
						echo "   <= $CASPIDAdir less that ${minAVAILdisk1} GB"
					else
						echo "   ($CASPIDAdir less that ${minAVAILdisk1} GB)"
					fi
				fi
				df -h . | sed  -e "s/^/$INDENT/" 
			else
				echo
			fi
		fi
	else
		NECHO "$CASPIDAdir:" "   <== does not exist"; echo
		INCREMENT_EXCEPTIONS 1 18
	fi

	for VARdir in /opt/caspida /etc/caspida/local /home/caspida /tmp /var /var/lib /var/log /var/opt /var/tmp
	do
		echo
		if test -d $VARdir
		then
			NECHO "$VARdir usage:" ""
			sudo du -Pxh -d 1 $VARdir 2>/dev/null |\
			egrep -ve '^0\s' |\
			egrep -ve '^4.0K\s' |\
			egrep -ve '^8.0K\s' |\
			egrep -ve '^..K\s' > $TMP1file
			if ! test -s $TMP1file
			then
				echo "(minimal)" > $TMP1file
			fi
				
			head -1 $TMP1file
			ALL_BUT_FIRST $TMP1file
		fi
	done

	ls -l /var/log/caspida/system/ > $TMP1file 2>/dev/null
	if test -s $TMP1file
	then
		echo
		NECHO "/var/log/caspida/system:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	fi

	echo

	
	cd "$PWD"	# restore current directory
	
	if $SPARK_MASTER || $SPARK_WORKER
	then
		if cd  /var/vcap2 2>/dev/null
		then
			df . | egrep -ve '^Filesystem' | sed -r 's/^(\S)/\n\1/' | sed -n '/^\S/,/^$/p' |\
			sed -e :a -e '$!N; s/\n//; ta' | sed -e 's/  */ /g' | cut -d ' ' -f 1,2,4,6 > $TMP1file 	# source size avail target
	
			PARTvcap=`cut -d ' ' -f 1 < $TMP1file`
			DEVvcap=`echo "$PARTvcap" | sed -e 's/[0-9]*$//'`
	
			DISK=`cut -d ' ' -f 2 < $TMP1file`	
			if test -z "$DISK"
			then
				DISK=0
			fi
	
			DISK=`expr $DISK / 1024 / 1024`	# reduce to GB
			NECHO "/var/vcap2:" "size: ${DISK} GB"
			if test "$DISK" -lt "$minDISK2f"
			then
				echo -n "   <== less than ${minDISK2f} GB"
				INCREMENT_EXCEPTIONS 1 19
			fi
	
			if ! grep -q "/var/vcap2" $TMP1file
			then
				echo "   <== not a separate mount"
				INCREMENT_EXCEPTIONS 1 20
			else
				echo
			fi
	
		        #
		        #       make sure that there's space on /var/vcap2
		        #
		
			AVAIL=`cut -d ' ' -f 3 < $TMP1file`
			if test -z "$AVAIL"
	                then
	                        AVAIL=0
	                fi
	
			if $uba40feature
			then
				:
				# minAVAILdisk2="`expr $minAVAILdisk2 \* 2`"
			fi
		        AVAIL=`expr $AVAIL / 1024 / 1024` # reduce to GB
		
		        NECHO "/var/vcap2:" "available: $AVAIL GB"
		        if test $AVAIL -lt $minAVAILdisk2
		        then
				halfAVAIL=`expr $minAVAILdisk2 / 2`
		        	if test $AVAIL -lt $halfAVAIL
				then
		                	echo -n "   <== /var/vcap2 less that ${minAVAILdisk2} GB"
		                	INCREMENT_EXCEPTIONS 1 21
				else
		                	echo -n "   <= /var/vcap2 less that ${minAVAILdisk2} GB"
				fi
				if $uba40feature
				then
					echo "; additional disk space advised for new models"
				else
					echo
				fi
				df -h . | sed  -e "s/^/$INDENT/" 
		        else
		                echo
		        fi
		fi
	fi

	cd "$PWD"	# restore current directory
	
	if cd  /var/vcap 2>/dev/null
	then
		df . | egrep -ve '^Filesystem' | sed -r 's/^(\S)/\n\1/' | sed -n '/^\S/,/^$/p' |\
		sed -e :a -e '$!N; s/\n//; ta' | sed -e 's/  */ /g' | cut -d ' ' -f 1,2,4,6 > $TMP1file 	# source size avail target

		PARTvcap=`cut -d ' ' -f 1 < $TMP1file`
		DEVvcap=`echo "$PARTvcap" | sed -e 's/[0-9]*$//'`

		DISK=`cut -d ' ' -f 2 < $TMP1file`	
		if test -z "$DISK"
		then
			DISK=0
		fi

		DISK=`expr $DISK / 1024 / 1024`	# reduce to GB
		NECHO "/var/vcap:" "size: ${DISK} GB"
		if test "$DISK" -lt "$minDISK2f"
		then
			echo -n "   <== less than ${minDISK2f} GB"
			INCREMENT_EXCEPTIONS 1 19
		fi

		if ! grep -q "/var/vcap" $TMP1file
		then
			echo "   <== not a separate mount"
			INCREMENT_EXCEPTIONS 1 20
		else
			echo
		fi

	        #
	        #       make sure that there's space on /var/vcap
	        #
	
		AVAIL=`cut -d ' ' -f 3 < $TMP1file`
		if test -z "$AVAIL"
                then
                        AVAIL=0
                fi

		if $uba40feature
		then
			minAVAILdisk2="`expr $minAVAILdisk2 \* 2`"
		fi
	        AVAIL=`expr $AVAIL / 1024 / 1024` # reduce to GB
	
	        NECHO "/var/vcap:" "available: $AVAIL GB"
	        if test $AVAIL -lt $minAVAILdisk2
	        then
			halfAVAIL=`expr $minAVAILdisk2 / 2`
	        	if test $AVAIL -lt $halfAVAIL
			then
	                	echo -n "   <== /var/vcap less that ${minAVAILdisk2} GB"
	                	INCREMENT_EXCEPTIONS 1 21
			else
	                	echo -n "   <= /var/vcap less that ${minAVAILdisk2} GB"
			fi
			if $uba40feature
			then
				echo "; additional disk space advised for new models"
			else
				echo
			fi
			df -h . | sed  -e "s/^/$INDENT/" 
	        else
	                echo
	        fi

		echo
		for VCAPdir in /var/vcap/store /var/vcap/sys/log /var/vcap/release_archives
		do
		echo
			if test -d $VCAPdir
			then
				NECHO "$VCAPdir usage:" ""
				sudo du -csh ${VCAPdir}/* 2>/dev/null |\
                        	egrep -ve '^0\s' |\
                        	egrep -ve '^4.0K\s' |\
                        	egrep -ve '^8.0K\s' |\
                        	egrep -ve '^..K\s' > $TMP1file
                        	if ! test -s $TMP1file
                        	then
                                	echo "(minimal)" > $TMP1file
                        	fi

				head -1 $TMP1file
				ALL_BUT_FIRST $TMP1file
			fi
		done
 
		echo
		CURRpwd=`pwd`
		NECHO "temporary spark usage:" ""
		if cd /var/vcap/sys/tmp/spark 2>/dev/null  # may be symbolic link
		then
			TEMPmp=`df . | tail -1 | sed -e 's/\s\s*/ /g' | cut -d ' ' -f 6` # spark temp mount point
			TEMPspark=`du -csh . | tail -1 | sed -e 's/\s\s*.*$//'`	
			if echo "$TEMPspark" | egrep -e '[3-9][0-9][0-9]G'
			then
				ERROR="<= spark is using '$TEMPspark' temporarily; monitor $TEMPmp availability"
			else
				ERROR="(using $TEMPmp)"
			fi
			echo "$TEMPspark   $ERROR"
			cd $CURRpwd
		fi

		echo
		CURRpwd=`pwd`
		NECHO "temporary impala usage:" ""
		if cd /var/vcap/sys/tmp/impala/ 2>/dev/null  # may be symbolic link
		then
			TEMPmp=`df . | tail -1 | sed -e 's/\s\s*/ /g' | cut -d ' ' -f 6` # impala temp mount point
			TEMPimpala=`du -csh . | tail -1 | sed -e 's/\s\s*.*$//'`	
			if echo "$TEMPimpala" | egrep -e '[3-9][0-9][0-9]G'
			then
				ERROR="<= impala is using '$TEMPimpala' temporarily; monitor $TEMPmp availability"
			else
				ERROR="(using $TEMPmp)"
			fi
			echo "$TEMPimpala   $ERROR"
			cd $CURRpwd
		fi

		OLDjobs=`find /var/vcap/sys/log/caspida -type f -name 'jobex*' -mtime +45 -exec echo -n -e {}"\0" \; | du -hc --files0-from=- | egrep -e 'total$' | sed -e 's/\s\s*.*$//'`
		if test "$OLDjobs" = ""
		then
			NECHO "jobexecutor logs usage:" "$OLDjobs"; echo
		fi
	else
		NECHO "disk:" "   <== /var/vcap does not exist"; echo
		INCREMENT_EXCEPTIONS 1 22
	fi

	cd "$PWD"	# restore current directory

	if $DATABASE_HOST
	then
		PSQLdir="/var/vcap/store/postgresql"
		if test -d /var/vcap/store/pgsql 
		then
			PSQLdir="$PSQLdir /var/vcap/store/pgsql"
		fi
		echo
		NECHO "postgresql total:" ""; 
		sudo du -sch $PSQLdir 2>/dev/null | tail -1 | cut -d ' ' -f 1
		sudo find $PSQLdir -type d -name pgsql_tmp -exec du -sh {} \; 2>/dev/null > $TMP1file
		if test -s $TMP1file
		then
			NECHO "postgresql temp:" "`head -1 $TMP1file`"; echo
		fi
	fi

	echo
	if quotaon -p -a >$TMP1file 2>/dev/null
	then
		quota >> $TMP1file 2>/dev/null	

	        USERlist="redis solr hbase sentry yarn mapred flume hdfs hive impala redis zookeeper postgres influxdb"
        	if ! $uba41feature
        	then
                	USERlist="$USERlist neo4j"
        	fi

		for U in $USERlist
		do
			sudo quota -u $U >>$TMP1file 2>/dev/null
		done

		sed -i -e '/^$/d' $TMP1file 	# remove empty lines

		if test -s $TMP1file
		then
			NECHO "disk quota:" "`head -1 $TMP1file`"; echo "   <= disk quotas are not expected"
			ALL_BUT_FIRST $TMP1file
		else
			NECHO "disk quota:" "(not evident)"; echo
		fi
	else
		NECHO "disk quota:" "(not announced)"; echo
	fi
) | tee -a $OUTfile
	
(
	echo
	if crontab -l 2> $TMP2file | egrep -v '^#|^ ' > $TMP1file		# show crontab -u caspida
	then 
		:		# it was successful
	else
		sudo crontab -u caspida -l 2> $TMP2file | egrep -v '^#|^ ' > $TMP1file
	fi

	if test -s $TMP1file
	then
		echo
		NECHO "caspida crontab:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
		echo
	else
		if $LEADERnode
		then
			if test -s $TMP2file
			then
				NECHO "caspida crontab:" "`head -1 $TMP2file`"; echo 
				ALL_BUT_FIRST $TMP2file
				echo
			fi
		fi
	fi

	VM_OVERCOMMIT_MEMORY=`cat /proc/sys/vm/overcommit_memory 2>/dev/null`
	NECHO "vm overcommit_memory:" "'$VM_OVERCOMMIT_MEMORY'"
	if test "$VM_OVERCOMMIT_MEMORY" != "1"
	then
		echo "   <== '1' is required value for proper performance"
		INCREMENT_EXCEPTIONS
	else
		echo
	fi

	VM_SWAPPINESS=`cat /proc/sys/vm/swappiness 2>/dev/null`

        REQUIREDswappiness="10"
        if $uba43feature
	then
		REQUIREDswappiness="1"
	fi
	NECHO "vm swappiness:" "'$VM_SWAPPINESS'"
	if test "$VM_SWAPPINESS" = $REQUIREDswappiness
	then
		echo
	else
		echo "   <= '$REQUIREDswappiness' is recommended value for proper performance"
	fi

	echo
	echo "false" > $SNODEfile
	free -mt > $TMP1file 2>/dev/null		# show physical  memory
	ERROR=""
	if test -s $TMP1file
	then
		set `grep "^Mem:" $TMP1file`
		BC=`expr $6 + $7`
		if grep "^Total:" $TMP1file > $TMP2file
		then
			VMtotal=`sed -e 's/\s\s*/ /g' < $TMP2file | cut -d ' ' -f 2`
			if test $VMtotal -gt $SUPERmem
			then
				echo "true" > $SNODEfile
			fi
			VMused=`sed -e 's/\s\s*/ /g' < $TMP2file | cut -d ' ' -f 3`
			VMused=`expr $VMused - $BC`		# buffers and cache are not used
			VMpercent=`expr ${VMused}000 / $VMtotal 2>/dev/null`
			if ! test -z "$VMpercent"
			then
				if test "$VMpercent" -gt 980
				then
					ERROR="   <== adjusted memory utilization is over 98%"
					sed -i -e "s/^\(Total:.*\)\$/\1$ERROR/" $TMP1file
					INCREMENT_EXCEPTIONS 1 22a
				else
					if test "$VMpercent" -gt 900
					then
						ERROR="   <== adjusted memory utilization is over 90%"
						sed -i -e "s/^\(Total:.*\)\$/\1$ERROR/" $TMP1file
					fi
				fi
			fi
		fi 
		echo
		NECHO "mem usage:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
		if ! test "$ERROR" = ""
		then
			echo
			vmstat 1 10 2>/dev/null | sed  -e "s/^/$INDENT/"
		fi
	fi

	SWAPsize=`free -g 2>/dev/null| egrep -ie swap | sed -e 's/\s\s*/ /g' | cut -d ' ' -f 2`
	if ! expr "$SWAPsize" + 0 >/dev/null 2>&1
	then
		SWAPsize=0
	fi

	if test $SWAPsize -gt 4
	then
		for PROCstat in /proc/*/status
		do
			awk -f $SWAPawk $PROCstat 2>/dev/null
		done > $TMP1file
		if test -s $TMP1file
		then
			echo
			NECHO "larger ($SWAPsize GB) swap usage:" "`head -1 $TMP1file`"; echo "   <= swap not recommended"
			ALL_BUT_FIRST $TMP1file
		fi
	fi

	base64 -d > ~/.tophealthrc	<<\EndOfFile
dG9waGVhbHRoJ3MgQ29uZmlnIEZpbGUgKExpbnV4IHByb2Nlc3NlcyB3aXRoIHdpbmRvd3MpCklk
OmksIE1vZGVfYWx0c2NyPTAsIE1vZGVfaXJpeHBzPTEsIERlbGF5X3RpbWU9My4wLCBDdXJ3aW49
MApEZWYJZmllbGRzY3VyPailpsu1s7TEuru9wLy2t7nFJykqKywtLi8wMTI4Pj9BQkNGR0hJSkxN
Tk9QUVJTVFVWV1hZWltcXV5fYGFiY2RlZmdoaWoKCXdpbmZsYWdzPTE2MTIwNCwgc29ydGluZHg9
MjIsIG1heHRhc2tzPTAsIGdyYXBoX2NwdXM9MCwgZ3JhcGhfbWVtcz0wCglzdW1tY2xyPTEsIG1z
Z3NjbHI9MSwgaGVhZGNscj0zLCB0YXNrY2xyPTEKSm9iCWZpZWxkc2N1cj2lprm3urO0xLu8vafF
KCkqKywtLi8wMTI1Njg+P0BBQkNGR0hJSktMTU5PUFFSU1RVVldYWVpbXF1eX2BhYmNkZWZnaGlq
Cgl3aW5mbGFncz0xOTM4NDQsIHNvcnRpbmR4PTAsIG1heHRhc2tzPTAsIGdyYXBoX2NwdXM9MCwg
Z3JhcGhfbWVtcz0wCglzdW1tY2xyPTYsIG1zZ3NjbHI9NiwgaGVhZGNscj03LCB0YXNrY2xyPTYK
TWVtCWZpZWxkc2N1cj2luru8vb6/wMHDxLO0t8UmJygpKissLS4vMDEyNTY4OUJGR0hJSktMTU5P
UFFSU1RVVldYWVpbXF1eX2BhYmNkZWZnaGlqCgl3aW5mbGFncz0xOTM4NDQsIHNvcnRpbmR4PTIx
LCBtYXh0YXNrcz0wLCBncmFwaF9jcHVzPTAsIGdyYXBoX21lbXM9MAoJc3VtbWNscj01LCBtc2dz
Y2xyPTUsIGhlYWRjbHI9NCwgdGFza2Nscj01ClVzcglmaWVsZHNjdXI9paanqKqwube6xMUpKywt
Li8xMjM0NTY4Ozw9Pj9AQUJDRkdISUpLTE1OT1BRUlNUVVZXWFlaW1xdXl9gYWJjZGVmZ2hpagoJ
d2luZmxhZ3M9MTkzODQ0LCBzb3J0aW5keD0zLCBtYXh0YXNrcz0wLCBncmFwaF9jcHVzPTAsIGdy
YXBoX21lbXM9MAoJc3VtbWNscj0zLCBtc2dzY2xyPTMsIGhlYWRjbHI9MiwgdGFza2Nscj0zCkZp
eGVkX3dpZGVzdD0wLCBTdW1tX21zY2FsZT0wLCBUYXNrX21zY2FsZT0wLCBaZXJvX3N1cHByZXNz
PTAKCg==
EndOfFile

	if ! test -L  ~/bin/tophealth
	then
		mkdir -p ~/bin/
		ln -s `which top` ~/bin/tophealth	
	fi
 
	base64 -d > ~/.topresidentrc	<<\EndOfFile
dG9waGVhbHRoJ3MgQ29uZmlnIEZpbGUgKExpbnV4IHByb2Nlc3NlcyB3aXRoIHdpbmRvd3MpCklk
OmksIE1vZGVfYWx0c2NyPTAsIE1vZGVfaXJpeHBzPTEsIERlbGF5X3RpbWU9My4wLCBDdXJ3aW49
MApEZWYJZmllbGRzY3VyPailpsu1s7TEuru9wLy2t7nFJykqKywtLi8wMTI4Pj9BQkNGR0hJSkxN
Tk9QUVJTVFVWV1hZWltcXV5fYGFiY2RlZmdoaWoKCXdpbmZsYWdzPTE2MTIwNCwgc29ydGluZHg9
MjQsIG1heHRhc2tzPTAsIGdyYXBoX2NwdXM9MCwgZ3JhcGhfbWVtcz0wCglzdW1tY2xyPTEsIG1z
Z3NjbHI9MSwgaGVhZGNscj0zLCB0YXNrY2xyPTEKSm9iCWZpZWxkc2N1cj2lprm3urO0xLu8vafF
KCkqKywtLi8wMTI1Njg+P0BBQkNGR0hJSktMTU5PUFFSU1RVVldYWVpbXF1eX2BhYmNkZWZnaGlq
Cgl3aW5mbGFncz0xOTM4NDQsIHNvcnRpbmR4PTAsIG1heHRhc2tzPTAsIGdyYXBoX2NwdXM9MCwg
Z3JhcGhfbWVtcz0wCglzdW1tY2xyPTYsIG1zZ3NjbHI9NiwgaGVhZGNscj03LCB0YXNrY2xyPTYK
TWVtCWZpZWxkc2N1cj2luru8vb6/wMHDxLO0t8UmJygpKissLS4vMDEyNTY4OUJGR0hJSktMTU5P
UFFSU1RVVldYWVpbXF1eX2BhYmNkZWZnaGlqCgl3aW5mbGFncz0xOTM4NDQsIHNvcnRpbmR4PTIx
LCBtYXh0YXNrcz0wLCBncmFwaF9jcHVzPTAsIGdyYXBoX21lbXM9MAoJc3VtbWNscj01LCBtc2dz
Y2xyPTUsIGhlYWRjbHI9NCwgdGFza2Nscj01ClVzcglmaWVsZHNjdXI9paanqKqwube6xMUpKywt
Li8xMjM0NTY4Ozw9Pj9AQUJDRkdISUpLTE1OT1BRUlNUVVZXWFlaW1xdXl9gYWJjZGVmZ2hpagoJ
d2luZmxhZ3M9MTkzODQ0LCBzb3J0aW5keD0zLCBtYXh0YXNrcz0wLCBncmFwaF9jcHVzPTAsIGdy
YXBoX21lbXM9MAoJc3VtbWNscj0zLCBtc2dzY2xyPTMsIGhlYWRjbHI9MiwgdGFza2Nscj0zCkZp
eGVkX3dpZGVzdD0wLCBTdW1tX21zY2FsZT0wLCBUYXNrX21zY2FsZT0wLCBaZXJvX3N1cHByZXNz
PTAKCg==
EndOfFile

	if ! test -L  ~/bin/topresident
	then
		mkdir -p ~/bin/
		ln -s `which top` ~/bin/topresident	
	fi
 
	WIDTH="-w 512"
	if ! (top -h 2>/dev/null | egrep -qe " -w ")
	then
		WIDTH=""		# width not supported with some versions of top(1)
	fi

	CPUs=`grep "^processor" /proc/cpuinfo 2>/dev/null | wc -l`
	if test $CPUs -eq 0
	then
		CPUs=16
	fi

	if $uba41feature
	then
		SUPERnode=`cat $SNODEfile`
		if $SUPERnode
		then
			if ! $MULTInode
			then
				if test $CPUs -le $SUPERcpu
				then
					echo "false" > $SNODEfile
				fi
			fi
		fi
	else
		echo "false" > $SNODEfile
	fi

	if $IMPALA_HOST
	then
		echo
		CATALOGDjava=`egrep "^export\s*JAVA_TOOL_OPTIONS" /usr/bin/catalogd 2>/dev/null | tail -1`
		NECHO "catalogd:" "'$CATALOGDjava'"
		if ! $uba42feature
		then
			if test -z "$CATALOGDjava"
			then
				echo "   <== check /usr/bin/catalogd memory usage"
				INCREMENT_EXCEPTIONS
			else
				if echo "$CATALOGDjava" | egrep -q -e  "-Xmx4[gG]"
				then
					echo
				else
					echo "   <== check /usr/bin/catalogd memory usage"
					INCREMENT_EXCEPTIONS
				fi
			fi
		else
			echo
		fi
	fi

	if $PERSISTENCE_GRAPHDB_SERVER
	then
		echo
		NEO4Jjava=`grep "^wrapper.java.maxmemory" /etc/neo4j/neo4j-wrapper.conf 2>/dev/null`
		NECHO "neo4j wrapper:" "'$NEO4Jjava'"
		if test -z "$NEO4Jjava"
		then
			echo "   <== check /etc/neo4j/neo4j-wrapper.conf memory usage"
			INCREMENT_EXCEPTIONS
		else
			if echo "$NEO4Jjava" | grep -q "=2048"
			then
				echo
			else
				echo "   <=- check /etc/neo4j/neo4j-wrapper.conf memory usage"
				INCREMENT_EXCEPTIONS
			fi
		fi
	fi

	echo
	NECHO "server time:" ""
	if timedatectl status > $TMP1file 2>/dev/null 
	then
		head -1 $TMP1file	# timedatectl offers much more
		ALL_BUT_FIRST $TMP1file
	else
		> $TMP1file		# empty
		date			# show current time with TZ
	fi

	if test -s $TMP1file
	then
		UTCtimestamp=`grep Universal $TMP1file | sed -e 's/^[^:]*: //'`
		RTCtimestamp=`grep 'RTC time:' $TMP1file | sed -e 's/^[^:]*: //'`
		LOCALtz="`egrep 'Time\s*zone:' $TMP1file | sed -e 's/^.*Time\s*zone: //' -e 's/\s.*$//'`"

		RTClocal=`egrep 'RTC in local TZ' $TMP1file | sed -e 's/\s//g' -e 's/^.*://'`
		if test "$RTClocal" = "yes"
		then
			RTCtz="$LOCALtz"
		else
			RTCtz="UTC"
		fi

		UTCsecs=`TZ=UTC date --date="$UTCtimestamp" "+%s"`
		RTCsecs=`TZ="$RTCtz" date --date="$RTCtimestamp" "+%s"`
		DIFFsecs=`expr $UTCsecs - $RTCsecs`
		echo
		NECHO "clocks:" "UTC: $UTCsecs; RTC: $RTCsecs; diff: $DIFFsecs"

		ABSdiff=`expr $DIFFsecs \* $DIFFsecs`
		if test $ABSdiff -lt 26
		then
			echo  # +/- 5 seconds acceptable
		else
			if test $DIFFsecs -gt 0
			then
				if test $DIFFsecs -ge 300
				then
					echo "   <== future skew from RTC"
					INCREMENT_EXCEPTIONS
				else
					if test  $DIFFsecs -ge 60
					then
						echo "   <= future skew from RTC"
					else
						echo "   (future skew from RTC)"
					fi
				fi
			else
				echo "   <= past skew from RTC"
			fi
		fi
	fi

	> $TMP1file
	ls -1t /var/log/wtmp* |\
	while read LOG
	do
		last -10 -f $LOG -x reboot shutdown runlevel 2>/dev/null
		echo
	done >> $TMP1file
	if test -s $TMP1file
	then
		echo
		NECHO "last system:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	fi

	lastlog -t 7 > $TMP1file 2>/dev/null
	if test -s $TMP1file
	then
                echo
                NECHO "lastlog (7 days):" "`head -1 $TMP1file`"; echo
                ALL_BUT_FIRST $TMP1file
        fi

	if test -s /var/log/caspida/caspida.out
	then
		egrep -e '[0-9]: Running: /opt/caspida/bin/Caspida setup$' /var/log/caspida/caspida.out
		egrep -e '[0-9]: Running: \./Caspida setup$' /var/log/caspida/caspida.out
		egrep -e '[0-9]: Running: /opt/caspida/bin/Caspida start-all --no-caspida$' /var/log/caspida/caspida.out
		egrep -e '[0-9]: Running: \./Caspida start-all --no-caspida$' /var/log/caspida/caspida.out
		egrep -e '[0-9]: Running: /opt/caspida/bin/Caspida start-all$' /var/log/caspida/caspida.out
		egrep -e '[0-9]: Running: \./Caspida start-all$' /var/log/caspida/caspida.out
		egrep -e '[0-9]: Running: /opt/caspida/bin/Caspida start$' /var/log/caspida/caspida.out
		egrep -e '[0-9]: Running: \./Caspida start$' /var/log/caspida/caspida.out
		egrep -e '[0-9]: Running: /opt/caspida/bin/Caspida stop-all$' /var/log/caspida/caspida.out
		egrep -e '[0-9]: Running: \./Caspida stop-all$' /var/log/caspida/caspida.out
		egrep -e '[0-9]: Running: /opt/caspida/bin/Caspida stop$' /var/log/caspida/caspida.out
		egrep -e '[0-9]: Running: \./Caspida stop$' /var/log/caspida/caspida.out
	fi |\
	while read LINE
	do
		KEY=`echo "$LINE" | sed -e 's/: Running.*$//'`
		KEY=`date -d "$KEY" '+%s 2>/dev/null'`
		echo "$KEY;$LINE"
	done |\
	sort -n |\
	tail -15 |\
	sed -e 's/^[^;]*;//' > $TMP1file
	if test -s $TMP1file
	then
		echo
		NECHO "UBA start/stop:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	fi

	BOOTtime="`uptime -s`"
	echo
	NECHO "last boot:" "$BOOTtime" 
	BOOTtime=`date --date="$BOOTtime" +'%s' 2>/dev/null`
	echo " - $BOOTtime   ($THISnode)"

	if test "$DATABASE_HOST_list" = ""
	then
		DATABASE_HOST_list="localhost"
	fi
	ssh $DATABASE_HOST_list 'psql -d caspidadb -t -c "SELECT pg_postmaster_start_time();"' 2>/dev/null | sed -e 's/^\s//' -e '/^$/d' > $TMP1file
	PSQLstart=`cat $TMP1file`

	if test "$PSQLstart" = ""
	then
		:
	else
		echo
		NECHO "psql start:" "$PSQLstart"
		PSQLstart=`date --date="$PSQLstart" +'%s'`
		echo -n " - $PSQLstart   ($THISnode)"
		if test $BOOTtime -gt $PSQLstart
		then
			echo "   <== '$THISnode' re-booted after UBA start-all"
			INCREMENT_EXCEPTIONS
		else
			echo
		fi
	fi

	echo


	AGO30=`expr $NOW - \( $DayOfSeconds \* 30 \)`
	if test $BOOTtime -ge $AGO30
	then
		> $TMP1file

		BEFOREtime=`expr $BOOTtime - 7200 2>/dev/null`

		(LANG=en_US.UTF-8 last -Fxf /var/log/wtmp 2>/dev/null )  |\
		egrep -ve '\(00:00\)' |\
		egrep -e 'still|-' |\
		while read LASTWHEN
		do
	       		WHEN=`echo "$LASTWHEN" | sed -e 's/ - .*$//' -e 's/^.* \(Sun\|Mon\|Tue\|Wed\|Thu\|Fri\|Sat\) //'`
	       		WHEN=`date --date="$WHEN" +'%s' 2>/dev/null`
	       		if test "$WHEN" = ""
	       		then
	               		continue                # no timestamp; nothing to do
	       		fi
			if test "$WHEN" -gt "$BOOTtime" 2>/dev/null
			then
				continue
			fi
	       		if test "$WHEN" -lt "$BEFOREtime" 2>/dev/null
	       		then
				break
	       		fi
			echo "$LASTWHEN"
		done > $TMP1file
	
		if test -f $TMP1file
		then
			echo 
			NECHO "last -Fx (before boot):" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		fi
	
		> $TMP2file				# WHOWHERE
		if test -r /var/log/wtmp
		then
			(who -u /var/log/wtmp 2>/dev/null ) |\
			tail -100 |\
			tac |\
			while read WHOWHEREWHEN
			do
				set a $WHOWHEREWHEN
				case $# in
					0|1|2|3|4)continue;;
				esac	
				WHEN=`date --date="$4 $5" +'%s' 2>/dev/null`
				if test "$WHEN" = ""
				then
					continue		# no timestamp; nothing to do
				fi
				if test "$WHEN" -gt "$BOOTtime" 2>/dev/null
				then
					continue
				fi
				if test "$WHEN" -lt "$BEFOREtime" 2>/dev/null
                                then
                                        break
                                fi
				WHOWHERE="$2 $3"
				if grep -q "$WHOWHERE" $TMP2file
				then
					continue		# skip subsequent logins
				else
					echo "$WHOWHERE" >> $TMP2file
				fi
				echo "$WHOWHEREWHEN"
			done  | head -20> $TMP1file
			if test -s $TMP1file
			then
				echo
				NECHO "who -u (before boot):" "`head -1 $TMP1file`"; echo
				ALL_BUT_FIRST $TMP1file
			fi
		fi
	fi

	echo
	echo "`uptime`   (${THISnode})" > $TMP1file
	NECHO "uptime:" "`head -1 $TMP1file`"			
	if egrep -qe "load average: 1\." $TMP1file 
	then
		if $MULTInode
		then
			echo "   (node under light load)"
		else
			echo "   <= node under light load; unusual for single-node deployment"
		fi
	else
		echo
	fi

	export COLUMNS=1024 

	if !  $HOME/bin/tophealth $WIDTH -bn 1 > $TMP1file 2>&1
	then 
		top $WIDTH -bn 1 > $TMP1file 2>&1		# IFF tophealth should fail
	fi

	echo 
	HEADER="`head -1 $TMP1file`"
	NECHO "top:" "$HEADER"
	# LOADaverage=`echo "$HEADER" | sed -e 's/^.*load average: //' -e 's/\..*$//'`	# 1-minute
	LOADaverage=`echo "$HEADER" | sed -e '/load average: / s/^.*, \(\S\S*$\)/\1/'  -e 's/\..*$//'`  # 15-minute
	if test "$LOADaverage" = ""
	then
		LOADaverage="1"
	fi
	LOADmax=`expr $CPUs \* 2`
	if test "$LOADaverage" -gt "$LOADmax"
	then
		echo "   <== node appears to be under heavy load for $CPUs cpu"
		INCREMENT_EXCEPTIONS 1 23
		echo "$LOADaverage" > $LOADfile
	else
		echo 
	fi
	ALL_BUT_FIRST $TMP1file
	
	if $HOME/bin/topresident $WIDTH -bn 1 > $TMP1file 2>&1
	then
		echo 
		NECHO "topresident:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	fi

	if $uba431feature
	then
		TECcount=`grep 'TriggeringEventsCalculator' $TMP1file | wc -l`
		if test $TECcount -gt 10
		then
			echo
			NECHO "events calculator:" "$TECcount"
			echo "   <= unexpected number of TriggeringEventsCalculator jobs; check Release Notes" 
		fi
	fi

	if $RULE_OFFLINE_HOST
	then
		echo
		grep "SparkSubmit" $TMP1file > $TMP2file
		NECHO "offline batch:" "`wc -l < $TMP2file`"; echo
	fi
	> $TMP2file

	if $DATABASE_HOST
	then
		PSQLquery="SELECT count(pid) \
			FROM pg_stat_activity where query != '' AND state='idle';" 
		psql -d caspidadb -t -c "$PSQLquery" 2>/dev/null |\
		sed -e '/^$/d' -e '/ rows*)/d' -e 's/\s\s*//g' > $TMP1file
		if test -s $TMP1file
		then
			echo
			NECHO "psql idle count:" "`head -1 $TMP1file`"; echo
		fi

		PSQLquery="SELECT pid, datname, usename, client_addr, client_port, state, date_trunc('Seconds',query_start) as query_start, date_trunc('Seconds',backend_start) as backend_start, backend_type, wait_event, query \
			FROM pg_stat_activity where query != '' AND state!='idle' order by backend_start;" 
		psql -d caspidadb -c "$PSQLquery" 2>/dev/null |\
		sed -e '/^$/d' -e '/ rows*)/d' > $TMP1file
		if test -s $TMP1file
		then
			echo
			NECHO "psql active queries:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		fi

		PSQLquery=" SELECT \
			activity.pid, \
			activity.usename, \
			activity.state, \
			activity.query, \
			activity.client_addr, \
			activity.client_port, \
			blocking.pid AS blocking_pid, \
			date_trunc('Seconds',blocking.backend_start) as blocking_backend_start, \
			blocking.backend_type AS blocking_backend_type, \
			blocking.wait_event AS  blocking_wait_event, \
			blocking.query AS blocking_query \
			FROM pg_stat_activity AS activity \
			JOIN pg_stat_activity AS blocking ON blocking.pid = ANY(pg_blocking_pids(activity.pid));"

		psql -d caspidadb -c "$PSQLquery"  2>/dev/null |\
                sed -e '/^$/d' -e '/ rows*)/d' > $TMP1file
                if test -s $TMP1file
                then
			BLOCKlines=`wc -l < $TMP1file`
			if test $BLOCKlines -gt 2
			then
                        	echo
                        	NECHO "psql blocking queries:" "`head -1 $TMP1file`"; echo
                        	ALL_BUT_FIRST $TMP1file
			fi
                fi
	fi

	if which vmstat >/dev/null 2>&1
	then
		vmstat -w 2 5 > $TMP1file 2>/dev/null
                echo
                NECHO "vmstat:" "`head -1 $TMP1file`"
                ALL_BUT_FIRST $TMP1file
        fi


	if which pidstat >/dev/null 2>&1
	then
		pidstat -dl 1 3 > $TMP1file 2>/dev/null
		echo
		NECHO "pidstat:" "`head -1 $TMP1file`"
		ALL_BUT_FIRST $TMP1file
	fi

	> $TMP2file

	sudo cat /proc/slabinfo > $TMP2file 2>/dev/null	
	if test -s $TMP2file
	then
		sed -ie 's/\r//' $TMP2file
		echo
		NECHO "slabinfo:" "`head -1 $TMP2file`"; echo
		ALL_BUT_FIRST $TMP2file
		> $TMP2file
	fi

	> $TMP1file

	if test -s /var/log/kern.log
	then
		LOGlist=`ls -r1t  /var/log/kern.log* 2>/dev/null | tail -3`
		zegrep -e " [i]nvoked oom-killer| Out of memory: Kill| Killed process" $LOGlist 2>/dev/null | tail -6 > $TMP1file
	else
		if test -s /var/log/messages
		then
			LOGlist=`ls -r1t  /var/log/messages* 2>/dev/null | tail -3`
			sudo zegrep -e " [i]nvoked oom-killer| Out of memory: Kill| Killed process" $LOGlist  2>/dev/null | egrep -ve "[z]egrep"  | tail -6 > $TMP1file
		else
			> $TMP1file
		fi
	fi

	if test -s $TMP1file
	then
		ls -l `echo $LOGlist` > $TMP2file 2>/dev/null
		NECHO "recent logs:" "`head -1 $TMP2file`"; echo
		ALL_BUT_FIRST $TMP2file

		if egrep -qe "${TODAYmmmdd} |${YESTERDAYmmmdd} |${DAYBEFOREmmmdd} |${TODAYmmm_d} |${YESTERDAYmmm_d} |${DAYBEFOREmmm_d} " $TMP1file
		then
			ERROR="   <= exceeding memory constraints; review sizing"
		else
			ERROR=""
		fi
		echo
		NECHO "most recent OoM:" "`head -1 $TMP1file`"; echo "$ERROR"
		ALL_BUT_FIRST $TMP1file
	fi

	echo
        if test -s $RHEL_OS_file >/dev/null 2>&1
        then
                if sudo  systemctl status iptables >/dev/null 2>&1
                then
                        (echo -n "iptables, "; sudo -n service iptables status | cat -v)  2>&1
                else
                        (echo -n "firewalld, "; sudo -n firewall-cmd --info-zone=trusted )  2>&1
                fi
	else
		( echo -n "ufw, "; sudo -n ufw status) 2>&1
        fi > $TMP1file

	(echo "... iptables ..."; echo)  >> $TMP1file
	sudo iptables -L >> $TMP1file 2>/dev/null

	> $TMP2file
	> $TMP3file
	cp $TMP1file $FWfile
	ERROR=""
	if $MEMBERnode
	then
		PATIENCE=`expr $NUMnodes \* 4 2>/dev/null`
		if test "$PATIENCE" = ""
		then
			PATIENCE=30
		fi
		sleep $PATIENCE
		for I in 1 2 3
		do
			scp $FIRSTnode:$FWfile $TMP2file 2>/dev/null
			if test -s $TMP2file
			then
				sleep $PATIENCE
			else
				break
			fi
		done
		if ! diff $FWfile $TMP2file  >$TMP3file 2>&1
		then
			ERROR="<= review differences; firewall configuration doesn't match leader"
		fi
		
	fi

        NECHO "firewall:" "`head -1 $TMP1file`"; echo "   $ERROR"
        ALL_BUT_FIRST $TMP1file

	if test -s $TMP3file
	then
		echo
		NECHO "firewall (differences):" "`head -1 $TMP3fie`"; echo
		ALL_BUT_FIRST $TMP3file
	fi

        echo
	if which netstat > /dev/null 2>&1
	then
		# sudo netstat -pant 2>&1 | egrep -v 'SYN_RECV|TIME_WAIT'
		sudo netstat -pant 2>&1 | egrep -v 'TIME_WAIT' | tee $TMP2file
	else
		sudo ss -pnt 2>&1 | cat
	fi | sed -e 's/^.*:\([0-9]\{1,\}\)/\1|&/' | sort -t '|' -n -k 1 | cut -d '|' -f 2- > $TMP1file 2>&1
	NECHO "listening ports:" "`head -1 $TMP1file`"; echo
	ALL_BUT_FIRST $TMP1file

	if test -s $TMP2file
	then
		egrep -v 'LISTEN|Internet|Address' $TMP2file |\
		sed -e 's/\s\s*/ /g' |\
		cut -d ' ' -f 5 |\
		cut -d ':' -f 1 |\
		sed -e '/^$/d' |\
		sort |\
		uniq -c |\
		sort -nr > $TMP3file
		if test -s $TMP3file
		then
			echo
			NECHO "foreign addresses:" "`head -1 $TMP3file`"; echo
			ALL_BUT_FIRST $TMP3file
		fi
	fi


	IMPALAports=`egrep -e '[0-9]/impalad' $TMP1file | wc -l`
	if test $IMPALAports -gt 0
	then
		echo
		NECHO "impala port connections:" "$IMPALAports"
		if test $IMPALAports -gt 500
		then
			echo "   <== excessive number of impala port connections (UBA-14199)"
			INCREMENT_EXCEPTIONS
		else
			echo
		fi
		if test $IMPALAports -gt 100
		then
			JOBcron=`sed -ne '/"StatsCollector"/,/cronExpr/ p' /opt/caspida/conf/jobconf/caspida-jobs.json | grep cronExpr | sed -e 's/^  *//'`
			echo
			NECHO "StatsCollector:" "$JOBcron"
			if $uba50feature
			then
				echo
			else
				if $uba43feature
				then
					if echo "$JOBcron" | grep -q "0 0 20"
					then
						echo
					else
						echo "   <== check Release Notes - UBA-12111"
						INCREMENT_EXCEPTIONS
					fi
				else
					echo
				fi
			fi	
		fi
		if which lsof >/dev/null 2>&1
		then
			sudo lsof -Pni :21050 > $TMP1file 2>/dev/null
		fi
		if ! test -s $TMP1file
		then
			if which ss >/dev/null 2>&1
			then
				sudo ss -tulpn 2>/dev/null | grep :21050 > $TMP1file
			fi
		fi
		if test -s $TMP1file
		then
			echo
			NECHO "port 21050:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		fi

	fi

) | tee -a $OUTfile

(
	S_TIME_FORMAT="ISO" iostat -xtd > $TMP1file 2>/dev/null
	if test -s $TMP1file
	then
		echo 
		NECHO "iostat -xtd:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	fi

	if ! test -f $LOADfile
	then
		IOPSbs=4				# 4k blocks
		IOPScount=`expr 1024 \* 1`		# 1 MB file
		IOPScount=`expr $IOPScount / $IOPSbs` 
	

		S_TIME_FORMAT="ISO" iostat -xytd 1 60 > $TMP1file 2>/dev/null &
		IOSTATpid=$!

		sleep 2

		for IOPSfile in $IOPSfile_tmp $IOPSfile_vcap $IOPSfile_vcap2
		do 
			IOPSdir=`dirname $IOPSfile`
			if test -d $IOPSdir
			then
				IOPSpart=`df $IOPSdir | egrep "^/" | cut -d ' ' -f 1`
				echo
				dd if=/dev/urandom of=$IOPSfile bs=${IOPSbs}k oflag=direct count=$IOPScount > $TMP2file 2>&1	# create the file
				IOPS write $IOPSbs $TMP2file $IOPSpart

				dd if=$IOPSfile of=/dev/null bs=${IOPSbs}k iflag=direct > $TMP2file 2>&1
				IOPS read $IOPSbs $TMP2file $IOPSpart

				if which hdparm >/dev/null 2>&1
				then
					DISKsource=`df --output=source $IOPSdir | tail -1`
					sudo hdparm -T $DISKsource > $TMP3file 2>/dev/null 
					if test -s $TMP3file
					then
						echo
						NECHO "hdparm -T ${DISKsource}:" "`head -1 $TMP3file`"; echo
						ALL_BUT_FIRST $TMP3file
					fi
				fi
			fi
		done

		sleep 2
		if ps  $IOSTATpid >/dev/null 2>&1
		then
			kill $IOSTATpid >/dev/null 2>&1
		fi

		echo
		NECHO "iostat -xytd 1 60:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	else
		echo
		NECHO "iops:" "   (bypassed; system under load)"; echo
	fi
) | tee -a $OUTfile

(
	#
	#	summarize open files by user
	#	

	echo
	
	sudo ls -l /proc/*/fd 2>/dev/null | grep '^l' | cut -d ' ' -f 3 | sort | uniq -c > $TMP1file
	if test -s $TMP1file
	then
		NECHO "user proc fd:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
		echo
	fi

	NECHO "user open files:" ""
	
	if ! test -f $LOADfile
	then
		if sudo which lsof >/dev/null 2>&1
		then
			MAXdefault=`grep '^\*' /etc/security/limits.conf 2>/dev/null | grep nofile | grep -v soft | sed -e 's/\s\s*/ /g' | cut -d ' ' -f 4`
			if test -z "$MAXdefault"
			then
				MAXdefault="65536"
			fi
			sudo lsof -F up 2>/dev/null | sed -e '/^[^u]/d' -e 's/^u//' | sort -n | uniq -c |\
			while read COUNT uid
			do
				UNAME=`getent passwd $uid | cut -d ':' -f 1`
				MAX=`grep nofile /etc/security/limits.d/${UNAME}.conf 2>/dev/null | grep -v 'soft' | sed -e 's/\s\s*/ /g' | cut -d ' ' -f 4`
				if test -z "$MAX"
				then
					MAX=`egrep "^${UNAME}\b" /etc/security/limits.conf 2>/dev/null | grep nofile | grep -v soft | sed -e 's/\s\s*/ /g' | cut -d ' ' -f 4`
				fi
				if test -z "$MAX"
				then
					MAX=$MAXdefault
				fi
	
				if test "$COUNT" = "$MAX"
				then
					ERROR="  <== open files have been max'd"
					INCREMENT_EXCEPTIONS 1 24
				else
					ERROR=""
				fi
				echo "${UNAME} $COUNT $MAX $ERROR" | awk '{ printf "%-12.12s open:%6.6s; max:%6.6s %s\n", $1,$2,$3,$4}'
			done | egrep -ve '^root\b|^daemon\b|^syslog\b|^ntp\b|^messagebus\b' | sort > $TMP1file
			echo "`head -1 $TMP1file`$ERROR"
			ALL_BUT_FIRST $TMP1file
		else
			echo "   (lsof not found)"
		fi
	else
		echo "   (bypassed; system under load)"
	fi

) | tee -a $OUTfile

if $CASPIDA_PROPERTIES_EXIST
then
	if $uba42feature
	then
		ERROR=""
	else
		ERROR="   <= check Support EOL"
	fi

	echo
	NECHO "uba version:" "$UBAversion"; echo $ERROR

	CONTENT=`egrep -e '"version"' /opt/caspida/content/*/content-descriptor.json 2>/dev/null | sort | tail -1 | cut -d '"' -f 4`
	if ! test -z "$CONTENT"
	then
		NECHO "last content:" "$CONTENT"; echo
	fi

	if $LEADERnode
	then
		if test -f /var/run/caspida/caspida-sysmonitor.pid
		then
			echo
			STARTdate="`date -r /var/run/caspida/caspida-sysmonitor.pid`"
			NECHO "sysmonitor started:" "$STARTdate"; echo
		fi
	fi

	if $DATABASE_HOST
	then
		POSTGRESversion=`psql --version 2>/dev/null`
		if test $? = 0
		then
			echo
			NECHO "postgresql:" "$POSTGRESversion"; echo
			PGvers=`echo "$POSTGRESversion" | sed -n 's/^.* \([0-9]*\.[0-9]*\)\b.*$/\1/p'`
		fi

		echo
                DBversion="`psql -d caspidadb -t -c 'select currentversion from dbinfo;' 2>/dev/null | tr -d ' '`"
                NECHO "current db:" "'$DBversion'"; echo
	fi

	if $IMPALA_HOST
	then
		echo
		if ! which impala-shell > /dev/null 2>&1
		then
			NECHO "impala:" "impala-shell   <= not found in executable path; "; printenv PATH	
		else
			NECHO "impala version:" ""
			if impala-shell -d caspida --quiet  --version > $TMP1file 2>$TMP2file
			then
				head -1 $TMP1file
				ALL_BUT_FIRST $TMP1file
			else
				echo "`head -1 $TMP2file`   <== impala-shell failed to execute"
				ALL_BUT_FIRST $TMP2file
				INCREMENT_EXCEPTIONS
			fi
		fi
	fi

	echo
	> $TMP1file
	for PROP in enabled server.indexers server.index.name
	do
		GET_CASPIDA_PROPERTY splunk.forwarder.$PROP
		echo "splunk.forwarder.$PROP=$PROPERTYvalue"
	done >> $TMP1file

	STTYcurr=`stty -g`
	$SPLUNK_HOME/bin/splunk list forward-server > $TMP2file 2>/dev/null &

	SPLUNKpid=$!
        sleep 3
	if ps $SPLUNKpid >/dev/null 2>&1
	then
		sudo kill -9 $SPLUNKpid >/dev/null 2>&1
		if ! test "$TTY" = "not a tty"
		then
			stty $STTYcurr    # restore stty settings
		fi
	fi

	if grep -q "SPLUNK SOFTWARE LICENSE AGREEMENT" $TMP2file
	then
		echo "(license not accepted)"
	else
		cat $TMP2file 
	fi >> $TMP1file 
	if test -s $TMP1file
	then
		echo
		NECHO "splunk forwarding:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	fi
	echo

	ps -elf | egrep -e '[Ss]plunkd' > $TMP1file
	if test -s $TMP1file
	then
		echo >> $TMP1file
		$SPLUNK_HOME/bin/splunk display boot-start >> $TMP1file 2>/dev/null
		echo
		NECHO "splunkd:" "`head -1 $TMP1file`" ; echo
		ALL_BUT_FIRST $TMP1file
	fi

	CONTAINERstreaming=false	
	if $uba40feature
	then
		GET_CASPIDA_PROPERTY system.usecontainers
		if test "$PROPERTYvalue" = "true"
		then
			GET_CASPIDA_PROPERTY system.containerized.apps
			case "$PROPERTYvalue" in
				*"ubastreamingmodels"*)	CONTAINERstreaming=true;;
			esac
		fi	
	fi

	if $JOB_MANAGER
	then
		JMpid=`ps -elf | egrep "[Cc]aspidaJobManager" | sed -e 's/\s\s*/ /g' | cut  -d ' ' -f 4`
		if ps "$JMpid" >/dev/null 2>&1
		then
			ps e $JMpid 2>/dev/null | fold -s -w 100 > $TMP1file
			if test -s $TMP1file
			then
				echo
				NECHO "jobmanager env:" "`head -1 $TMP1file`"; echo
				ALL_BUT_FIRST $TMP1file
			fi
		fi
	fi

	> $TMP1file			# clear expected
	> $TMP2file			# clear unexpected

	if $UISERVER_HOST
	then
		DEST=$TMP1file		# expected
	else
		DEST=$TMP2file		# unexpected
	fi
	echo "caspida-ui" >> $DEST
	echo "caspida-outputconnector" >> $DEST 
	if ! $uba32feature
	then
		echo "caspida-sysmonitor" 
	else
		echo "caspida-resourcesmonitor"
	fi >> $DEST
	
	if $DATABASE_HOST
	then
		DEST=$TMP1file		# expected
	else
		DEST=$TMP2file		# unexpected
	fi
	
	if $RHELrelease
	then
		echo "postgresql-$PGvers" >> $DEST
	else
		echo "postgresql" >> $DEST
	fi

	if $KAFKA_BROKER
	then
		DEST=$TMP1file		# expected
	else
		DEST=$TMP2file		# unexpected
	fi
	echo "kafka-server" >> $DEST

	if $SPARK_SERVER
	then
		DEST=$TMP1file		# expected
	else
		DEST=$TMP2file		# unexpected
	fi
	echo "spark-server" >> $DEST

	if $SPARK_MASTER
	then
		DEST=$TMP1file		# expected
	else
		DEST=$TMP2file		# unexpected
	fi

	if $SPARK_WORKER
	then
		DEST=$TMP1file		# expected
	else
		DEST=$TMP2file		# unexpected
	fi

	if $IMPALA_HOST
	then
		DEST=$TMP1file		# expected
	else
		DEST=$TMP2file		# unexpected
	fi
	echo "impala-server" >> $DEST

	if ! $CONTAINERstreaming
	then
		if $STORM_UI_HOST
		then
			DEST=$TMP1file		# expected
		else
			DEST=$TMP2file		# unexpected
		fi
		echo "storm-ui" >> $DEST
	
		if $STORM_SUPERVISOR
		then
			DEST=$TMP1file		# expected
		else
			DEST=$TMP2file		# unexpected
		fi
		echo "storm-supervisor" >> $DEST
	
		if $STORM_NIMBUS_HOST
		then
			DEST=$TMP1file		# expected
		else
			DEST=$TMP2file		# unexpected
		fi
		echo "storm-nimbus" >> $DEST
	fi

	if $RESOURCESMONITOR
	then
		DEST=$TMP1file		# expected
	else
		DEST=$TMP2file		# unexpected
	fi
	echo "caspida-resourcesmonitor" >> $DEST

	if $SYSMONITOR
	then
		DEST=$TMP1file		# expected
	else
		DEST=$TMP2file		# unexpected
	fi
	echo "caspida-sysmon" >> $DEST

	if $JOB_MANAGER
	then
		DEST=$TMP1file		# expected
	else
		DEST=$TMP2file		# unexpected
	fi
	echo "caspida-jobmanager" >> $DEST

	if $JOB_AGENT
	then
		DEST=$TMP1file		# expected
	else
		DEST=$TMP2file		# unexpected
	fi
	echo "caspida-jobagent" >> $DEST

	if ! $uba41feature
	then
		if $ANALYTICS_HOST
		then
			DEST=$TMP1file		# expected
		else
			DEST=$TMP2file		# unexpected
		fi
		echo "caspida-analytics" >> $DEST
	fi

	if $ZOOKEEPER_SERVER
	then
		DEST=$TMP1file		# expected
	else
		DEST=$TMP2file		# unexpected
	fi
	echo "zookeeper-server" >> $DEST

	if $HADOOP_NAMENODE
	then
		DEST=$TMP1file		# expected
	else
		DEST=$TMP2file		# unexpected
	fi
	echo "hadoop-hdfs-namenode" >> $DEST

	if $HADOOP_SNAMENODE
	then
		DEST=$TMP1file		# expected
	else
		DEST=$TMP2file		# unexpected
	fi
	echo "hadoop-hdfs-secondarynamenode" >> $DEST

	if $HADOOP_DATANODE
	then
		DEST=$TMP1file		# expected
	else
		DEST=$TMP2file		# unexpected
	fi
	echo "hadoop-hdfs-datanode" >> $DEST

	if ! $uba32feature
	then
		if $HBASE_MASTER
		then
			DEST=$TMP1file		# expected
		else
			DEST=$TMP2file		# unexpected
		fi
		echo "hbase-master" >> $DEST

		if $HBASE_REGIONSERVER
		then
			DEST=$TMP1file		# expected
		else
			DEST=$TMP2file		# unexpected
		fi
		echo "hbase-regionserver" >> $DEST
	fi

	if $HIVE_HOST
	then
		DEST=$TMP1file		# expected
	else
		DEST=$TMP2file		# unexpected
	fi
	echo "hive-metastore" >> $DEST

	if $PERSISTENCE_GRAPHDB_SERVER
	then
		DEST=$TMP1file		# expected
	else
		DEST=$TMP2file		# unexpected
	fi
	if $PERSISTENCE_GRAPHDB_SERVER
	then
		if $RHELrelease
		then
			echo "neo4j" >> $DEST
		else
			echo "neo4j-service" >> $DEST
		fi
	fi

	
	if $PERSISTENCE_DATASTORE
	then
		DEST=$TMP1file		# expected
	else
		DEST=$TMP2file		# unexpected
	fi
	if ! $uba32feature
	then
		echo "opentsdb" >> $DEST
	else
		echo "influxdb" >> $DEST
	fi

	if $PERSISTENCE_IRSERVER
	then
		DEST=$TMP1file		# expected
	else
		DEST=$TMP2file		# unexpected
	fi
	echo "redis-ir-server" >> $DEST

	if $PERSISTENCE_SERVER
	then
		DEST=$TMP1file		# expected
	else
		DEST=$TMP2file		# unexpected
	fi
	echo "redis-server" >> $DEST

	if $RULE_REALTIME_HOST
	then
		DEST=$TMP1file		# expected
	else
		DEST=$TMP2file		# unexpected
	fi
	echo "caspida-realtimeruleexec" >> $DEST

	if $RULE_OFFLINE_HOST
	then
		DEST=$TMP1file		# expected
	else
		DEST=$TMP2file		# unexpected
	fi
	echo "caspida-offlineruleexec" >> $DEST

	if $uba41feature
	then
		if $CONTAINER_MASTER || $CONTAINER_WORKER 
		then
			DEST=$TMP1file	# expected
		else
			DEST=$TMP2file  # unexpected
		fi
		echo "kubelet" >> $DEST
		echo "docker" >> $DEST
	fi

	SERVICESlist="`sort -u  $TMP1file`"			# expected services

	for SERVICE in $SERVICESlist
	do
		echo
		echo "${SERVICE}:" | awk '{ printf "%-31.31s\n", $1}'
		sudo service "$SERVICE" status 2>/dev/null |\
		cat -v | sed -e 's/M-bM-\^WM-\^O /(O) /' -e 's/^\s*M.*[@|#]/           |__/' |\
		while read STATUSline
		do
			case "$STATUSline" in
				*"dead and pid file exists[FAILED]"*)	ERROR="   <== expected service '$SERVICE' has failed"; INCREMENT_EXCEPTIONS 1 51;;
				*"filter status=dead"*)		ERROR="";;
				*"container not running"*)	ERROR="";;
				*"not running"*)	ERROR="   <== expected service '$SERVICE' is not running"; INCREMENT_EXCEPTIONS 1 51;;
				*"unrecognized service"*)	ERROR="   <== expected service '$SERVICE' is not recognized"; INCREMENT_EXCEPTIONS 1 52;;
				*"is dead, but"*)	ERROR="";;
				*"context deadline"*)	ERROR="";;
				*"Active: failed"*)	ERROR="   <== expected service '$SERVICE' has failed"; INCREMENT_EXCEPTIONS 1 72;;
				*"(dead"*)	ERROR="   <== expected service '$SERVICE' is dead"; INCREMENT_EXCEPTIONS 1 72;;
				*" shutdown"*)	ERROR="";;
				*"container runtime is down"*) ERROR="";;
				*"shutting down client connections"*) ERROR="";;
				*"client shutting down"*) ERROR="";;
				*"confirmShutdown"*) ERROR="";;
				*"Active: activating"*)	ERROR="   <== expected service '$SERVICE' still activating"; INCREMENT_EXCEPTIONS 1 72;;
				*"download"*) ERROR="";;
				*"down"*)	ERROR="   <== expected service '$SERVICE' is down"; INCREMENT_EXCEPTIONS 1 72;;
				*)		ERROR="";;
			esac
			echo "${STATUSline}$ERROR"
		done
	done > $TMP1file

	echo
	NECHO "services (expected):" ""
	head -1 $TMP1file; ALL_BUT_FIRST $TMP1file

	if test -s $TMP2file
	then
		SERVICESlist="`sort -u  $TMP2file`"			# unexpected services

		for SERVICE in $SERVICESlist
		do
			if ! $uba50feature
			then
				if test $SERVICE = 'docker'
				then
					continue
				fi
			fi
			echo
			echo "${SERVICE}:" | awk '{ printf "%-31.31s\n", $1}'
			sudo service "$SERVICE" status 2>/dev/null |\
			cat -v | sed -e 's/M-bM-\^WM-\^O /(O) /' -e 's/^\s*M.*[@|#]/           |__/' |\
			while read STATUSline
			do
				case "$STATUSline" in
					*"is running"*|*"(running)"*)		ERROR="   <== unexpected service '$SERVICE' is running" 
								INCREMENT_EXCEPTIONS 1 26;;
					*)			ERROR="";;
				esac
				echo "${STATUSline}$ERROR"
			done
		done > $TMP2file

		if grep -q "<== unexpected" $TMP2file >/dev/null 2>&1
		then
			echo
			NECHO "services (unexpected):" ""
			head -1 $TMP2file; ALL_BUT_FIRST $TMP2file
		fi
	fi

fi | tee -a $OUTfile

(
	if $UISERVER_HOST
	then
                if $uba41feature
                then
                        SEARCHheads="/etc/caspida/local/conf/splunk_search_head.json"
                        if test -f $SEARCHheads
                        then
                                if python -m json.tool < $SEARCHheads > $TMP1file 2>&1
                                then
					echo
                                        NECHO "splunk_search_head.json:" "`head -1 $TMP1file`"; echo
                                        ALL_BUT_FIRST $TMP1file
                                else
                                        NECHO "splunk_search_head.json:" "`head -1 $SEARCHheads`"; echo "   <== JSON decoding of '$SEARCHheads' failed"
                                        ALL_BUT_FIRST $SEARCHheads
                                        INCREMENT_EXCEPTIONS
                                fi
                        fi
                fi
	fi
	
	echo

	PROPERTY_LIST="alert.email.eps.alert.watermark alert.email.eps.alert.enable alert.email.eps.alert.frequency alert.email.lists \
		analytics.messaging.read.maxmsgs analytics.store.maxfiles analytics.store.block.size \
		analytics.store.hdfs.block.size models.threats.highconfidence models.threats.goBackDays models.collab.raiseallnew \
		models.threats.skipDeviceThreats identity.session.cache.enabled identity.sessionization.enabled \
		identity.resolution.attribution.query.timerange \
		identity.resolution.blackList.expiration.days \
		identity.resolution.data.retention \
		identity.resolution.export.enabled \
		identity.resolution.hrcache.disabled \
		identity.resolution.hrcache.capacity.smallcache \
		identity.resolution.domains.list \
		identity.resolution.hrdata \
		identity.resolution.hrdata.filter.unknown \
		identity.resolution.hrcache.refresh.interval \
		identity.resolution.hrcache.refresh.limit \
		uiServer.host "

	if $uba32feature
	then
		PROPERTY_LIST="$PROPERTY_LIST identity.resolution.device.domains.normalize identity.resolution.device.domains.list "
	fi
	if $uba32feature && ! $uba41feature
	then
		PROPERTY_LIST="$PROPERTY_LIST identity.resolution.hrdata.accepted.address.range "
	fi
	if $uba33feature
	then
		PROPERTY_LIST="$PROPERTY_LIST splunk.indexed.realtime connector.splunk.use.time connectors.output.splunkes.ssl"
	fi
	if $uba40feature
	then
		PROPERTY_LIST="$PROPERTY_LIST ui.httpsPort ui.splunk.authentication sso.saml.loginUrl sso.saml.loginCallbackUrl sso.saml.issuer sso.saml.privateCert analytics.consumer.numworkers analytics.aggregator.numworkers ubaMonitor.pipeline.DeviceWorkflow.pollCount"
	fi
	if $uba40feature
	then
		PROPERTY_LIST="$PROPERTY_LIST parser.device.internal.domains splunk.maximum.finalize.time identity.resolution.hrcache.capacity splunk.export.output.mode"
	fi
	if $uba41feature
	then
		PROPERTY_LIST="$PROPERTY_LIST identity.resolution.device.auto.normalize identity.session.cache.limit jobmanager.master.allow.live system.realtime.workers.maxmodelcache system.disk.usage.high.watermark system.disk.usage.low.watermark system.network.interface"
	fi
	if $uba42feature
	then
		PROPERTY_LIST="$PROPERTY_LIST ui.auth.rootca ui.auth.privateKey ui.auth.serverCert decorator.filter.precedence splunk.kafka.ingestion.search.max.lag.seconds splunk.kafka.ingestion.search.delay.seconds splunk.kafka.ingestion.search.interval.seconds splunk.direct.enum.normalize splunk.direct.enum.monitor.interval splunk.direct.enum.monitor.top.count ubaMonitor.etl.enumMonitor.warn.threshold ubaMonitor.etl.enumMonitor.bad.threshold"
	fi
	if $uba43feature
	then
		PROPERTY_LIST="$PROPERTY_LIST splunk.live.micro.batching splunk.live.micro.batching.delay.seconds splunk.live.micro.batching.interval.seconds uba.splunkes.retry.delay.minutes splunk.forwarder.server.index.name splunk.forwarder.enabled splunk.forwarder.server.indexers system.docker.networkcidr "
	fi
	if $uba431feature
	then
		PROPERTY_LIST="$PROPERTY_LIST persistence.anomalies.trashed.maintain.days parser.ip.scanners connector.splunk.micro.batching.backtrace executor.offline.workersPerInstance triggering.event.pre.calculate.links uba.splunkes.integration.enabled uba.sys.audit.push.splunk.enabled"
	fi
	if $uba4311feature
	then
		PROPERTY_LIST="$PROPERTY_LIST connector.splunk.max.backtrace.time.in.hour"
	fi

	if $uba50feature
	then
		PROPERTY_LIST="$PROPERTY_LIST replication.enabled replication.primary.host replication.standby.host \
			attribution.keyvalue.delimiter parser.morphlines.name.policy parser.morphlines.selected.formats \
			backup.filesystem.enabled backup.filesystem.directory.restore backup.filesystem.full.interval \
			ui.sso.saml.use.relaystate splunk.forwarder.enabled splunk.forwarder.server.indexers"
	fi

	if $uba51feature
	then
		DSmicro=`egrep -e '^splunk\.micro\.batching\.search\.' $UBA_SITE_PROPERTIES 2>/dev/null | cut -d '=' -f 1 | sed -e 's/$/ /'`
		PROPERTY_LIST="$PROPERTY_LIST `echo $DSmicro`"
	fi


	if $uba41feature
	then
		UBA_CUSTOM_PROPERTIES=`echo "$UBA_PROPERTIES" | sed -e "s+$UBA_DEFAULT_PROPERTIES++"`	
		for PROPERTY in system.messaging system.realtime containers ubaMonitor identity.session 
		do
			egrep -he "^${PROPERTY}\.\S*\s*=" $UBA_CUSTOM_PROPERTIES |\
			sed -e 's/\s*=.*$/ /'
		done | sort -u > $TMP1file
		sed -ie ':a;/ $/{N;s/\n//;ba}' $TMP1file
	else
		> $TMP1file	# no tuning
	fi

	CUSTOM_LIST=`cat $TMP1file`
	for PROPERTY in $PROPERTY_LIST $CUSTOM_LIST 
	do
		egrep -he "^${PROPERTY}\s*=" $UBA_PROPERTIES | tail -1
	done | sort -u >$TMP1file

	if $uba41feature
	then
		sed -ie '/analytics.aggregator.numworkers/ s/$/   <= analytics.aggregator.numworkers property not expected/' $TMP1file
		sed -ie '/analytics.consumer.numworkers/ s/$/   <= analytics.consumer.numworkers property not expected/' $TMP1file
	fi

	if $uba42feature
	then
		sed -ie '/^ubaMonitor\./ s/$/   (ubaMonitor property can be removed)/' $TMP1file
	fi
	
	if false      # expected but not delivered as $uba50feature
	then
		if egrep -q "^triggering.event.pre.calculate.links" $TMP1file
		then
			GET_CASPIDA_PROPERTY triggering.event.pre.calculate.links
			if test "$PROPERTYvalue" = "true"
			then
				sed -ie '/^triggering.event.pre.calculate.links/ s/$/   <= triggering.event.pre.calculate.links not expected uba-site.properties/' $TMP1file
			else
				sed -ie '/^triggering.event.pre.calculate.links/ s/$/   <== triggering.event.pre.calculate.links must be removed from uba-site.properties/' $TMP1file
			INCREMENT_EXCEPTIONS
			fi
		fi
	fi
	
	ls -Ul $UBA_PROPERTIES > $TMP2file 2>/dev/null
	echo >> $TMP2file
	sort $TMP1file >> $TMP2file

	NECHO "properties:" "`head -1 $TMP2file`"; echo
	ALL_BUT_FIRST $TMP2file

	if $DATABASE_HOST
	then
		cat > $TMP2file <<EndOfData
			show log_destination; 
			show syslog_facility; 
			show syslog_ident; 
			show logging_collector; 
			show data_directory;
			show log_directory; 
			show log_filename; 
EndOfData
		psql -d caspidadb -x -f $TMP2file 2>/dev/null |\
		sed -e '/^$/d' -e '/+--/d' -e 's/\s\s*//g' > $TMP1file

		CURRENTlog="unknown"
		LOGGINGcollector=`grep logging_collector $TMP1file | cut -d '|' -f 2`
		if test "$LOGGINGcollector" = "on"
		then
			DATAdir=`grep data_directory $TMP1file | cut -d '|' -f 2`
			LOGdir=`grep log_directory $TMP1file | cut -d '|' -f 2`
                        LOGpath="$DATAdir/$LOGdir"
			LOGname=`grep log_filename $TMP1file | cut -d '|' -f 2`
			LOGprefix=`echo "$LOGname" | sed -e 's/\(\w\w*\)\W.*$/\1/'`
			LOGext=`echo "$LOGname" | sed -e 's/^[^\.][^\.]*//'`
			if sudo test -d "$LOGpath"
			then
				CURRENTlog=`sudo ls -1tr "$LOGpath" | grep $LOGprefix | grep "$LOGext" | tail -1`
				CURRENTlog="$LOGpath/$CURRENTlog"
			else
				CURRENTlog="unexpected"
			fi
		else
			PSQLpid=`ps -elf | grep "[p]ostgresql" | sed -e 's/\s\s*/ /g' | cut -d ' ' -f 4`
			if expr "$PSQLpid" + 0 >/dev/null 2>&1
			then
				CURRENTlog=`sudo ls -l /proc/$PSQLpid//fd/2 2>/dev/null |  sed -e 's/\s\s*/ /g' | rev | cut -d ' ' -f 1 | rev`
				if test "$CURRENTlog" = ""
				then
					CURRENTlog="undetermined"
				fi
			fi
		fi
		echo "current_log|$CURRENTlog" >> $TMP1file

		awk 'BEGIN {FS="|"}{printf "%-17.17s | %s\n", $1, $2}' < $TMP1file > $TMP2file
		if test -s $TMP2file
		then
			echo
			NECHO "psql logging:" "`head -1 $TMP2file`"; echo
			ALL_BUT_FIRST $TMP2file
		fi

		sudo grep ERROR "$CURRENTlog" 2>/dev/null | tail -30 > $TMP1file
		if test -s $TMP1file
		then
			echo
			NECHO "recent psql err:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		fi
	fi
	
	if $DATABASE_HOST
	then
		cat > $TMP2file <<EndOfData
			show archive_mode; 
			show wal_level; 
			show wal_compression; 
			show max_standby_archive_delay; 
			show wal_keep_segments;
			show archive_timeout; 
			show archive_command; 
EndOfData
		psql -d caspidadb -x -f $TMP2file 2>/dev/null |\
		sed -e '/^$/d' -e '/---/d' -e 's/\s\s*|\s\s*/|/g' > $TMP1file

		awk 'BEGIN {FS="|"}{printf "%-25.25s | %s\n", $1, $2}' < $TMP1file > $TMP2file
		if test -s $TMP2file
		then
			echo
			NECHO "psql archiving:" "`head -1 $TMP2file`"; echo
			ALL_BUT_FIRST $TMP2file
		fi
	fi

	REPLICATIONcoordinator=false

        GET_CASPIDA_PROPERTY backup.filesystem.enabled                  
	BACKUPenabled=`echo "$PROPERTYvalue" | tr -d ' ' | tr '[:upper:]' '[:lower:]'` 
	echo
	NECHO "backup.enabled:" "backup.filesystem.enabled=$BACKUPenabled"; echo

	if test "$BACKUPenabled" = 'true'
	then
		REPLICATIONcoordinator=true

		GET_CASPIDA_PROPERTY backup.filesystem.full.interval
		NECHO "backup interval:" "backup.filesystem.full.interval=$PROPERTYvalue"; echo

		GET_CASPIDA_PROPERTY backup.filesystem.directory.restore
		BACKUPfilesystem=`echo "$PROPERTYvalue" | sed -e 's/^\s\s*//' -e 's/\s\s*$//'`
		if test -d "$BACKUPfilesystem"
		then
			echo "backup.filesystem.directory.restore=$BACKUPfilesystem" > $TMP1file
			echo >> $TMP1file
			ls -ld "$BACKUPfilesystem" >> $TMP1file
			echo >> $TMP1file
			(cd "$BACKUPfilesystem"; ls -la ) >> $TMP1file
			echo >> $TMP1file
			df -h "$BACKUPfilesystem" >> $TMP1file
			echo >> $TMP1file
			sudo du -sch "$BACKUPfilesystem"/* >> $TMP1file 2>/dev/null
			sudo ls -lrt "${BACKUPfilesystem}/caspida" > $TMP2file 2>/dev/null
			if test -s $TMP2file
			then
				echo
				echo "ls -lrt ${BACKUPfilesystem}/caspida/     # oldest"
				head -7 $TMP2file
				echo
				echo "ls -lrt ${BACKUPfilesystem}/caspida/     # latest"
				tail -7 $TMP2file
			fi >> $TMP1file
			sudo ls -lrt "${BACKUPfilesystem}/delete" > $TMP2file 2>/dev/null
			if test -s $TMP2file
			then
				echo
				echo "ls -lrt ${BACKUPfilesystem}/delete/     # oldest"
				head -7 $TMP2file
				echo
				echo "ls -lrt ${BACKUPfilesystem}/delete/     # latest"
				tail -7 $TMP2file
			fi >> $TMP1file
			sudo ls -lrt "${BACKUPfilesystem}/wal_archive" > $TMP2file 2>/dev/null
			if test -s $TMP2file
			then
				echo
				echo "ls -lrt ${BACKUPfilesystem}/wal_archive/     # oldest"
				head -7 $TMP2file
				echo
				echo "ls -lrt ${BACKUPfilesystem}/wal_archive/     # latest"
				tail -7 $TMP2file
				echo
				echo "ls -lrt ${BACKUPfilesystem}/wal_archive/*.backup     # recent backups"
				grep ".backup" $TMP2file | tail -4 > $TMP3file
				cat $TMP3file
				WALbackup=`head -1 $TMP3file`
				WALbackup=`echo "/$WALbackup" | rev | cut -d ' ' -f 1 | rev`
				if test $WALbackup != ""
				then
					echo
					sudo /usr/bin/pg_archivecleanup -n  ${BACKUPfilesystem}/wal_archive $WALbackup 2>/dev/null |\
					tee $TMP2file |\
					while read FILE
					do
						MODsecs=`sudo stat -c %Y $FILE 2>/dev/null`	
						echo "${MODsecs}|$FILE"
					done |\
					sort -t '|' -nr -k 1 |\
					cut -d '|' -f 2 |\
					tee $TMP3file |\
					while read FILE
					do
						sudo ls -ld $FILE 2>/dev/null
					done > $TMP4file

					echo
					echo "... first for cleanup ..."
					head -5 < $TMP4file
					
					echo
					echo "... last for cleanup ..."
					tail -5 < $TMP4file

					tr '\n' '\0' < $TMP3file > $TMP5file

					WALusage=`sudo du -ch --files0-from=$TMP5file 2>/dev/null | tail -1`
					WALcount=`wc -l < $TMP2file 2>/dev/null`

					echo
					echo "cleanup $WALcount files ($WALusage) before $WALbackup"
					echo
				fi
			fi >> $TMP1file
			NECHO "backup filesystem:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		fi	

                if $DATABASE_HOST
		then
			psql -d caspidadb -c "select * from replication where type='Filesystem';" 2>/dev/null |\
			sed -e '/^$/d' -e '/ rows*)/d' > $TMP1file
			if test -s $TMP1file
			then
				echo
				NECHO "replication (backup):" "`head -1 $TMP1file`"; echo
				ALL_BUT_FIRST $TMP1file
			fi
		fi

                if $DATABASE_HOST
		then
			PSQLquery="SELECT *, \
    				current_setting('archive_mode')::BOOLEAN AND (last_failed_wal IS NULL OR last_failed_wal <= last_archived_wal) AS is_archiving, \
				CAST (archived_count AS NUMERIC) / EXTRACT (EPOCH FROM age(now(), stats_reset)) AS current_archived_wals_per_second \
			FROM pg_stat_archiver;"

			psql -d caspidadb -x -c "$PSQLquery" 2>/dev/null |\
			sed -e '/^$/d' -e '/ rows*)/d' > $TMP1file
			if test -s $TMP1file
			then
				echo
				NECHO "pg_stat_archiver:" "`head -1 $TMP1file`"; echo
				ALL_BUT_FIRST $TMP1file
			fi
		fi
	fi

        if $uba501feature && $DATABASE_HOST
	then
		PSQLconfd=`sudo find /var/vcap/store/p*ql -type d -name conf.d  2>/dev/null`
		if test "$PSQLconfd" = ""
		then
			PSQLconfd=undetermined
		fi

		PSQLmain=`sudo find /var/vcap/store/p*ql -type d -name main 2>/dev/null`
		if test "$PSQLmain" = ""
		then
			PSQLmain=undetermined
		fi


		PGprog=`find /usr -type f -name pg_controldata 2>/dev/null`
		if test "$PGprog" = ""
		then
			PGprog=undetermined
		fi
		sudo $PGprog -D $PSQLmain > $TMP1file 2>/dev/null
		if test -s $TMP1file
		then
			echo
			NECHO "pg_controldata:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		fi




		PSQLwal=`sudo find /var/vcap/store/p*ql -type d -name pg_wal 2>/dev/null`
		if test "$PSQLwal" = ""
		then
			PSQLwal=undetermined
		fi

		(echo; echo) > $TMP1file
		sudo du -sch "$PSQLwal" >> $TMP1file 2>/dev/null
		sudo ls -lrt "$PSQLwal" > $TMP2file 2>/dev/null
		if test -s $TMP2file
		then
			echo
			echo "ls -lrt ${PSQLwal}/caspida/     # oldest"
			head -7 $TMP2file
			echo
			echo "ls -lrt ${PSQLwal}/caspida/     # latest"
			tail -7 $TMP2file
		fi >> $TMP1file
		if test -s $TMP2file
		then
			echo
                	NECHO "psql pg_wal:" "$PSQLwal"; echo
			ALL_BUT_FIRST $TMP1file
		fi

		ARCHIVEconf="${PSQLconfd}/archiving.conf"

		sudo cat "$ARCHIVEconf" > $TMP1file 2>/dev/null
		echo
		NECHO "archiving.conf" "`sudo ls -l $ARCHIVEconf 2>/dev/null`"; echo
		if test -s $TMP1file
		then
			NECHO "" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		fi 

		RECOVERYconf="${PSQLmain}/recovery.conf"

		sudo cat "$RECOVERYconf" > $TMP1file 2>/dev/null
		echo
		NECHO "recovery.conf" "`sudo ls -l $RECOVERYconf 2>/dev/null`"; echo

		if test -s $TMP1file
		then
			NECHO "" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		fi 
	fi

        GET_CASPIDA_PROPERTY replication.enabled                   # is this a DR cluster?
	REPLICATIONstate=`echo "$PROPERTYvalue" | tr -d ' ' | tr '[:upper:]' '[:lower:]'` 
	echo
	NECHO "replication.enabled:" "replication.enabled=$REPLICATIONstate"; echo

	if test "$REPLICATIONstate" = 'true'
	then
		REPLICATIONcoordinator=true
	fi

	if $REPLICATIONcoordinator
	then
		CASPIDAjobs="/etc/caspida/local/conf/caspida-jobs.json"
		if test -f $CASPIDAjobs
		then
			sed -e 's+//.*$++' -e '/^.\*/d' < $CASPIDAjobs > $TMP1file 
                        if python -m json.tool < $TMP1file > $TMP2file 2>&1
			then
				ERROR=""
			else
				ERROR="   <== JSON syntax error in $CASPIDAjobs"
				INCREMENT_EXCEPTIONS
				cat $TMP1file >> $TMP2file
			fi 
			if test -s $TMP2file
			then
				echo
				NECHO "caspida-jobs.json:" "`head -1 $TMP2file`"; echo $ERROR
				ALL_BUT_FIRST $TMP2file
			fi
		fi
				
		tail -75 /var/log/caspida/replication/replication.log 2>/dev/null > $TMP1file
		if test -s $TMP1file
		then
			echo
			NECHO "replication log:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
			echo
		fi
	fi

	if $DATABASE_HOST
	then
		for PSQLquery in 'select * from pg_publication;' 'select * from pg_replication_slots;'
		do
			psql -d caspidadb -c "$PSQLquery" 2>/dev/null > $TMP1file
			ROWcount=`wc -l < $TMP1file`
			if test "$ROWcount" = "4"
			then
				continue
			else
				cat $TMP1file
			fi
		done > $TMP2file
		if test -s $TMP2file
		then
			echo
			NECHO "replication tables:" "`head -1 $TMP2file`"; echo
			ALL_BUT_FIRST $TMP2file
		fi
	fi

	if test "$REPLICATIONstate" = 'true'
	then
		NECHO "replication nodes:" ""; echo $REPLICATION_NODE_list
                if $DATABASE_HOST
		then
			psql -d caspidadb -c 'select * from replication;' 2>/dev/null | egrep -v ' rows*\)' > $TMP1file
			if test -s $TMP1file
			then
				Rtype=`cat $REPLtype 2>/dev/null`
				NECHO "replication ($Rtype):" "`head -1 $TMP1file`"; echo
				ALL_BUT_FIRST $TMP1file
				echo
			fi
	
			NUMnodes=`echo "$REPLICATION_NODE_list" | sed -e 's/[^,]//g' | wc -c`
			if test "$NUMnodes" = "20"
			then
				DBpeer=`echo "$REPLICATION_NODE_list" | cut -d ',' -f 2`
			else
				DBpeer=`echo "$REPLICATION_NODE_list" | cut -d ',' -f 1`
			fi

			ssh $DBpeer "psql -d caspidadb -c 'select * from replication;' 2>$TMP2file" |\
			egrep -ve ' rows*\)$' >$TMP1file

			cat $TMP2file >> $TMP1file

			NECHO "replication ($DBpeer):" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
			echo
		fi

		if $LEADERnode
		then
			SSHconf="/opt/caspida/conf/replication/ssh_config"
			if test -s $SSHconf
			then
				NECHO "replication ssh_config:" "$SSHconf"; echo
				NECHO "" "`head -1 $SSHconf`"; echo
				ALL_BUT_FIRST $SSHconf
				echo
			fi
		fi
	fi

	if $DATABASE_HOST
	then
		echo
		psql -d caspidadb -c "select * from entityAttributeMappings;" 2>/dev/null > $TMP1file
		NECHO "entityAttributeMappings:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file

		echo
		> $TMP3file		# make sure file is empty

		if $uba41feature
		then
			DEFAULTfile=$TMP/configuration/EntityValidations.json
			(cd $TMP; jar xf /opt/caspida/lib/CaspidaSecurity.jar configuration/EntityValidations.json ) # extract file from JAR
		else
			DEFAULTfile=/opt/caspida/conf/etl/configuration/EntityValidations.json
		fi

		if $uba40feature
		then
			if grep '"generic" : \["", "-", "null", "0x0"\]' /etc/caspida/local/conf/etl/configuration/EntityValidations.json > $TMP1file 2>/dev/null
			then
				NECHO "EntityValidations.json:" "`head -1 $TMP1file`";
				echo "   <== obsolete entry; /etc/caspida/local/conf/etl/configuration/EntityValidations.json must be corrected"
				INCREMENT_EXCEPTIONS
			fi
		fi

	        for FILE in $DEFAULTfile /etc/caspida/local/conf/etl/configuration/EntityValidations.json
        	do
        		if test -f $FILE
                	then
                                case $FILE in
                                        /opt/*) LABEL="internalIPRange (d):"; SOURCEfile=$DEFAULTfile;;
                                        $TMP/*) LABEL="internalIPRange (d):"; SOURCEfile=/opt/caspida/lib/CaspidaSecurity.jar;;
                                        *)      LABEL="internalIPRange (l):"; SOURCEfile=$FILE;;
                                esac

                                NECHO "$LABEL" "$FILE"
                                sed -e 's+//.*$++' -e '/^.\*/d' < $FILE > $TMP1file
				if test -s $TMP1file
				then
                                	if ! python -m json.tool < $TMP1file > $TMP2file 2>/dev/null
                                	then
                                        	echo "   <== invalid JSON syntax; needs to be corrected"
                                        	INCREMENT_EXCEPTIONS
                                        	continue                # can't proceed with this file
					else
						echo "   (okay)"
                                	fi
				else
					echo "   (empty)"
					continue
				fi
				GEOcount=`grep '"location"' $TMP2file | wc -l`
				grep ": " $TMP2file | egrep -ve '"internalGeoAttributions":|"cidr":|"location":|"city":|"countryCode":|"latitude":|"longitude":|"internalIPRange":|"invalidValues":|"user":|"generic":|"application":|"resource":|"device":|"validations":|"url":|"data":' > $TMP1file
				if test -s $TMP1file
				then
                                        NECHO "$LABEL" "$FILE   <== unexpected JSON label; needs to be corrected"; echo
                                        INCREMENT_EXCEPTIONS
					NECHO "" "`head -1 $TMP1file`"; echo
					ALL_BUT_FIRST $TMP1file
				fi
                                sed -n '/"internalIPRange"/,/]/p' < $TMP2file | tr -d '         ][}' > $TMP1file

                                if egrep -qe '"internalIPRange":.*,' $TMP1file
                                then
                                        IPlist=""       # is empty
                                else
                                        echo `cat $TMP1file` > $TMP2file
                                        IPlist=`cut -d ':' -f 2 < $TMP2file`
                                fi

                                if test -z "$IPlist"
                                then
                                        NECHO "$LABEL" "[]"
                                        echo "  $FILE is empty  <== update with internal, routable, non-private IP ranges for proper devica escoping"
                                        INCREMENT_EXCEPTIONS
                                else
                                        echo $SOURCEfile > $TMP1file
                                        echo "$IPlist" | sed -e 's/,/, /g' | fmt -w 60 | sed -e 's/, /,/g' -e '$ s/,$//' | tee -a $TMP3file >> $TMP1file
                                        if egrep -qe '/[0-7]"' $TMP1file
                                        then
                                                ERROR="   <== contains CIDR suffix lower than 8"
                                                INCREMENT_EXCEPTIONS
                                        else
                                                ERROR=""
                                        fi
                                        NECHO "$LABEL" "`head -1 $TMP1file`"; echo "$ERROR"
                                        ALL_BUT_FIRST $TMP1file
                                fi
				NECHO "geo locations:" "$GEOcount"; echo
				echo
                        fi
                done	

		if test -s $TMP3file
		then
			echo
			sed -e 's/"//g' -e 's/ //g' -e 's/,/\n/g' -e '/^$/d' <$TMP3file | tee $TMP2file | sort -u -n -t . -k 1,1 -k 2,2 -k 3,3 -k 4,4 | sed -e '/^$/d' > $TMP1file
			IPtotal=`wc -l < $TMP2file`

			egrep -ve '10\.[0-9]+\.[0-9]+\.[0-9]+/[0-9]+|192\.168\.[0-9]+\.[0-9]+/[0-9]+|169\.254\.[0-9]+\.[0-9]+/[0-9]+|127\.[0-9]+\.[0-9]+\.[0-9]+/[0-9]+|172\.16\.0\.0/12|172.1[7-9]\.[0-9]+\.[0-9]+/[0-9]+|172.2[0-9]\.[0-9]+\.[0-9]+/[0-9]+|172.3[0-2]\.[0-9]+\.[0-9]+/[0-9]+' < $TMP1file > $TMP2file
			IPpublic=`wc -l < $TMP2file`
			NECHO "internal public IP:" "`head -1 $TMP2file`"
			if test $IPpublic -lt 1
			then
				echo "  <= verify internal, routable, non-private IP ranges are correct"
				> $INTPUBfile
			else
				cp $TMP2file $INTPUBfile
				echo "   ($IPpublic public of $IPtotal entries)"
			fi	
			ALL_BUT_FIRST $TMP2file
			echo

			if $DATABASE_HOST
			then
				CIDRvalues=""
        			while read CIDR
        			do
                			if test -z "$CIDRvalues"
                			then 
                        			CIDRvalues="isipaddress(networkid) AND ( networkid::inet << inet '$CIDR'"
                			else
               	         			CIDRvalues="$CIDRvalues OR networkid::inet << inet '$CIDR'"
               	 			fi
        			done < $TMP1file 
				CIDRvalues="$CIDRvalues )"
				psql -d caspidadb -t -c "select networkid from systems where scope='External' AND $CIDRvalues;" 2>/dev/null |\
				sed -e 's/\s//g' -e '/^$/d' > $TMP1file

				misScoped=`wc -l < $TMP1file | sed -e 's/\s//g'`
				if test "$misScoped" -gt "0"
				then
					NECHO "misScoped:" "$misScoped"; echo "   <= some 'Internal' IPs have been scoped 'External'"
					head -5 $TMP1file > $TMP2file
					NECHO "" "`head -1 $TMP2file`"; echo
					ALL_BUT_FIRST $TMP2file
				fi
			fi
		fi
	fi

	
) | tee -a $OUTfile

if $UISERVER_HOST
then
	if test -f /etc/caspida/local/conf/splunk_search_head.json
	then
		python -m json.tool < /etc/caspida/local/conf/splunk_search_head.json > $TMP1file 2>/dev/null
		if test -s $TMP1file
		then
			SEARCHhead=`grep -m 1 '"host"' $TMP1file | sed -e 's/^.*"host": "//' -e 's/",$//'` 
			if test "$SEARCHhead" != ""
			then
				SEARCHip=`getent ahostsv4 "$SEARCHhead" 2>/dev/null | cut -d " " -f 1 | sort -u | head -1`
				case "$SEARCHip" in
					10.*)		CHECKpublic=false;;
					192.168.*)	CHECKpublic=false;;
					172.16.*)	CHECKpublic=false;;
					*)		CHECKpublic=true;;
				esac
			else
				CHECKpublic=false
			fi
		else
			CHECKpublic=false
		fi
	else
		CHECKpublic=false
	fi
	
	UIpublic=""
	if $CHECKpublic
	then
	        echo
        	NECHO "ui public ip:" ""
        	if which dig >/dev/null 2>&1
        	then
                	if dig +short myip.opendns.com @resolver1.opendns.com 2>/dev/null > $TMP1file
                	then
                        	UIpublic=`head -1 $TMP1file`
				echo $UIpublic
                	else
                        	echo '""   (dig failed)'
                	fi
        	else
                	echo '""   (dig not available)'
     		fi
	fi
 
	GET_CASPIDA_PROPERTY uiServer.host
	UIserver=$PROPERTYvalue
	NECHO "uiServer.host:" "'$UIserver'"
	if test "$UIserver" = ""
	then
		echo "   (not defined)"
	else
		echo
	fi
	
	echo
	for UIurl in $UIserver $UIpublic $UISERVER_HOST_list
	do
		NECHO "ui connect:" "$UIurl"
		if curl --connect-timeout 10 -kIsS https://${UIurl}/?loginType=uba  > $TMP1file 2>&1
		then
			echo "   (success)"
			break
		else
			echo "   <= curl failed to ui $UIurl"
			NECHO "" ""
			head -1 $TMP1file
			ALL_BUT_FIRST $TMP1file
		fi
	done

	tail -500 /var/log/caspida/ui/log.log > $TMP1file 2>/dev/null

	egrep -e "[^@]@[^@]" $TMP1file 2>/dev/null |\
	sed -e 's/^.*\s\([^@][^@]*@[^@][^@]*\)\s.*$/\1/' -e 's/,//g' -e 's/user=//' -e 's/\(\S\S*\).*$/\1/' 2>/dev/null |\
	sort -u |\
	egrep -ve "^202[0-5]-[0-1][0-9]-[0-3][0-9]" |\
	while read EMAIL
	do
		EMAILmasked=`TRANSLATE "$EMAIL" 2>/dev/null`
		if test "$EMAILmasked" = ""
		then
			continue
		fi
		sed -ie "s/$EMAIL/$EMAILmasked/g" $TMP1file 2>/dev/null
	done

	tail -50 $TMP1file > $TMP2file

	if egrep -qe "\s*ssoLogin:|info:\s_onLogin:|/saml/login|/saml/logout" $TMP1file 2>/dev/null
	then
		echo >> $TMP2file
		echo "   ... SSO ..." >> $TMP2file
		echo >> $TMP2file
		egrep -e "\s*ssoLogin:|info:\s_onLogin:|info:\s[lL]ogin|/saml/login|/saml/logout" $TMP1file |\
		tail -15 >> $TMP2file
	fi

	if test -s $TMP2file
	then
		echo
		NECHO "ui/log.log:" "`head -1 $TMP2file`"; echo
		ALL_BUT_FIRST $TMP2file
	fi

	ls -laR /var/vcap/store/caspida/certs*  2>/dev/null > $TMP1file
	if test -s $TMP1file
	then
		echo
		NECHO "ui certs:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	fi

	ROOTCA="/var/vcap/store/caspida/certs/my-root-ca.crt.pem"
	PRIVATEKEY="/var/vcap/store/caspida/certs/my-server.key.pem"
	SERVERCERT="/var/vcap/store/caspida/certs/my-server.crt.pem"
	if $uba42feature
	then
		GET_CASPIDA_PROPERTY ui.auth.rootca
                if test -n "$PROPERTYvalue"
		then
			ROOTCA="$PROPERTYvalue"
		fi	
		GET_CASPIDA_PROPERTY ui.auth.privateKey
                if test -n "$PROPERTYvalue"
		then
			PRIVATEKEY="$PROPERTYvalue"
		fi	
		GET_CASPIDA_PROPERTY ui.auth.serverCert
                if test -n "$PROPERTYvalue"
		then
			SERVERCERT="$PROPERTYvalue"
		fi	
	fi
	
	echo
	NECHO "root ca:" "$ROOTCA"
	openssl x509  -text -noout -in "$ROOTCA" 2>$TMP2file |\
	sed -e 's/[0-9A-Fa-f]\{2,2\}:[0-9A-Fa-f]\{2,2\}/XX:XX/g' -e 's/:XX:[0-9A-Fa-f]\{2,2\}\(:*\)$/:XX:XX\1/' -e 's/^[0-9A-Fa-f]\{2,2\}$/XX/' > $TMP1file
	if test -s $TMP1file
	then
		echo
	else
		cp $TMP2file $TMP1file
		echo "   <== certificate error; must be corrected"
		INCREMENT_EXCEPTIONS
	fi
	echo
	NECHO "" "`head -1 $TMP1file`"; echo
	ALL_BUT_FIRST $TMP1file
	
	echo
	NECHO "private key:" "$PRIVATEKEY"
	openssl rsa  -text -noout -in "$PRIVATEKEY" 2>$TMP2file |\
	sed -e 's/[0-9A-Fa-f]\{2,2\}:[0-9A-Fa-f]\{2,2\}/XX:XX/g' -e 's/:XX:[0-9A-Fa-f]\{2,2\}\(:*\)$/:XX:XX\1/' -e 's/^[0-9A-Fa-f]\{2,2\}$/XX/' > $TMP1file
	if test -s $TMP1file
	then
		echo
	else
		cp $TMP2file $TMP1file
		echo "   <== certificate error; must be corrected"
                INCREMENT_EXCEPTIONS
	fi
	echo
	NECHO "" "`head -1 $TMP1file`"; echo
	ALL_BUT_FIRST $TMP1file
	
	echo
	NECHO "server cert:" "$SERVERCERT"
	openssl x509  -text -noout -in "$SERVERCERT" 2>$TMP2file |\
	sed -e 's/[0-9A-Fa-f]\{2,2\}:[0-9A-Fa-f]\{2,2\}/XX:XX/g' -e 's/:XX:[0-9A-Fa-f]\{2,2\}\(:*\)$/:XX:XX\1/' -e 's/^[0-9A-Fa-f]\{2,2\}$/XX/' > $TMP1file
	if  test -s $TMP1file
	then
		echo
	else
		cp $TMP2file $TMP1file
		echo "   <== certificate error; must be corrected"
                INCREMENT_EXCEPTIONS
	fi
	echo
	NECHO "" "`head -1 $TMP1file`"; echo
	ALL_BUT_FIRST $TMP1file
	
fi | tee -a $OUTfile

if $DATABASE_HOST
then

	if $uba40feature 
	then
		psql -d caspidadb -c "select * from appconfig;" 2>/dev/null | egrep -v ' rows*\)' >$TMP1file
		echo
		NECHO "app config:" "`head -1 $TMP1file`"; echo
        	ALL_BUT_FIRST $TMP1file

		psql -d caspidadb -t -c "select value from appconfig where name='settings'" 2>/dev/null >$TMP1file
		if test -s $TMP1file
		then
			if python -m json.tool < $TMP1file >$TMP2file 2>&1
			then
				ERROR=""
			else
				ERROR= "   <== UI will not start with invalid settings"
				INCREMENT_EXCEPTIONS
			fi
			NECHO "ui settings:" "`head -1 $TMP2file`"; echo "$ERROR"
			ALL_BUT_FIRST $TMP2file
		fi
	fi

	PSQLquery="$OBSCUREfunction; \
		select CASE WHEN email='admin' THEN 'admin' ELSE pg_temp.obscure(email) END as email, \
		CASE when email='admin' THEN substring(password,20,8) ELSE '********' END as hash, \
		role,date_trunc('Seconds', creationtime) as creationtime from accounts;"

	psql -d caspidadb -c "$PSQLquery" 2>/dev/null | egrep -v ' rows*\)' >$TMP1file
	if test -s $TMP1file
	then
		echo
		NECHO "ui accounts (local):" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	fi

	PSQLquery="select * from accountroles;"

        psql -d caspidadb -c "$PSQLquery" 2>/dev/null | egrep -v ' rows*\)' >$TMP1file
        if test -s $TMP1file
        then
                echo
                NECHO "ui account roles:" "`head -1 $TMP1file`"; echo
                ALL_BUT_FIRST $TMP1file
        fi


		#
		#	Check sizing limits exceeded
		#
		case "$NUMnodes" in
			1)	MAXeps=4; MAXdevices=100; MAXaccounts=50; MAXetl=6; NUMvalid=true;; 	# all values are K
			3)	MAXeps=12; MAXdevices=200; MAXaccounts=50; MAXetl=10; NUMvalid=true;; 	
			5)	MAXeps=20; MAXdevices=300; MAXaccounts=200; MAXetl=12; NUMvalid=true;; 
			7)	MAXeps=28; MAXdevices=500; MAXaccounts=350; MAXetl=24; NUMvalid=true;;
			10)	MAXeps=45; MAXdevices=500; MAXaccounts=350; MAXetl=32; NUMvalid=true;;
			20)	MAXeps=80; MAXdevices=1000; MAXaccounts=750; MAXetl=64; 
				if $uba41feature
				then
					NUMvalid=true
				else
					NUMvalid=false
				fi;;
			*)	MAXeps=4; MAXdevices=100; MAXaccounts=50; MAXetl=6; NUMvalid=false;;
		esac

		if ! $MULTInode
		then
			if $uba41feature
			then
				SUPERnode=`cat $SNODEfile`
				if $SUPERnode
				then
					MAXeps=28
					MAXdevices=300
					MAXaccounts=250
					MAXetl=24
					NECHO "super node" ""
					echo "eps: ${MAXeps}K; dev: ${MAXdevices}K; accounts: ${MAXaccounts}K; etl: $MAXetl"
				fi
			fi
		fi
		
		MAXeps=`expr $MAXeps \* 1000`
		MAXdevices=`expr $MAXdevices \* 1000`
		MAXaccounts=`expr $MAXaccounts \* 1000`
		echo $MAXeps > $MAXEPSfile
		echo $MAXetl > $MAXETLfile

		if $uba32feature
		then
			psql -d caspidadb -c 'select count(*),scope,state from v_systems group by scope,state order by scope desc, state desc;' 2>/dev/null |\
			egrep -v ' rows*\)' > $TMP1file 
			echo
			NECHO "device state (v_systems):" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file

			psql -d caspidadb -c "select dataformat, datasourcename, count(*) from v_systems where state='Unresolved' AND isipaddress(name) AND (name::inet << '10.0.0.0/8' OR name::inet << '192.168.0.0/16' OR  name::inet << '172.16.0.0/12') group by 1,2 order by 1,2 ;" 2>/dev/null |\
			egrep -v ' rows*\)' > $TMP1file
			if test -s $TMP1file
			then
				echo
				NECHO "rfc1928 unresolved:" "`head -1 $TMP1file`"; echo
				ALL_BUT_FIRST $TMP1file
			fi

			if test -s $INTPUBfile
			then
				awk -f $INTPUBawk < $INTPUBfile |\
				sed -e :a -e '$!N; s/\n//; ta' > $TMP1file
				INTPUB="`cat $TMP1file`"
			else
				INTPUB=""
			fi
		
			NETIDtype="	CASE \
						WHEN ismacaddress(networkid) \
							THEN 'macaddr' \
						WHEN lower(networkid) ~ '^[0-9a-f]{12,12}\$' \
							THEN '12-hex' \
                                        	WHEN NOT isipaddress(networkid) \
                                                        THEN 'unknown' \
                                                WHEN networkid::inet << inet '$DOCKERcidr' \
							THEN 'docker0' \
                                                WHEN (networkid::inet << inet '10.0.0.0/8' \
                                                                OR networkid::inet << inet '192.168.0.0/16' \
                                                                OR networkid::inet << inet '172.16.0.0/12' \
                                                                OR networkid::inet << inet'169.254.0.0/16') \
                                                        THEN 'private' \
                                                WHEN networkid::inet << inet '127.0.0.0/8' \
							THEN 'loopback' \
                                                $INTPUB ELSE 'public' \
                                        END "  

			PSQLquery="SELECT scope, state, \
					$NETIDtype AS netid_type, \
				count(*) from systems \
				GROUP BY scope,state,netid_type  ORDER BY scope desc, state desc, netid_type;"

			cat > $PSQLfile <<EndOfData
$PSQLquery
EndOfData


			psql -d caspidadb -f "$PSQLfile" 2>/dev/null |\
			egrep -v ' rows*\)' > $TMP1file 
			echo
			NECHO "device state (systems):" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file

			PSQLquery="SELECT s.scope as scope, s.state as state, \
					'public' AS netid_type, \
					ds.name as datasource_name, \
					count(*) from systems s \
				LEFT OUTER JOIN datasources ds ON s.datasourceid=ds.id \
				WHERE 'public'=$NETIDtype \
 				AND s.scope='Internal' \
				GROUP BY scope,state,netid_type,datasource_name  \
				ORDER BY scope, state, netid_type, datasource_name;"

                        cat > $PSQLfile <<EndOfData
$PSQLquery
EndOfData

			PSQLtimeout=""
			if $MEMBERnode
			then
				if which timeout >/dev/null 2>&1
				then
					PSQLtimeout="timeout 600"
				fi
			fi


			$PSQLtimeout  psql -d caspidadb -f "$PSQLfile" 2>/dev/null |\
			sed -e '/^$/d' -e '/ row.*)/d' > $TMP1file
			if test -s $TMP1file
			then
				ROWcount=`wc -l < $TMP1file`
				if test $ROWcount -gt 2
				then
					echo
					NECHO "pub source (systems):" "`head -1 $TMP1file`"; echo
					ALL_BUT_FIRST $TMP1file
				fi
			fi

			PSQLquery="SELECT s.scope as scope, s.state as state, \
					'public' AS netid_type, \
					ds.name as datasource_name, \
					s.networkid as networkid \
					FROM systems s \
				LEFT OUTER JOIN datasources ds ON s.datasourceid=ds.id \
				WHERE 'public'=$NETIDtype \
 				AND s.scope='Internal' \
				AND ds.name like '_%' \
				LIMIT 10;"

                        cat > $PSQLfile <<EndOfData
$PSQLquery
EndOfData
			$PSQLtimeout  psql -d caspidadb -f "$PSQLfile" 2>/dev/null |\
			sed -e '/^$/d' -e '/ row.*)/d' > $TMP1file
			if test -s $TMP1file
			then
				ROWcount=`wc -l < $TMP1file`
				if test $ROWcount -gt 2
				then
					echo
					NECHO "pub sample (systems):" "`head -1 $TMP1file`"; echo
					ALL_BUT_FIRST $TMP1file
				fi
			fi

			PSQLquery="SELECT s.id as id \
					FROM systems s \
				LEFT OUTER JOIN datasources ds ON s.datasourceid=ds.id \
				WHERE 'public'=$NETIDtype \
 				AND s.scope='Internal' \
				ORDER by ds.name;"

                        cat > $PSQLfile <<EndOfData
$PSQLquery
EndOfData
			$PSQLtimeout  psql -d caspidadb -t -f "$PSQLfile" 2>/dev/null |\
			sed -e '/^$/d' -e '/ row.*)/d' > $TMP1file
			if test -s $TMP1file
			then
				INFOcount=0
				while read ID
				do
					if psql -d caspidadb -t -c "select event from  devicerawevents  where entityid='$ID';" 2>/dev/null | python -m json.tool 2>/dev/null
					then
						INFOcount=`expr $INFOcount + 1`
						echo
					fi
					if test $INFOcount = 5
					then
						break
					fi
				done < $TMP1file > $TMP2file
				if test -s $TMP2file
				then
					echo
					NECHO "public raw (systems):" "`head -1 $TMP2file`"; echo
					ALL_BUT_FIRST $TMP2file
				fi
			fi
		fi

		if $uba50feature
		then
			PSQLquery="SELECT scope, state, \
					$NETIDtype AS netid_type, \
				count(*) from usystems \
				GROUP BY scope,state,netid_type  ORDER BY scope desc, state desc, netid_type;"

			psql -d caspidadb -c "$PSQLquery" 2>/dev/null |\
			egrep -v ' rows*\)' > $TMP1file 
			echo
			NECHO "device state (usystems):" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file

			PSQLquery="SELECT s.scope as scope, s.state as state, \
					'public' AS netid_type, \
					ds.name as datasource_name, \
					count(*) from usystems s \
				LEFT OUTER JOIN datasources ds ON s.datasourceid=ds.id \
                                WHERE 'public'=$NETIDtype \
                                AND s.scope='Internal' \
				GROUP BY scope,state,netid_type,datasource_name  \
				ORDER BY scope, state, netid_type, datasource_name;"

			$PSQLtimeout  psql -d caspidadb -c "$PSQLquery" 2>/dev/null |\
			sed -e '/^$/d' -e '/ row.*)/d' > $TMP1file
			if test -s $TMP1file
			then
				ROWcount=`wc -l < $TMP1file`
				if test $ROWcount -gt 2
				then
					echo
					NECHO "pub source (usystems):" "`head -1 $TMP1file`"; echo
					ALL_BUT_FIRST $TMP1file
				fi
			fi
			PSQLquery="SELECT s.scope as scope, s.state as state, \
					'public' AS netid_type, \
					ds.name as datasource_name, \
					s.networkid as networkid \
					FROM usystems s \
				LEFT OUTER JOIN datasources ds ON s.datasourceid=ds.id \
				WHERE 'public'=$NETIDtype \
 				AND s.scope='Internal' \
				AND ds.name like '_%' \
				LIMIT 10;"

			$PSQLtimeout  psql -d caspidadb -c "$PSQLquery" 2>/dev/null |\
			sed -e '/^$/d' -e '/ row.*)/d' > $TMP1file
			if test -s $TMP1file
			then
				ROWcount=`wc -l < $TMP1file`
				if test $ROWcount -gt 2
				then
					echo
					NECHO "pub sample (usystems):" "`head -1 $TMP1file`"; echo
					ALL_BUT_FIRST $TMP1file
				fi
			fi

			PSQLquery="SELECT s.id as id \
					FROM usystems s \
				LEFT OUTER JOIN datasources ds ON s.datasourceid=ds.id \
				WHERE 'public'=$NETIDtype \
 				AND s.scope='Internal' \
				ORDER by ds.name;"

			$PSQLtimeout  psql -d caspidadb -t -c "$PSQLquery" 2>/dev/null |\
			sed -e '/^$/d' -e '/ row.*)/d' > $TMP1file
			if test -s $TMP1file
			then
				INFOcount=0
				while read ID
				do
					if psql -d caspidadb -t -c "select event from  devicerawevents  where entityid='$ID';" 2>/dev/null | python -m json.tool 2>/dev/null
					then
						INFOcount=`expr $INFOcount + 1`
						echo
					fi
					if test $INFOcount = 5
					then
						break
					fi
				done < $TMP1file > $TMP2file
				if test -s $TMP2file
				then
					echo
					NECHO "public raw (usystems):" "`head -1 $TMP2file`"; echo
					ALL_BUT_FIRST $TMP2file
				fi
			fi
		fi

		

		if $uba32feature
		then
                	SCANNEDsubnets=`psql -d caspidadb -c "select distinct network((name||'/24')::inet) as net,count(*) from systems where scope='Internal' AND state='Unresolved' AND isipaddress(name) group by net order by 2 desc;" 2>/dev/null | grep -e '256$' | tee $TMP1file | wc -l`
		else
                	SCANNEDsubnets=`psql -d caspidadb -c "select distinct network((networkid||'/24')::inet) as net,count(*) from systems where scope='Internal'  AND networkid ~ '^[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\$' group by net order by 2 desc;" 2>/dev/null | egrep -e '256$' | tee $TMP1file | wc -l`

		fi
                if test "$SCANNEDsubnets" \> 0
                then
			cp $TMP1file $SEQUENTIALsubnets
			echo
                        NECHO "unresolved subnets:" "$SCANNEDsubnets"
			if test "$SCANNEDsubnets" -gt 64
			then
				echo "   (check for sequential IP assignment by scanners or firewalls in network)"	
			else
				echo
			fi
			
			echo
			NECHO "unresolved subnet sample:" "`head -1 $TMP1file`"; echo
			head -5 $TMP1file > $TMP2file
			ALL_BUT_FIRST $TMP2file
                fi


		(
			psql -d caspidadb -t -c "select scope, 'normal', state, count(*) from systems where not (isipaddress(name) or ismacaddress(name) or  lower(name) ~ '^[0-9a-f]{12,12}\$' or lower(name) ~ '^.*\..*\.[a-z]+\$') group by scope, state;"

			psql -d caspidadb -t -c "select scope, '12-hex', state, count(*) from systems where lower(name) ~ '^[0-9a-f]{12,12}\$' group by scope, state;"

			# psql -d caspidadb -t -c "select scope, 'fqdn', state, count(*) from systems where lower(name) ~ '^.*\..*\.[a-z]+\$'  group by scope, state;"
			psql -d caspidadb -t -c "select scope, 'fqdn', state, count(*) from systems where NOT (isipaddress(name) OR ismacaddress(name)) AND name ilike '%.%' group by scope, state;"

			psql -d caspidadb -t -c "select scope,'ipaddr',state, count(*) from systems where isipaddress(name) group by scope, state;"

			psql -d caspidadb -t -c "select scope,'macaddr',state, count(*) from systems where ismacaddress(name) group by scope, state ;"

			psql -d caspidadb -t -c "select scope, 'total', '          ', count(*) from systems group by scope;"

		) 2>/dev/null |\
		egrep -v -e '^$' | tr -d ' ' | awk 'BEGIN {FS="|"}1{printf "%-8.8s | %-7.7s | %-10.10s | %9d\n", $1,$2,$3,$4}' | sort > $TMP1file

		echo
		NECHO "device names:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file

		psql -d caspidadb -t -c "select name from systems where scope='Internal' and lower(name) ~ '^[0-9a-f]{12,12}\$' limit 5;" 2>/dev/null | sed -e '/^$/ d' >$TMP1file
		if test -s $TMP1file
		then
			echo
			NECHO "12-hex sample:" ""; head -1 $TMP1file
			ALL_BUT_FIRST $TMP1file
		fi

		# psql -d caspidadb -t -c "select regexp_replace(name,  '^[^\.]+\.', '') from systems where scope='Internal' and lower(name) ~ '^.*\..*\.[a-z]+\$';" |\
		psql -d caspidadb -t -c "select regexp_replace(name,  '^[^\.]+\.', '') from systems where NOT (isipaddress(name) OR ismacaddress(name)) AND scope='Internal' AND name ilike '%.%';" 2>/dev/null |\
		sed -e 's/^ //' -e '/^$/d' |\
		sort |\
		uniq -c |\
		sed -e '/^  *1 /d' |\
		tr '[:upper:]' '[:lower:]' |\
		sort -b -n -r  > $TMP1file
		if test -s $TMP1file
		then
			GET_CASPIDA_PROPERTY identity.resolution.device.domains.list
			DOMAINlist=`echo $PROPERTYvalue | tr '[:upper:]' '[:lower:]' | tr ',' ' '`
			for DOMAIN in $DOMAINlist
			do
				if egrep -q " $DOMAIN\$" $TMP1file
				then
					sed -i "s/ $DOMAIN\$/ $DOMAIN   <= device normalization expected/" $TMP1file
				fi
			done
			echo
			NECHO "fqdn devices/domains:" ""; head -1 $TMP1file
			ALL_BUT_FIRST $TMP1file
		fi

		if $uba431feature
		then
			echo
			NECHO "irscan information:" ""
			/opt/caspida/bin/irscan -C -t Unfiltered > $TMP1file 2>/dev/null
			if test -s $TMP1file
			then
				head -1 $TMP1file
				ALL_BUT_FIRST $TMP1file
			else
				echo "(no results)"
			fi
		fi

		grep ERROR /var/log/caspida/system/IRBlackListUpdateExecutor.log 2>/dev/null | tail -10 > $TMP1file
		if test -s $TMP1file
		then
			echo
			NECHO "last irblock errs:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		fi

		echo
		# COUNT=`psql -d caspidadb -t -c "select count(*) from systems where scope='Internal' AND state='Resolved' AND score!=0 AND lower(name) ~ '^.*\..*\.[a-z]+$';" | sed 's/ //g'`
		COUNT=`psql -d caspidadb -t -c "select count(*) from systems where NOT (isipaddress(name) OR ismacaddress(name)) AND name ilike '%.%' AND scope='Internal' AND state='Resolved' AND score!=0;" 2>/dev/null | sed 's/ //g'`
		NECHO "anomalous fqdn:" "$COUNT"; echo
		COUNT=`psql -d caspidadb -t -c "select substring(name,1,case when position('.' in name)= 0 then length(name) else position('.' in name)-1 end), count(*) from systems where NOT (isipaddress(name) OR ismacaddress(name)) group by 1 having count(*) >= 2;" 2>/dev/null | sed -e '/^$/ d' | wc -l`
		NECHO "duplicate fqdn:" "$COUNT"; echo

		echo
		psql -d caspidadb -t -c "select summary from anomalies where anomalytype='UnusualDomainName' AND status='Active' AND summary ilike 'AGD%';" 2>/dev/null |\
		sed -e'/^$/d' -e 's/^ AGD ..//' -e 's/\*\*.*$//' -e 's/^www\.//'|\
		sort | uniq -c |\
		sort -nr > $TMP1file
		NECHO "AGD sample:" "(total count `wc -l < $TMP1file`)"; echo
		head -10 $TMP1file | sed -e "s/^/$INDENT/"

		(
			psql -d caspidadb -t -c "select 'shortname', count(*) from urls where NOT isipaddress(name) AND NOT name ilike '%.%';"
			psql -d caspidadb -t -c "select 'ipaddr', count(*) from urls where isipaddress(name);"
			psql -d caspidadb -t -c "select 'fqdn', count(*) from urls where NOT isipaddress(name) AND name ilike '%.%';"
			psql -d caspidadb -t -c "select 'total', count(*) from urls;"
		) 2>/dev/null |\
                egrep -v -e '^$' | tr -d ' ' | awk 'BEGIN {FS="|"}{printf "%-9.9s | %9d\n", $1,$2}' | sort > $TMP1file

		echo
		NECHO "domain names:" "`head -1 $TMP1file`"; echo
                ALL_BUT_FIRST $TMP1file

		if ! $uba42feature
		then
			if test -f /var/log/caspida/system/deleteDevices.log
			then
				egrep -e "^==== starting ====|^Number of" /var/log/caspida/system/deleteDevices.log | tail -6 > $TMP1file
				echo
				NECHO "nightly device cleanup:" "`head -1 $TMP1file`"; echo
				ALL_BUT_FIRST $TMP1file
			fi
		fi

		echo


		for DEVICEtable in systems irsystems
		do
			cat > $TMP3file <<EndOfData
BEGIN;
SET LOCAL enable_indexscan = off;
SET LOCAL enable_bitmapscan = off;
SET LOCAL enable_indexonlyscan = off;
SELECT id, count(*) FROM $DEVICEtable GROUP BY id HAVING count(*) > 1;
ROLLBACK;
EndOfData
			psql -d caspidadb -f $TMP3file 2>/dev/null |\
			grep row | grep -v '(0 rows' > $TMP1file
			if test -s $TMP1file
			then
				echo -n "$DEVICEtable: "
				cat $TMP1file
			fi
		done > $TMP2file
		if test -s $TMP2file
		then
			echo 
			NECHO "duplicate id:" "`head -1 $TMP2file`"; echo "   <== psql duplicate id; UBA-13513"
			ALL_BUT_FIRST $TMP2file
			INCREMENT_EXCEPTIONS
		fi

		NECHO "sizing ..." ""; echo
		psql -d caspidadb -c "select scope,count(*) from v_systems where state='Resolved' group by 1;" 2>/dev/null | tr -d ' '  > $TMP1file # get devices
		DEVinternal=`grep Internal $TMP1file | cut -d '|' -f 2` 
		DEVexternal=`grep External $TMP1file | cut -d '|' -f 2` 
		NECHO "  devices (resolved):" "internal: $DEVinternal; external: $DEVexternal"
		if test -z "$DEVinternal"
		then
			DEVinternal=0
		fi
		if test -z "$DEVexternal"
		then
			DEVexternal=0
		fi
		DEVtotal=`expr $DEVinternal + $DEVexternal`
		if test $DEVtotal -le  $MAXdevices
		then
			echo
		else	
			echo "   <== exceeds device sizing $MAXdevices for $NUMnodes nodes"
			INCREMENT_EXCEPTIONS 1 53 
		fi


		psql -d caspidadb -c 'select count(*) from hrdatausers' 2>/dev/null | egrep -v ' rows*\)' > $TMP1file 		# get number of unique HRData users

        	USERScount=`sed -e '3!d' -e 's/ //g' < $TMP1file`
        	if test -z "$USERScount"
        	then
                	USERScount=0
        	fi

		psql -d caspidadb -c 'select count(*) from hrdataaccounts' 2>/dev/null | egrep -v ' rows*\)' > $TMP1file 		# get number of unique HRData accounts

		ACCOUNTScount=`sed -e '3!d' -e 's/ //g' < $TMP1file`
        	if test -z "$ACCOUNTScount"
        	then
                	ACCOUNTScount=0
        	fi
		NECHO "  accounts:" "$ACCOUNTScount"
		if test $ACCOUNTScount -le $MAXaccounts
		then
			echo
		else
			echo "   <== exceeds accounts sizing $MAXaccounts for $NUMnodes nodes"
			INCREMENT_EXCEPTIONS 1 54 
		fi	

                if $uba42feature
                then
                	URL="https://${JOB_MANAGER_host}:9002/licenses/get"
                        TOKEN=`grep '^\s*jobmanager.restServer.auth.user.token=' /opt/caspida/conf/uba-default.properties | cut -d'=' -f2`
                        AUTH="-Ssk -H 'Authorization: Bearer $TOKEN'"
                else
                	URL="http://${JOB_MANAGER_host}:9002/licenses/get"
                        AUTH=""
                fi
		eval curl --connect-timeout 30 $AUTH $URL 2>/dev/null | python -m json.tool  > $TMP1file 2>/dev/null

		if ! test -s $TMP1file
		then
			NECHO "license (ui):" "not returned from jobmanager"
			echo "   <== license not available; check permissions"
			INCREMENT_EXCEPTIONS
			CREATIONsecs=0
		else
			CREATIONtime=`grep "creationTime" $TMP1file | sed -e 's/[^0-9]*//g'`
			if test "$CREATIONtime" = ""
			then
				CREATIONtime=$JEpoch
			fi
	
			CREATIONsecs=`expr "$CREATIONtime" / 1000`
			CREATIONtime=`date -d "@$CREATIONsecs"`
			EXPIRATIONtime=`grep "expirationTime" $TMP1file | sed -e 's/[^0-9]*//g'`
			if test "$EXPIRATIONtime" = ""
			then
				EXPIRATIONtime=$JEpoch
			fi

			EXPIRATIONsecs=`expr "$EXPIRATIONtime" / 1000`
			EXPIRATIONtime=`date -d "@$EXPIRATIONsecs"`
			echo
			NECHO "license (ui):" "created: $CREATIONtime; expires: $EXPIRATIONtime"
			if test "$NOW" -ge "$EXPIRATIONsecs" 
			then
				echo "   <== UBA license expired at $EXPIRATIONtime"
				INCREMENT_EXCEPTIONS
			else
				ThirtyDaysOfSeconds=`expr $DayOfSeconds \* 30`
				ThirtyDaysFromNow=`expr $NOW + $ThirtyDaysOfSeconds`
				if test "$ThirtyDaysFromNow" -ge "$EXPIRATIONsecs" 
				then
					echo "   <= UBA license will be expiring at $EXPIRATIONtime"
				else
					echo
				fi
			fi
		fi
		

		if $LEADERnode
		then
			LICENSEfilename=`grep -H ">${CREATIONsecs}<" /opt/splunk/etc/licenses/enterprise/* 2>/dev/null | head -1 | cut -d ':' -f 1`
			NECHO "license (file):" "'$LICENSEfilename'"
		else
			LICENSEfilename=`ssh $FIRSTnode "grep -H '>${CREATIONsecs}<' /opt/splunk/etc/licenses/enterprise/* 2>/dev/null" 2>/dev/null | head -1 | cut -d ':' -f 1` 
			NECHO "license (${FIRSTnode}):" "'$LICENSEfilename'"
			if ! test -z "$LICENSEfilename"
			then
				> $LICENSEfile
				scp ${FIRSTnode}:$LICENSEfilename $LICENSEfile 2>/dev/null
				LICENSEfilename=$LICENSEfile
			fi
		fi

		if test -z "$LICENSEfilename"
		then
			echo "   <= license (file) does not match (ui); check permissions and contents"
			if $LEADERnode
			then
				ls -lrt /opt/splunk/etc/licenses/enterprise/* > $TMP1file 2>&1
			else
				ssh $FIRSTnode 'ls -lrt /opt/splunk/etc/licenses/enterprise/* 2>&1' > $TMP1file 2>/dev/null
			fi
			if test -s $TMP1file
			then
				NECHO "licenses:" "`head -1 $TMP1file`"; echo
				ALL_BUT_FIRST $TMP1file
			fi
		else
			echo
			EXPIREseconds=`grep expiration_time $LICENSEfilename |  sed -e 's/[^0-9]*//g'`
			EXPIREdate=`date -d "@$EXPIREseconds"`
			NECHO "license expires:" "'$EXPIREdate'" 
			if test "$NOW" -ge "$EXPIREseconds" 
			then
				echo "   <== UBA license expired at $EXPIREdate"
				INCREMENT_EXCEPTIONS
			else
				ThirtyDaysOfSeconds=`expr $DayOfSeconds \* 30`
				ThirtyDaysFromNow=`expr $NOW + $ThirtyDaysOfSeconds`
				if test "$ThirtyDaysFromNow" -ge "$EXPIREseconds" 
				then
					echo "   <= UBA license will be expiring at $EXPIREdate"
				else
					echo
				fi
			fi
			LICENSEDusers=`grep 'key="users"' $LICENSEfilename | sed -e 's/[^0-9]*//g'`
			NECHO "licensed users:" "'$LICENSEDusers'"

			if ! expr "$LICENSEDusers" + 0 >/dev/null 2>&1
			then
				LICENSEDusers=$MAXaccounts
				echo "  (using $MAXaccounts)"
			else
				echo
			fi

			ACTIVEusers=`psql -d caspidadb -t -c "select count(*) from users where type='Human';" 2>/dev/null | sed -e 's/ //g' -e '$d'`
			NECHO "active users:" "'$ACTIVEusers' (users)"
			if test "$ACTIVEusers" -gt "$LICENSEDusers" >/dev/null 2>&1
			then 
				echo "   (exceeds licensing '$LICENSEDusers')"
			else
				echo 
			fi
			IDENTIFIEDusers=`psql -d caspidadb -t -c "select count(*) from hrdatausers;" 2>/dev/null | sed -e 's/ //g' -e '$d'`
			NECHO "identified users:" "'$IDENTIFIEDusers' (hrdatausers)"
			if test "$IDENTIFIEDusers" -gt "$LICENSEDusers" >/dev/null 2>&1
			then 
				echo "   (exceeds licensing '$LICENSEDusers')"
			else
				echo
			fi
			IDENTIFIEDaccounts=`psql -d caspidadb -t -c "select count(*) from hrdataaccounts;" 2>/dev/null | sed -e 's/ //g' -e '$d'`
			NECHO "identified accounts:" "'$IDENTIFIEDaccounts' (hrdataaccounts)"
			if test "$IDENTIFIEDaccounts" -gt "$LICENSEDusers" >/dev/null 2>&1
			then 
				echo "   (exceeds licensing '$LICENSEDusers')"
			else
				echo
			fi
		fi

		CAPACITYtag=identity.resolution.hrcache.capacity
		GET_CASPIDA_PROPERTY $CAPACITYtag
                if test "$PROPERTYvalue" != ""
		then
			CAPACITYcache=$PROPERTYvalue
		else
			CAPACITYcache=300000
		fi

		NEEDcache=`psql -d caspidadb -t -c "select count(distinct loginid) + count(distinct email) + count(distinct(case when loginid<>domainloginid then domainloginid end)) from hrdataaccounts;" 2>/dev/null | tr -d " "`

		echo
		NECHO "hr ir entities:" "$NEEDcache" 
		if test "$NEEDcache" -gt "$CAPACITYcache" 2>/dev/null
		then
			echo "   <= exceeds ${CAPACITYcache} guesstimate; review value of '$CAPACITYtag' in $UBA_SITE_PROPERTIES"
		else
			echo "   (hrcache.capacity: ${CAPACITYcache})"
		fi

		/opt/caspida/bin/irscan -H -m 2>/dev/null |\
		egrep "^Number|^Cache|^Account" > $TMP1file
		if test -s $TMP1file
		then
			if grep "lookup keys" < $TMP1file > $TMP2file 2>/dev/null
			then
				KEYScount=`head -1 $TMP2file | sed -e 's/^.*://' -e 's/\s\s*//g'`
				if test "$KEYScount" = "$CAPACITYcache"
				then
					sed -ie "/lookup keys/ s/$/   <== hrcache is full; increase value of $CAPACITYtag in $UBA_SITE_PROPERTIES/" $TMP1file
					INCREMENT_EXCEPTIONS
				fi
			fi

			echo
			NECHO "hrcache summary:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		fi

		echo
		> $TMP1file
		for HRDATAtable in hrdatausers hrdataaccounts
		do
			PSQLquery="select \
				'$HRDATAtable' as table, 
				date_trunc('seconds',min(inserted)) as min_inserted, date_trunc('seconds',max(inserted)) as max_inserted, \
				date_trunc('seconds',min(updated)) as min_updated, date_trunc('seconds',max(updated)) as max_updated \
				from $HRDATAtable;"
			psql -d caspidadb -c "$PSQLquery" 2>/dev/null |\
			sed -e '/row.*)/d' >> $TMP1file
			
			PSQLquery="select '$HRDATAtable' as table, 
				date_trunc('month', updated) as updated_month,count(*) \
				from $HRDATAtable group by 2 order by 2;" 
			psql -d caspidadb -c "$PSQLquery" 2>/dev/null |\
			egrep -v ' rows*\)' >> $TMP1file
		done

		if test -s $TMP1file
		then
			echo
                	NECHO "hr update history:" "`head -1 $TMP1file`"; echo
                	ALL_BUT_FIRST $TMP1file
		fi


		echo
		psql -d caspidadb -c "select hrstatus,count(*), case when hrstatus isnull then '' when hrstatus like '' then '' when hrstatus like 'Active' then '' when hrstatus like 'InActive' then '' else '<= invalid value should be Active|InActive' end as comment from hrdatausers group by 1 order by 1;"  2>/dev/null |\
		egrep -v ' rows*\)' > $TMP1file
		ERROR=""
		if grep -q " Active " $TMP1file
		then
			if ! grep -q " InActive " $TMP1file
			then
				ERROR="   (unusual to not find 'InActive' with 'Active'; verify Identity/HRData logic)"
			fi
		fi
		NECHO "hr user status:" "`head -1 $TMP1file`$ERROR"; echo
		ALL_BUT_FIRST $TMP1file

		
		if ! test -f $LOADfile
		then
			psql -d caspidadb -t -c "select id from v_users where type='Human' AND hrstatus='InActive' AND score<5  AND anomaliescount=0 AND threatscount=0;" 2>/dev/null |\
			sed -e 's/^\s//' -e 's/\s\s*//g' -e '/^$/d' | sort > $TMP1file 

			impala-shell -d caspida --quiet -B -q "select distinct userid from usersummary order by userid;" 2>/dev/null |\
		 	sort > $TMP2file
			comm -23 $TMP1file $TMP2file > $TMP3file  

			NECHO "idle InActive:" "`wc -l <$TMP3file`"; echo

			psql -d caspidadb -t -c "select id from v_users where type='Human' AND hrstatus!='InActive' AND score<5  AND anomaliescount=0 AND threatscount=0;" 2>/dev/null |\
			sed -e 's/^\s//' -e 's/\s\s*//g' -e '/^$/d' | sort > $TMP1file 
			comm -23 $TMP1file $TMP2file > $TMP3file  

			NECHO "idle Active:" "`wc -l <$TMP3file`"; echo
                        echo

			NECHO "analytics users:" ""
			wc -l <$TMP2file
			echo
		fi

		psql -d caspidadb -c "select vu.status,count(1) from v_users vu, hrdataaccounts hr  where  hr.employeeid in ( select employeeid from hrdataaccounts where hraccounttype='Normal' AND NOT (( uaccode isnull  AND uac ilike '%ACCOUNTDISABLE%') OR (uaccode!='' AND (mod(uaccode::int,4) > 1))) ) AND vu.employeeid=hr.employeeid  group by 1;" 2>/dev/null |\
		egrep -v ' rows*\)' > $TMP1file
                NECHO "normal, enabled:" "`head -1 $TMP1file`"; echo
                ALL_BUT_FIRST $TMP1file


		STATUScount=`psql -d caspidadb -t -c "select count(*) from hrdatausers  where hrstatus like 'Active' OR hrstatus like 'InActive';" 2>/dev/null | sed -e '/^$/d' -e 's/ //g'`

		NORMALcount=`psql -d caspidadb -t -c "select count(*) from hrdataaccounts  where hraccounttype=ANY(ARRAY['Normal','Intern','Contractor','Guest']);" 2>/dev/null | sed -e '/^$/d' -e 's/ //g'`

		if test $STATUScount -gt $NORMALcount
		then
			NECHO "hr status:" "combined status count '$STATUScount' exceeds '$NORMALcount' ~Normal accounts"; echo "   (verify Identity/HRData logic)" 
			echo
		fi

		psql -d caspidadb -c "select hrstatus,count(*) as UAC_disabled from v_users where statuscode like '%ACCOUNTDISABLE%' group by 1 order by 1;" 2>/dev/null |\
                egrep -v ' rows*\)' | sed -e '/^$/d' > $TMP1file
		ROWS=`wc -l < $TMP1file`
		if test $ROWS -gt 2
		then
			ERROR=""
		else
			ERROR="   (unusual to not see 'disabled'; verify Identity/HRData logic)" 
		fi
                NECHO "disabled v_users:" "`head -1 $TMP1file`$ERROR"; echo
                ALL_BUT_FIRST $TMP1file

		UACcheck=`psql -d caspidadb -t -c "select column_name from information_schema.columns where table_name='hrdataaccounts' AND column_name='uac';" 2>/dev/null | sed -e '/^$/d'`
                if test -z "$UACcheck"
                then
			echo
                        NECHO "hrdataaccounts schema:" "'$UACcheck'"
                        echo "   <= 'uac' column is missing from 'hrdataaccounts' table"
                        echo
                        psql -d caspidadb -c "select count(*) as UAC_disabled from hrdatausers where statuscode like '%ACCOUNTDISABLE%';" 2>/dev/null |\
                        egrep -v ' rows*\)' | sed -e '/^$/d' > $TMP1file
			ROWS=`wc -l < $TMP1file`
			if test $ROWS -gt 2
			then
				ERROR=""
			else
				ERROR="   (unusual to not see 'disabled'; verify Identity/HRData logic)" 
			fi
                        NECHO "disabled hr users:" "`head -1 $TMP1file`$ERROR"; echo
                        ALL_BUT_FIRST $TMP1file
                else
                        echo
                        psql -d caspidadb -c "select hraccounttype, count(*) as UAC_disabled from hrdataaccounts where uac like '%ACCOUNTDISABLE%' OR (uaccode is not null AND (uaccode::int % 4) > 1) group by 1 order by 1;" 2>/dev/null |\
                        egrep -v ' rows*\)' | sed -e '/^$/d' > $TMP1file
			ROWS=`wc -l < $TMP1file`
			if test $ROWS -gt  2
			then
				ERROR=""
			else
				ERROR="   (unusual to not see 'disabled'; verify Identity/HRData logic)" 
			fi
                        NECHO "disabled hr accounts:" "`head -1 $TMP1file`$ERROR"; echo
                        ALL_BUT_FIRST $TMP1file


			NORMALpriv=`psql -d caspidadb -t -c "select count(*) from hrdataaccounts where hraccounttype=ANY(ARRAY['Normal','Intern','Contractor','Guest']) AND  (ARRAY['Domain Admins','Enterprise Admins', 'Administrators', 'Exchange Organization Administrators', 'Backup Operators', 'DHCP Administrators', 'DnsAdmins', 'Hyper-V Administrators', 'Schema Admins'] && usergroups::text[]);" 2>/dev/null| sed -e 's/ //g' -e '/^$/d'`
			if ! test -z "$NORMALpriv"
			then
				if test "$NORMALpriv" -gt "0"
				then
					echo
					NECHO "norm account with priv:" "'$NORMALpriv'"; echo "   (unusual for ~Normal account to be in well known 'admin' groups; review least privilege policy)"
				fi
			fi
                fi
		
		echo
		psql -d caspidadb -c "select value::json->>'uniqueIdPolicy' as uniqueIdPolicy,value::json->>'escapeCharacters' as escapeCharacters from hrconfig;" 2>/dev/null | egrep -v ' rows*\)' > $TMP1file
		NECHO "hrconfig:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file

		HRDATAtype=`psql -d caspidadb -t -c "select name,type,endprocessingtime from datasources where type ilike '%HR%' ORDER BY 3 DESC;" 2>/dev/null | head -1 |tr -d ' ' | tee $TMP2file | cut -d '|' -f 2`
                if test -z "$HRDATAtype"
                then
                        HRDATAtype="HRData"
                fi
		HRDATAname=`cut -d '|' -f 1 < $TMP2file | head -1 | sed -e 's/^\s\s*//' -e 's/\s\s*$//' -e '/^$/d'`

		for ATTRIBUTION in User.json Account.json AccountTypes.json
		do
			FILEpath=/opt/caspida/conf/attribution/${ATTRIBUTION}
			if test -s $FILEpath
			then
				ls -l $FILEpath > $TMP1file
				echo >> $TMP1file
				sed -e 's+^//.*$++' -e 's+[^:]//.*$++' -e '/^.\*/d' < $FILEpath |\
                                python -m json.tool 2>&1 | fold -s -80 >> $TMP1file
				echo
				NECHO "${ATTRIBUTION} (d):" "`head -1 $TMP1file`"; echo
				ALL_BUT_FIRST $TMP1file
			fi
			FILEpath=/etc/caspida/local/conf/attribution/${ATTRIBUTION}
			if test -s $FILEpath
			then
				ls -l $FILEpath > $TMP1file
				echo >> $TMP1file
				sed -e 's+^//.*$++' -e 's+[^:]//.*$++' -e '/^.\*/d' < $FILEpath |\
                                python -m json.tool 2>&1 | fold -s -80 >> $TMP1file
				echo
				NECHO "${ATTRIBUTION} (l):" "`head -1 $TMP1file`"; echo
				ALL_BUT_FIRST $TMP1file
			fi
		done

		echo
		for TABLE in users v_users  hrdatausers hrdataaccounts 
		do
			psql -d caspidadb -c "\d $TABLE;" 2>/dev/null |\
			grep '|' | grep -v Column | cut -d '|' -f 1 > $TMP1file
			echo `cat $TMP1file` | fold -s -80 > $TMP2file
			echo
			NECHO "$TABLE columns:" "`head -1 $TMP2file`"; echo
			ALL_BUT_FIRST $TMP2file
		done
		echo

		if ! test -z "$HRDATAname"
		then
			DSid=`psql -d caspidadb -t -c "select id from datasources where name='$HRDATAname';" 2>/dev/null | sed -e 's/^\s\s*//' -e 's/\s\s*$//' -e '/^$/d'`
			DISPLAY_DATASOURCE "$DSid"
			if grep -qi 'ldapsearch ' $TMP4file >$TMP1file 
			then
				NECHO "$HRDATAname observations:" "`head -1 $TMP1file`"; echo "   <= verfiy that ldapsearch does not time out"	
			fi
		fi

		echo 
		NECHO "hrdata evaluation ..." ""; echo

		PSQLquery="select count(*) from hrdatausers \
			where terminationdate < now();" 
		psql -d caspidadb -t -c "$PSQLquery" 2>/dev/null |\
		sed -e 's/\s\s*//g' > $TMP1file
		TERMINATIONcount=`head -1 $TMP1file`
		if test "$TERMINATIONcount" = "0"
		then
			TERMINATIONcount="(not used)"
		else
			TEMINATIONcountt="$TERMINATIONcount users"
		fi
		NECHO "  terminationDate:" "$TERMINATIONcount"; echo

		NECHO "  memberOf to usergroups:" ""   

		COUNTaccounts=`psql -d caspidadb -t -c "select count(*) from hrdataaccounts;" 2>/dev/null | sed -e 's/\s\s*//g'`
		if expr "$COUNTaccounts" + 0 >/dev/null 2>&1
		then
			QUARTERaccounts=`expr $COUNTaccounts / 4`
		else
			QUARTERaccounts=0
		fi
		
		psql -d caspidadb -t -c "select usergroups from hrdataaccounts where usergroups::text like '%CN=%' order by 1 desc limit 3;" 2>/dev/null |\
		sed -e '/^$/d' > $TMP1file
		if test -s $TMP1file
		then
			ROWS=`psql -d caspidadb -t -c "select count(*) from hrdataaccounts where usergroups::text like '%CN=%';"  2>/dev/null`
			if ! expr $ROWS + 0 > /dev/null 2>&1
                        then
                                ROWS=0
                        fi
                        if test $ROWS -ge $QUARTERaccounts
                        then
                                FLAG="<=="
                                INCREMENT_EXCEPTIONS
                        else
                                FLAG="<="
			fi
			echo "    rows: $ROWS" >> $TMP1file
			echo "`head -1 $TMP1file`   $FLAG 'CN=' not expected in usergroups; review HRData SPL for memberOf"
			ALL_BUT_FIRST $TMP1file
			INCREMENT_EXCEPTIONS
		else
			echo "(okay)"
		fi


		psql -d caspidadb -t -c "select distinct usergroups from hrdataaccounts group by 1 order by 1;" 2>/dev/null |\
		sed -e 's/^ {//' -e 's/}$//' -e '/^$/d'> $TMP1file 
		COMMONgrouping=`wc -l < $TMP1file | tr -d ' '`
		NECHO "" "    common: $COMMONgrouping; unique usergroups: " 
		> $TMP2file
		timeout 300  awk --field-separator="," '{for(i=2;i<=NF;i++){groups[$i]="1"}} END{print length(groups)}' < $TMP1file > $TMP2file
		if test -s $TMP2file
		then
			cat $TMP2file
		else
			echo "   (timeout)"
		fi
#		sed -e 's/^ {//' -e 's/}$//' -e 's/,/\n/g' -e 's/"//g' -e '/^$/d' < $TMP1file |\
#		head -20000 |\
#		sort -u |\
#		wc -l
#		uniq -c > $TMP1file
#		wc -l < $TMP1file
		
		NECHO "  manager to managerid:" ""   
		psql -d caspidadb -t -c "select managerid from hrdatausers where managerid like '%CN=%' order by 1 desc limit 3;" 2>/dev/null |\
		sed -e '/^$/d' > $TMP1file
		if test -s $TMP1file
		then
			ROWS=`psql -d caspidadb -t -c "select count(*) from hrdatausers where managerid like '%CN=%';" 2>/dev/null`
			if ! expr $ROWS + 0 > /dev/null 2>&1
			then
				ROWS=0
			fi
			if test $ROWS -ge 50
			then
				FLAG="<=="
				INCREMENT_EXCEPTIONS
			else
				FLAG="<="
			fi
			echo "  rows: $ROWS" >> $TMP1file
			echo "`head -1 $TMP1file`   $FLAG 'CN=' not expected in managerid; review HRData SPL for manager"
			ALL_BUT_FIRST $TMP1file
		else
			echo "(okay)"
		fi

		NECHO "  userAccountControl:" ""
		psql -d caspidadb -t -c "select uaccode from hrdataaccounts where NOT uaccode isnull order by 1 limit 3;" 2>/dev/null |\
                sed -e '/^$/d' > $TMP1file
		if test -s $TMP1file
                then
                        echo "`head -1 $TMP1file`"
                        ALL_BUT_FIRST $TMP1file
                        INCREMENT_EXCEPTIONS
                else
                        echo "(okay)"
                fi

		psql -d caspidadb t -c "select loginid from hrdataaccounts where loginid ilike 'administrator' OR loginid ilike 'guest';" 2>/dev/null |\
		sed -e '/^$/d' > $TMP1file  
		if test -s $TMP1file
		then
			NECHO "  built-ins:" "`head -1 $TMP1file`"; echo "   <= unexpected; not human entity"
			ALL_BUT_FIRST $TMP1file
		fi

		NECHO "  trust accounts:" ""
		psql -d caspidadb -t -c "select uac from hrdataaccounts where (uac like '%WORKSTATION_TRUST_ACCOUNT%' OR uac like '%INTERDOMAIN_TRUST_ACCOUNT%' OR uac like '%SERVER_TRUST_ACCOUNT%') order by 1 limit 3;" 2>/dev/null |\
                sed -e '/^$/d' > $TMP1file
		if test -s $TMP1file
                then
                        ROWS=`psql -d caspidadb -t -c "select count(*) from hrdataaccounts where (uac like '%WORKSTATION_TRUST_ACCOUNT%' OR uac like '%INTERDOMAIN_TRUST_ACCOUNT%' OR uac like '%SERVER_TRUST_ACCOUNT%');" 2>/dev/null`
			if ! expr $ROWS + 0 > /dev/null 2>&1
			then
				ROWS=0
			fi
                        echo "    rows: $ROWS" >> $TMP1file
                        echo "`head -1 $TMP1file`   ('*_TRUST_ACCOUNT' not expected in uac; review HRData SPL for UAC)"
                        ALL_BUT_FIRST $TMP1file
                else
                        echo "(okay)"
                fi

		MATCHdomainloginid=`psql -d caspidadb -t -c "select count(*) from hrdataaccounts WHERE loginid=domainloginid;" 2>/dev/null | sed -e 's/\s\s*//g'`
		if expr "$MATCHdomainloginid" + 0 >/dev/null 2>&1
		then
			NECHO "  domainloginid:" "$MATCHdomainloginid"
			echo "   <= there is no domain in domainloginid"
		fi

		PSQLquery="select \
		distinct(left(domainloginid, (length(domainloginid) - length(loginid)))) as domain  \
		from hrdataaccounts where domainloginid!=loginid;"

		psql -d caspidadb -t -c "$PSQLquery" 2>/dev/null |\
		sed -e '/^$/d' | sort > $TMP1file
		if test -s $TMP1file
		then
			NECHO "  account domains:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		fi

		echo
        	DIFFERENCE=`expr $ACCOUNTScount - $USERScount`
        	NECHO "identity normalization:" "accounts: $ACCOUNTScount, users: $USERScount, difference: $DIFFERENCE"
		if test "$USERScount" -gt  "0"
		then
        		if test $USERScount -eq $ACCOUNTScount
        		then
                		echo "   <== normalization failed; verify Identity/HRData logic"
				INCREMENT_EXCEPTIONS 1 27
        		else
                		if test $DIFFERENCE -lt $SIGNIFICANTdifference
                		then
                        		echo "    (difference is insignificant; verify Identity/HRData logic)"
                 		else
                        		echo
                		fi
			fi
		else
			echo
        	fi
 
		psql -d caspidadb -t -c "select parentid from hrdataaccounts where hraccounttype='Admin';" 2>/dev/null |\
		sed -e '/^$/d' -e 's/\s//g' |\
		sort -u > $TMP1file			# unique parents of Admin
		COUNTadmin=`wc -l < $TMP1file`

		psql -d caspidadb -t -c "select parentid from hrdataaccounts;" 2>/dev/null |\
                sed -e '/^$/d' |\
                sort |\
                uniq -c |\
                egrep  -e '^ *1 ' |\
                sed -e 's/^  *//' -e 's/  */|/' -e 's/^.*|//' > $TMP2file       # accounts without siblings
                COUNTsiblings=`wc -l < $TMP2file`

		comm -12 $TMP1file $TMP2file > $TMP3file	# admin without normal
		COUNTuncommon=`wc -l < $TMP3file`

		if test $COUNTuncommon -ne 0
		then
			echo
			NECHO "hr typing 'Admin' *:" "$COUNTuncommon of $COUNTadmin 'Admin' accounts have not been normalized (no siblings); not expected"; echo "   (review least privilege policy)"
			echo
			echo "loginid|domainloginid|employeeid|email|hrstatus|updated" > $TMP1file
			while read IDadmin
			do
				psql -d caspidadb -t -c "$OBSCUREfunction; select pg_temp.obscure(loginid) as loginid, pg_temp.obscure(domainloginid) as idomainloginid, pg_temp.obscure(a.employeeid) as employeeid, pg_temp.obscure(email) as email, u.hrstatus, date_trunc('Minutes',a.updated) from hrdataaccounts a, hrdatausers u  where parentid='$IDadmin' AND a.parentid=u.caspidaid;" 2>/dev/null |\
				sed -e '/^$/d' -e 's/^ //' -e 's/  *|  */|/g' -e 's/|  */|/g'  -e '/^-----/d' -e '/rows*)$/d'
			done < $TMP3file  |\
			sort | head -25 >> $TMP1file

			awk 'BEGIN {FS="|"; DASHES="----------------------------------------"} {printf "%-30.30s | %-40.40s | %-30.30s | %-40.40s | %-8.8s | %-19.19s\n", $1,$2,$3,$4,$5,$6}; /employeeid/ {printf "%-30.30s + %-40.40s + %-30.30s + %-40.40s + %-8.8s + %-19.19s\n", DASHES,DASHES,DASHES,DASHES,DASHES,DASHES}' < $TMP1file > $TMP2file
			NECHO "" "`head -1 $TMP2file`"; echo
			ALL_BUT_FIRST $TMP2file
		fi

		for TYPE in "Service" "System"
		do
			psql -d caspidadb -t -c "select parentid from hrdataaccounts where hraccounttype='$TYPE';" 2>/dev/null |\
			sed -e '/^$/d' -e 's/\s//g' |\
			sort -u > $TMP1file			# unique parents of Service
			COUNTtype=`wc -l < $TMP1file`

			if test $COUNTtype = 0
			then
				continue
			fi

			psql -d caspidadb -t -c "select parentid from hrdataaccounts;" 2>/dev/null |\
 			sed -e '/^$/d' |\
 			sort |\
 			uniq -c |\
 			egrep -v -e '^ *1 ' |\
 			sed -e 's/^  *//' -e 's/  */|/' -e 's/^.*|//' > $TMP2file	# accounts with siblings
			COUNTsiblings=`wc -l < $TMP2file`

			comm -12 $TMP1file $TMP2file > $TMP3file	# Type with siblings
			COUNTcommon=`wc -l < $TMP3file`

			if test $COUNTcommon -ne 0
			then
				echo
				NECHO "hr typing '$TYPE' *:" "$COUNTcommon of $COUNTtype '$TYPE' accounts have been normalized (has siblings); not expected";  echo "   (verify Identity/HRData logic)"
				echo
				echo "loginid|domainloginid|employeeid|email|hrstatus|updated" > $TMP1file
				while read IDadmin
				do
					psql -d caspidadb -t -c "$OBSCUREfunction; select pg_temp.obscure(loginid) as loginid, pg_temp.obscure(a.domainloginid) as domainloginid, pg_temp.obscure(a.employeeid) as employeeid, pg_temp.obscure(email) as email, u.hrstatus, date_trunc('Minutes',a.updated) from hrdataaccounts a, hrdatausers u where parentid='$IDadmin' AND a.parentid=u.caspidaid;" 2>/dev/null |\
					sed -e '/^$/d' -e 's/^ //' -e 's/  *|  */|/g' -e 's/|  */|/g'  -e '/^-----/d' -e '/rows*)$/d'
				done < $TMP3file  |\
				sort | head -25 >> $TMP1file

				awk 'BEGIN {FS="|"; DASHES="----------------------------------------"} {printf "%-30.30s | %-40.40s | %-30.30s | %-40.40s | %-8.8s | %-19.19s\n", $1,$2,$3,$4,$5,$6}; /employeeid/ {printf "%-30.30s +  %-40.40s + %-30.30s + %-40.40s + %-8.8s + %-19.19s\n", DASHES,DASHES,DASHES,DASHES,DASHES,DASHES}' < $TMP1file > $TMP2file
				NECHO "" "`head -1 $TMP2file`"; echo
				ALL_BUT_FIRST $TMP2file
			fi
			echo
		done

		psql -d caspidadb -c 'select numAccounts,count(*) as numUsers from (select employeeid, count(loginid) as numAccounts from (select u.employeeid,a.loginid from hrdatausers u left join hrdataaccounts a on u.employeeid = a.employeeid ) x1 group by 1) x2 group by 1 order by 1 asc;' 2>/dev/null | egrep -v ' rows*\)' > $TMP1file
	
		NECHO "accounts by user:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file

		egrep -e '^ *[0-9]' <$TMP1file | tr -d ' ' > $TMP2file
	
		while read SUMMARY
		do
			COUNT=`echo $SUMMARY | cut -d '|' -f 1` 
			LIMIT=`echo $SUMMARY | cut -d '|' -f 2` 
			if test "$COUNT" -eq "0"
			then
				:
				continue
			fi
			if test "$LIMIT" -gt "$SAMPLElimit"
			then
				LIMIT=$SAMPLElimit
			fi

			SAMPLE_USERS_ACCOUNTS $COUNT $LIMIT
		done < $TMP2file

		psql -d caspidadb -t -c "select employeeid from hrdatausers;" 2>/dev/null |\
		sed -e '/^$/d' -e 's/^ //' |\
		sort -u > $TMP1file

		psql -d caspidadb -t -c "select employeeid from hrdataaccounts" 2>/dev/null |\
		sed -e '/^$/d' -e 's/^ //' |\
		sort -u > $TMP2file

		comm -23 $TMP1file $TMP2file >$TMP3file

		MISSINGcount=`wc -l < $TMP3file`

		if test $MISSINGcount -ne 0
		then
			echo "$MISSINGcount HRData users without accounts (childless)" > $TMP1file
			if test $MISSINGcount -ge 100
			then
				ERROR="(issue with initial Identity/HRData ingestion)"
			else
				ERROR=""
			fi
			echo " ... one example * ..." >>$TMP1file
			psql -d caspidadb -c "$OBSCUREfunction; select pg_temp.obscure(employeeid) as employeeid, pg_temp.obscure(displayname) as displayname, hrstatus, date_trunc('Minutes',inserted) as inserted,date_trunc('Minutes',updated) as updated from hrdatausers where employeeid='`head -1 $TMP3file`';" 2>/dev/null | egrep -v ' rows*\)' >> $TMP1file
			NECHO "missing accounts *:" "`head -1 $TMP1file`   $ERROR"; echo
			ALL_BUT_FIRST $TMP1file
		fi

		psql -d caspidadb -c "select grouptype, count(*) from peergroupdetails group by 1 order by 1;" 2>/dev/null |\
		sed -e '/^$/d' -e '/ rows*)/d' > $TMP1file
		if test -s $TMP1file
		then
			echo
			NECHO "peergroupdetails:" "`head -1 $TMP1file`"; echo
                	ALL_BUT_FIRST $TMP1file
		fi
		
		psql -d caspidadb -t -c "select distinct grouptype from peergroups;" 2>/dev/null |\
		sed -e 's/\s//g' -e '/^$/d' |
		while read TYPE
		do
			IDcount=`psql -d caspidadb -t -c "select count(*) from (select distinct groupid from peergroups where grouptype='$TYPE') p;" 2>/dev/null | sed -e 's/\s//g' -e '/^$/d'`
			echo "${TYPE}|${IDcount}"
		done |\
		awk 'BEGIN {FS="|"; printf "%-15.15s | %15s\n", "grouptype", "groupid count"; printf "%-15.15s-|-%15s\n", "---------------", "---------------"} {printf "%-15.15s | %15d\n", $1, $2}' > $TMP1file
		echo
                NECHO "peergroups:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file

		CNcount=`psql -d caspidadb -t -c "select count(*) from (select distinct groupid from peergroups where grouptype='ADGroups' AND groupid ilike '%CN=%') p;" 2>/dev/null | sed -e 's/\s//g' -e '/^$/d'`
		if test "$CNcount" = ""
		then
			CNcount=0
		fi
		if test $CNcount -gt 0
		then
			NECHO "peer ADGroups:" "'CN=' in groupid  ${CNcount}"; echo "   <= check HRData memberOf logic"
		fi

		echo
		psql -d caspidadb -c "select ou,count(*) from hrdatausers group by ou order by 2 desc limit 10;" 2>/dev/null | egrep -v ' rows*\)' > $TMP1file
		NECHO "hrdatausers ou:" "`head -1 $TMP1file`"; echo
                ALL_BUT_FIRST $TMP1file

		NULLou=`psql -d caspidadb -t -c "select count(*) from hrdatausers where ou is null;" 2>/dev/null | head -1 | tr -d ' '`	
		if ! test "$NULLou" = "0" 
		then
			NECHO "null ou:" "$NULLou"; echo
			echo
		fi

		psql -d caspidadb -c "select hraccounttype,count(*) from hrdataaccounts where loginid like '%$' group by 1 order by 1;" 2>/dev/null > $TMP1file
		if test -s $TMP1file
		then
			if ! grep -q '(0 rows)' $TMP1file
			then
				egrep -v ' rows*\)' $TMP1file > $TMP2file
				NECHO "*\$ accounts:" "`head -1 $TMP2file`"; echo "   (ignored by UBA; verify Identity/HRData logic)"
				ALL_BUT_FIRST $TMP2file
			fi
		fi

		psql -d caspidadb -c "select hraccounttype,count(*), '' as comment from hrdataaccounts group by 1 order by 1;" 2>/dev/null | egrep -v ' rows*\)' > $TMP1file 

		if egrep " System  *\|" $TMP1file > $TMP2file
		then
			SYSTEMcount=`sed -e 's/ //g' $TMP2file | cut -d '|' -f 2`
			if test $SYSTEMcount -gt 128
			then
				sed -i -e '/ System  */ s+$+   \(unexpected number of accounts; verify Identity/HRData logic\)+' $TMP1file
			fi
		fi
		NECHO "hraccounttype:" ""
		head -1 $TMP1file; ALL_BUT_FIRST $TMP1file
		
		TYPEadmin=false
		TYPEservice=false
		if egrep -q " Admin  *|" $TMP1file
		then
			TYPEadmin=true
		fi
		if egrep -q " Service  *|" $TMP1file
		then
			TYPEservice=true
		fi
		if ! ($TYPEadmin && $TYPEservice)
		then
			echo
			NECHO "" "missing:"; echo
			if ! $TYPEadmin
			then
				NECHO "" "    Admin"; echo "   <= 'Admin' account type is missing"
			fi
			if ! $TYPEService
			then
				NECHO "" "    Service"; echo "   <= 'Service' account type is missing"
			fi
		fi

		egrep -ve 'hraccounttype|----|Normal|rows*\)|^$' $TMP1file | sed -e 's/^ //' -e 's/\s\s*|.*$/|/' |\
        	while read TYPE_COMMENT
        	do
			TYPE=`echo "$TYPE_COMMENT" | cut -d '|' -f 1`
                	SAMPLE_USERS_TYPE "$TYPE"
        	done

		NECHO "users type:" ""
		psql -d caspidadb -c 'select type,count(*) from users group by 1' 2>/dev/null | egrep -v ' rows*\)' > $TMP1file 
		head -1 $TMP1file; ALL_BUT_FIRST $TMP1file

		NECHO "droppedusers:" ""
		psql -d caspidadb -c "select (select name from datasources where id=srcid) as datasource, srcfmt, count(*) from droppedusers group by srcid,srcfmt;" 2>/dev/null | egrep -v ' rows*\)' > $TMP1file 
		head -1 $TMP1file; ALL_BUT_FIRST $TMP1file

		case "$NUMnodes" in
			1) IMPALAtimeout=150;;
			3) IMPALAtimeout=300;;
			5) IMPALAtimeout=600;;
			7) IMPALAtimeout=1200;;
			10) IMPALAtimeout=1500;;
			*) IMPALAtimeout=2100;;
		esac
		if ! $MULTInode
		then
			if $uba41feature
			then
				SUPERnode=`cat $SNODEfile`
				if $SUPERnode
				then
					IMPALAtimeout=600
					NECHO "super node" ""
					echo "timeout: $IMPALAtimeout seconds"
				fi
			fi
		fi
		> $TMP1file


		psql -d caspidadb -c "select type,source,count(*) from blackwhitelist where (type='BlackIP' OR type='BlackDomain' OR type='WhiteIP' OR type='WhiteDomain') group by 1,2 order by 1,2;" 2>/dev/null |\
		egrep -v ' rows*\)' > $TMP1file
		if test -s $TMP1file
		then
			echo
			NECHO "blockAllowList:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		fi

		ANOMALIEScount=`psql -d caspidadb -t -c "select count(*) from anomalies;" 2>/dev/null | tr -d ' '`
		ANOMALYDETAILScount=`psql -d caspidadb -t -c "select count(*) from anomalydetails;" 2>/dev/null | tr -d ' '`
		ANOMALYENTITIEScount=`psql -d caspidadb -t -c "select count(distinct anomalyid) from anomalyentities;" 2>/dev/null | tr -d ' '`

		echo
		NECHO "anomaly tables:" "anomalies: ${ANOMALIEScount}; anomalydetails: ${ANOMALYDETAILScount}; anomalyentities: ${ANOMALYENTITIEScount}"

		ANOMALIESmax=800000
		if test "$NUMnodes" -ge 10 2>/dev/null
		then
			ANOMALIESmax=1500000
		fi

		if test "$ANOMALIEScount" -gt "$ANOMALIESmax" 2>/dev/null
		then
			echo "   <= consider reducing anomalies to less than $ANOMALIESmax for $NUMnodes nodes"
		else
			echo
		fi

		echo
		NECHO "anomalies status:" ""
		psql -d caspidadb -c "select status,count(*) from anomalies group by 1;" 2>/dev/null | egrep -v ' rows*\)' > $TMP1file
#		if grep -q Gone $TMP1file
#		then
#			sed -i "s/Gone/Gone   <== 'anomalies' table maintenance required/" $TMP1file
#			INCREMENT_EXCEPTIONS 1 28b
#		fi

		DELETEDanomalies=`grep Deleted $TMP1file | sed -e 's/\s\s*//g' | cut -d '|' -f 2`
		if test "$DELETEDanomalies" = ""
		then
			DELETEDanomalies=0
		fi
		if test $DELETEDanomalies -gt 400000
		then
			EMPTYtrash=true
			sed -ie '/Deleted/ s/$/   <= excessive deleted anomalies; empty trash on a daily basis/' $TMP1file
		else
			EMPTYtrash=false
		fi
		
                head -1 $TMP1file; ALL_BUT_FIRST $TMP1file

		if $EMPTYtrash
		then
			psql -d caspidadb -c "select date_trunc('Month',endtime) as end__month,count(*) from anomalies where status='Deleted' group by 1 order by 1;" 2>/dev/null  | egrep -v ' rows*\)' > $TMP1file
			if test -s $TMP1file
			then
				NECHO "deleted anomalies:" "`head -1 $TMP1file`"; echo
				ALL_BUT_FIRST $TMP1file
			fi
		fi

		NECHO "anomalies by type:" ""
		psql -d caspidadb -c "select count(*),anomalyType from anomalies group by anomalyType;" 2>/dev/null | egrep -v ' rows*\)' > $TMP1file
                head -1 $TMP1file; ALL_BUT_FIRST $TMP1file

		SAUcount=`grep "SuspiciousAccountUsage" $TMP1file | sed -e 's/\s//g' -e 's/|.*$//'`

		if test "$SAUcount" = ""
		then
			SAUcount=0
		fi

		TAUcount=`grep "TerminatedAccountUsage" $TMP1file | sed -e 's/\s//g' -e 's/|.*$//'`

		if test "$TAUcount" = ""
		then
			TAUcount=0
		fi

		SAUcount=`expr $TAUcount + $SAUcount 2>/dev/null`

		PSQLquery="SELECT '*AccountUsage' AS anomalytype,\
				(CASE WHEN summary like '%:%' THEN SPLIT_PART(summary,':', 1) ELSE summary END) as summary,\
				count(*) \
			FROM anomalies \
			WHERE anomalytype='SuspiciousAccountUsage' OR anomalytype='TerminatedAccountUsage' \
			GROUP BY 2  ORDER BY  2;"
#		PSQLquery="select anomalytype,summary,count(*) from anomalies where anomalytype='SuspiciousAccountUsage' group by 1,2 order by 1,2;"
		psql -d caspidadb -c "$PSQLquery" 2>/dev/null | egrep -v ' rows*\)' > $TMP1file

		if test -s $TMP1file
		then
			NECHO "suspicious account use:" ""

			TUcount=`grep "Activity by terminated user detected" $TMP1file | sed -e 's/\s//g' | cut -d '|' -f 3`
			if test "$TUcount" = ""
	       	        then
       	                	TUcount=0
       	        	fi

			if test "$SAUcount" -eq 0
			then
				PERcent=0
			else
				PERcent=`expr 100 \* $TUcount / $SAUcount`
			fi

			if test "$PERcent" -gt "60"
			then
				if test "$TUcount" -ge "50000"
				then
					sed -ie '/terminated user detected/ s/$/   <== excessive terminated user anomalies; review HRData SPL for status/' $TMP1file
					INCREMENT_EXCEPTIONS
				else
					sed -ie '/terminated user detected/ s/$/   <= high percentage of terminated user anomalies; review HRData SPL for status/' $TMP1file
				fi
			fi

                	head -1 $TMP1file; ALL_BUT_FIRST $TMP1file
		fi

		NECHO "recent anomalies by day:" ""
		psql -d caspidadb -c "select date_trunc('Day',eventtime) as eventtime,count(*) from anomalies where eventtime > '$RECENT_days' group by 1 order by 1" 2>/dev/null | egrep -v ' rows*\)'  > $TMP1file

		ERROR="   <== note no anomalies on ${DAYBEFORE}, $YESTERDAY or $TODAY"
		for DATE in $TODAY $YESTERDAY $DAYBEFORE
		do
			if grep -q "$DATE" $TMP1file
			then
				ERROR=""
				break
			fi
		done
		if ! test -z "$ERROR"
		then
			INCREMENT_EXCEPTIONS 1 29
		fi

		(head -2 $TMP1file; echo "   . . .$ERROR"; tail -7 $TMP1file) > $TMP2file
		head -1 $TMP2file; ALL_BUT_FIRST $TMP2file

		echo
		NECHO "old (+$MAINTAINdays days) anomalies:" ""
		psql -d caspidadb -c "select status,count(*) from anomalies where endtime < '$ANCIENT_days' group by 1 order by 1" 2>/dev/null | egrep -v ' rows*\)'  > $TMP1file
		head -1 $TMP1file
		ALL_BUT_FIRST $TMP1file

        	PSQLjoin="select distinct ta.anomalyid \
                        from threatanomalies ta \
                                INNER JOIN v_threats vt ON ta.threatid=vt.id \
                                LEFT OUTER JOIN v_anomalies va ON  ta.anomalyid=va.id \
                where va.status='Active' AND  vt.status = 'Active' \
                        AND ( ta.isactive='t' ) "
        	PSQLwhere="  a.endtime <= current_date - interval '91  days' AND a.id NOT IN ($PSQLjoin)"


		NO_THREATS_count=`psql -d caspidadb -t -c "select count(*) from anomalies a where $PSQLwhere ;" 2>/dev/null | head -1`
		if test "$NO_THREATS_count" != ""
		then
			NECHO "old (+$MAINTAINdays) w/o threats:" "$NO_THREATS_count old 'Active' anomalies without threats"; echo
			echo
		fi



		ssh $JOB_MANAGER_host tail -1000 /var/log/caspida/system/AnomalyPurger.log > $TMP1file 2>/dev/null
		if test -s $TMP1file
		then
			GET_CASPIDA_PROPERTY persistence.anomalies.trashed.maintain.days
			if test "$PROPERTYvalue" = ""
			then
				PROPERTYvalue="90 (documented default)"
			fi
        		echo " ... persistence.anomalies.trashed.maintain.days=$PROPERTYvalue ..." > $TMP2file
			echo
			grep "RdbmsAnomalyDaoImpl: Deleted" $TMP1file | tail -1 >> $TMP2file
			if test -s $TMP2file
			then
				NECHO "AnomalyPurger.log:" "`head -1 $TMP2file`"; echo
				ALL_BUT_FIRST $TMP2file
			fi
		fi

		psql -d caspidadb -c "select min(lag) as min_hrs, round(cast(avg(lag) as numeric),1) as avg_hrs, round(cast(max(lag) as numeric),1)  as max_hrs from (select round(cast(date_part('epoch',age(creationtime,eventtime))/3600 as numeric),2) as lag from anomalies where creationtime > (select max(creationtime) - interval '24 hours' from anomalies)) temp" 2>/dev/null | head -3 > $TMP1file 

		echo
		NECHO "anomaly lag (24 hrs):" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file

		LAGaverage=`tail -1 $TMP1file | tr -d ' ' | cut -d '|' -f 2`
		if test -z "$LAGaverage"
		then
			LAGaverage="0"
		fi
			
		LAG=`TZ=UTC psql -t -d caspidadb -c "select round(cast(date_part('epoch',age(creationtime,eventtime))/3600 as numeric),2) from anomalies where creationtime = (select max(creationtime) from anomalies);" 2>/dev/null | tr -d ' '`
		if test -z "$LAG"
		then
			LAG="0"
		fi
			
		echo 
		NECHO "anomaly lag (recent):" "$LAG hours"

		BCresult=`echo "$LAG > $LAGaverage" | bc -l 2>/dev/null`
		if test "$BCresult" = "1"
		then
			echo "   (most recent anomaly exceeds average)"
		else
			echo
		fi
		echo

		psql -d caspidadb -c "select * from threats where id='';" 2>/dev/null |\
		head -1 |\
		sed -e 's/\s\s*|\s\s*/ /g' > $TMP1file
		if test -s $TMP1file
		then
			echo
			NECHO "threats columns:" "`head -1 $TMP1file`"
			if grep -qi lastupdatedtime $TMP1file
			then
				echo
			else
				echo "   <== threats.lastupdatedtime is missing"
				INCREMENT_EXCEPTIONS
			fi
		fi

		echo
		NECHO "threats:" ""
		psql -d caspidadb -c "select isactive,status,count(*) from threats group by 1,2 order by 1,2;" 2>/dev/null | egrep -v ' rows*\)' > $TMP1file
                head -1 $TMP1file; ALL_BUT_FIRST $TMP1file

		if $uba32feature
		then
			echo
			LEGACYthreats=`psql -d caspidadb -t -c "select count(*) from threats where computedBy='DeviceThreat' or computedBy='UserThreat';" 2>/dev/null | head -1 | tr -d ' '`
			if test "$LEGACYthreats" = "0"
			then
				:
			else
				NECHO "legacy computedBy:" "$LEGACYthreats"
				echo "   <== need to set 'LegacyThreatComputation' in threats table"
				INCREMENT_EXCEPTIONS
			fi
			LEGACYcategory=`psql -d caspidadb -t -c "select count(*) from threats where threatcategory is null;" 2>/dev/null | head -1 | tr -d ' '`
			if test "$LEGACYcategory" = "0"
			then
				:
			else
				NECHO "legacy threatcategory:" "$LEGACYcategory"
				echo "   <== null values; need to set threatcategory in threats table"
				INCREMENT_EXCEPTIONS
			fi
		fi
		
		psql -d caspidadb -c "select isactive, threatcategory, count(*) from threats group by 1,2 order by 1,2;" 2>/dev/null | egrep -v ' rows*\)' > $TMP1file
		if test -s $TMP1file
		then
			echo
			NECHO "threats by category:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		fi

		THREATrulebased=`psql -d caspidadb -t -c "select count(*) from threats where isactive='t' AND 'RuleBased'=Any(threatcategory);" 2>/dev/null | tr -d ' '`
		if test "$THREATrulebased" = ""
		then
			THREATrulebased="0"
		fi

		if test $THREATrulebased -gt 0
		then
			if test "$NUMnodes" -ge 10
			then
				THREATSmax=2000
			else
				THREATSmax=1000
			fi
			NECHO "threats, rule-based:" "$THREATrulebased"
			if test $THREATrulebased -le $THREATSmax
			then
				echo
			else
				echo "   <= more than $THREATSmax active, rule-based threats for $NUMnodes nodes" 
			fi
		fi

		echo
		NECHO "threats by type:" ""
		psql -d caspidadb -c "select t.threattype,count(t.isactive),t.isactive,t.threattypeext,t.ruleid,tr.name as threatrule_name  from threats t left outer join v_threat_rules tr on t.ruleid=tr.id group by t.threattype,t.isactive,t.threattypeext,t.ruleid,tr.name order by t.isactive desc ,t.threattype;" 2>/dev/null | egrep -v ' rows*\)' > $TMP1file
                head -1 $TMP1file; ALL_BUT_FIRST $TMP1file

		NECHO "active threats by day:" ""
		psql -d caspidadb -c "select date_trunc('Day',updatetime) as updatetime,count(*) from threats where isactive='t' and updatetime > current_date - interval '28 days' group by 1 order by 1" 2>/dev/null | egrep -v ' rows*\)'  > $TMP1file
		(head -2 $TMP1file; echo "   . . ."; tail -7 $TMP1file) > $TMP2file
		head -1 $TMP2file; ALL_BUT_FIRST $TMP2file

		echo
		NECHO "old (+90 days) threats:" ""
		psql -d caspidadb -c "select isactive,count(*) from threats where lastupdatedtime < '$ANCIENT_days' group by 1 order by 1" 2>/dev/null | egrep -v ' rows*\)'  > $TMP1file
		head -1 $TMP1file
		ALL_BUT_FIRST $TMP1file

		echo
		if $uba40feature
		then
			firstlast=`psql -d caspidadb -c '\d v_threats' 2>/dev/null | egrep "^ firstcomputedtime|^ lastupdatedtime" | wc -l`
			if test $firstlast -ne 2
			then
				NECHO "v_threats:" "firstcomputedtime/lastupdatedtime columns missing   <== recreate view"; echo
				INCREMENT_EXCEPTIONS 
				echo
			fi
		fi

		NECHO "sessions (estimate):" ""
		psql -d caspidadb -t -c "SELECT reltuples::bigint AS estimate FROM pg_class where relname='sessions';" 2>/dev/null | tr -d ' ' > $TMP1file
                head -1 $TMP1file; ALL_BUT_FIRST $TMP1file

#		NECHO "sessions by day:" ""
#		psql -d caspidadb -c "select date_trunc('Day',lastactivity),count(*) from sessions where lastactivity > current_date - interval '7 days' group by 1 order by 1" 2>/dev/null | egrep -v ' rows*\)' > $TMP1file
#		
#		ERROR="   <== note no sessions on ${DAYBEFORE}, $YESTERDAY or $TODAY"
#                for DATE in $TODAY $YESTERDAY $DAYBEFORE
#                do
#                        if grep -q "$DATE" $TMP1file
#                        then
#                                ERROR=""
#                                break
#                        fi
#                done
#                if ! test -z "$ERROR"
#                then
#                        INCREMENT_EXCEPTIONS 1 30
#                fi
#
#		(head -2 $TMP1file; echo "   . . .$ERROR"; tail -7 $TMP1file) > $TMP2file
#		head -1 $TMP2file; ALL_BUT_FIRST $TMP2file

		
		echo
		GEOIPmmdb="conf/etl/geo-db/maxmind/GeoLite2-City.mmdb"
		if test -f /etc/caspida/local/$GEOIPmmdb
		then
			NECHO "MaxMind (local):" ""
			ls -l /etc/caspida/local/$GEOIPmmdb
			MMDB_BUILD /etc/caspida/local/$GEOIPmmdb
		fi

		NECHO "MaxMind (jar):" ""
		if jar tvf /opt/caspida/lib/CaspidaSecurity.jar $GEOIPmmdb 2>/dev/null
		then
			mkdir -p $TMP/uhc.mmdb/
			if test -d $TMP/uhc.mmdb/
			then
				(cd $TMP/uhc.mmdb/; jar xf /opt/caspida/lib/CaspidaSecurity.jar $GEOIPmmdb) 
				if test -f $TMP/uhc.mmdb/$GEOIPmmdb
				then
					MMDB_BUILD $TMP/uhc.mmdb/$GEOIPmmdb
					rm -rf $TMP/uhc.mmdb/ > /dev/null 2>&1
				fi
			fi
		fi

		if test -f /opt/splunk/share/GeoLite2-City.mmdb 
		then
			NECHO "MaxMind (file):" ""
			ls -l /opt/splunk/share/GeoLite2-City.mmdb 
			MMDB_BUILD /opt/splunk/share/GeoLite2-City.mmdb
		fi

		echo
                if $uba42feature
                then
			URL="https://${JOB_MANAGER_host}:9002/rules/list"
                        TOKEN=`grep '^\s*jobmanager.restServer.auth.user.token=' /opt/caspida/conf/uba-default.properties | cut -d'=' -f2`
                        AUTH="-Ssk -H 'Authorization: Bearer $TOKEN'"
                else
                        URL="http://${JOB_MANAGER_host}:9002/rules/list"
                        AUTH=""
                fi
		eval curl --connect-timeout 30 $AUTH $URL 2>$TMP2file | egrep '"id"|"name"|"path"|"active"' |\
		sed -e 's/["|,]//g' -e 's/ : /:/' -e 's/^  //' | awk 'BEGIN {FS=":"} /active/ {print $2; next} {printf "%s|",$2}' |\
		sort -nf -t '|' -k 1 | sed -s 's/^[^\|]*|//' | sort > $TMP1file
		if test -s $TMP1file
		then
			( echo "Rule|Path|Active"; echo "----|----|------"; cat $TMP1file ) > $TMP2file		# add headers
			while read LINE
			do
				echo "$LINE" | cut -d '|' -f 1 | wc -c
			done < $TMP2file | sort -nr > $TMP1file
			RULEsize=`head -1 $TMP1file`

			AWKprog="BEGIN{FS=\"|\"} {printf \"%-${RULEsize}.${RULEsize}s\\t%s\\t%s\\n\", \$1,\$3,\$2}"

			awk "$AWKprog" < $TMP2file > $TMP1file
		else
			echo "   <== no rules available; check jobmanager" > $TMP1file
			cat $TMP2file >> $TMP1file
			INCREMENT_EXCEPTIONS 1 31
		fi
		NECHO "rules:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file

		if test -d /etc/caspida/local/conf/rules
		then
			CURRwd="`pwd`"
			cd /etc/caspida/local/conf/rules
			find . -type f -name '*.rule' -exec ls -ld {} \; 2>/dev/null > $TMP1file
			cd "$CURRwd"
			if test -s $TMP1file
			then
				echo
				NECHO "rules (local):" "`head -1 $TMP1file`"; echo
				ALL_BUT_FIRST $TMP1file
			fi
		fi

		echo 
		NECHO "rules name:" ""
		psql -d caspidadb -c "select  id, ruletype, name from rules;" 2>/dev/null |\
		egrep -v ' rows*\)' > $TMP1file
		head -1 $TMP1file; ALL_BUT_FIRST $TMP1file

		echo
		NECHO "rule packages:" ""
                psql -d caspidadb -c "select name,version,namespace,date_trunc('Seconds',installtime) as installtime,date_trunc('Seconds',updatetime) as updatetime from rulepackages;" 2>/dev/null | egrep -v ' rows*\)' > $TMP1file
                head -1 $TMP1file; ALL_BUT_FIRST $TMP1file

		NECHO "rule specifications ..." "" ; echo
		psql -d caspidadb -t -c "select name,createdby,ruletype from rules where isactive=true order by ruletype,name;" 2>/dev/null |\
		sed -e 's/^ //' -e 's/  *| /\t/g' -e '/^$/d' |\
		while read LINE
		do
			NAME=`echo "$LINE" | cut -f 1`
			CREATEDBY=`echo "$LINE" | cut -f 2`
			RULETYPE=`echo "$LINE" | cut -f 3`
			echo "  name: '$NAME'; ruletype: '$RULETYPE'; created by: '$CREATEDBY'"
			echo "  specification:"
			psql -d caspidadb -t -c "select spec from rules where name='$NAME';" 2>/dev/null |\
			sed -e '/^$/d' |\
			python -m json.tool 2>/dev/null |\
			fmt -s -w 80 |\
			sed -e 's/^/    /' 
			echo
		done  | sed -e "s/^/$INDENT/"

		if test -d /etc/caspida/local/conf/modelregistry/
		then
			for REGISTRY in offlineworkflow streaming
			do
				echo
				NECHO "custom ${REGISTRY}:" ""
				if test -r /etc/caspida/local/conf/modelregistry/${REGISTRY}/ModelRegistry.json
				then
					ls -ld /etc/caspida/local/conf/modelregistry/${REGISTRY}/ModelRegistry.json > $TMP1file
					sed -e 's+//.*$++' -e '/^.\*/d' < /etc/caspida/local/conf/modelregistry/${REGISTRY}/ModelRegistry.json |\
					python -m json.tool 2>&1 >> $TMP1file
					head -1 $TMP1file
					ALL_BUT_FIRST $TMP1file
				else
					if test -f /etc/caspida/local/conf/modelregistry/${REGISTRY}/ModelRegistry.json
					then
						echo "   <== not readable; check /etc/caspida/local/conf/modelregistry/${REGISTRY}/ModelRegistry.json permissions"
						INCREMENT_EXCEPTIONS
					else
						echo "no customization"
					fi
				fi
			done
		else
			echo
			NECHO "modelregistry:" "no customizations"; echo
		fi

		if test -d /etc/caspida/local/conf/analytics
		then
			(cd /etc/caspida/local/conf/analytics; find . -type f -exec ls -l {} \;) > $TMP1file
			if test -s $TMP1file
			then
				echo
				NECHO "custom analytics:" "`head -1 $TMP1file`"; echo
				ALL_BUT_FIRST $TMP1file
			fi
		fi

		echo 
		/opt/caspida/bin/ModelRegistryControl.sh -l 2>/dev/null |\
		egrep '"displayName"|"type"|"enabled"' |\
		sed -e 's/^  //' -e 's/,$//' -e 's/ : /:/' -e 's/"//g' |\
		awk -f $AWKfile > $TMP1file 
		LINEcount=`wc -l < $TMP1file`
		if test $LINEcount -gt 2
		then
			NECHO "model registry:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		fi	


		echo
		NECHO "custom dashboards ..." "" ; echo
                psql -d caspidadb -t -c "select name,creationemail from dashboards;" 2>/dev/null |\
                sed -e 's/^ //' -e 's/  *| /\t/g' -e '/^$/d' |\
                while read LINE
                do
                        NAME=`echo "$LINE" | cut -f 1`
                        CREATEDBY=`echo "$LINE" | cut -f 2`
                        echo "  name: '$NAME'; created by: '$CREATEDBY'"
                        echo "  components:"
                        psql -d caspidadb -t -c "select components from dashboards where name='$NAME';" 2>/dev/null |\
                        sed -e '/^$/d' |\
                        python -m json.tool 2>/dev/null |\
                        fmt -s -w 80 |\
                        sed -e 's/^/    /'
                        echo
                done | sed -e "s/^/$INDENT/" 

		echo
		# psql -d caspidadb -c "select entitytype,name,id,creationemail,creationtime from tags where NOT (creationemail ilike 'UBA' or creationemail ilike 'user@caspida.com%') order by entitytype,id;" 2>/dev/null | egrep -v ' rows*\)' > $TMP1file

		psql -d caspidadb -c "select entitytype,name,creationemail,creationtime from tags group by 1,2,3,4 order by 1,2,3,4;" 2>/dev/null |\
		egrep -v ' rows*\)' > $TMP1file
		if test -s $TMP1file
		then
			echo
			NECHO "all tag names:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		fi 
		
		psql -d caspidadb -c "select entitytype,name,id,creationemail,creationtime from tags where NOT (creationemail ilike 'UBA' or creationemail ilike 'user@caspida.com%') order by entitytype,id;" 2>/dev/null |\
		sed -e '/^$/d' -e '/^-/d' -e '/^(/d' -e 's/^ //' -e 's/\s\s*|\s\s*/|/g' |\
		while read TAGline
		do
			TAGtype=`echo "$TAGline" | cut -d '|' -f 1`
			if test "$TAGtype" = "entitytype"
			then
				echo "$TAGline|count"
				echo "$TAGline|count" | sed -e 's/[^|]/-/g'
			else
				TAGid=`echo "$TAGline" | cut -d '|' -f 3`
				case $TAGtype in
					Device)	TAGtable="systems";;
					Url)	TAGtable="urls";;
					User)	TAGtable="users";;
					Application) TAGtable="applications";;
					Anomaly) TAGtable="anomalies";;
					*) continue;;
				esac
	
				TAGcount=`psql -d caspidadb -t -c "select count(*) from $TAGtable where ($TAGid = ANY(tagids));" 2>/dev/null | sed -e '/^$/d' -e 's/^\s\s*//'`
				echo "$TAGline|$TAGcount"
			fi
		done |\
		awk 'BEGIN {FS="|"} {printf "%-11.11s | %-25.25s | %-5.5s | %-30.30s | %-19.19s | %7.7s\n", $1,$2,$3,$4,$5,$6}' |\
		sed -e '/^-/s/\s/-/g' > $TMP1file
	 
		NECHO "watchlists:" ""
		head -1 $TMP1file; ALL_BUT_FIRST $TMP1file

fi | tee -a $OUTfile

if $JOB_MANAGER
then
	grep "WatchlistAAExecutor: Found watchlists" /var/log/caspida/system/WatchlistAAExecutor.log  2>/dev/null | tail -12 > $TMP1file

	if test -s $TMP1file
	then
		echo
		NECHO "WatchlistAAExecutor:" "`ls -l /var/log/caspida/system/WatchlistAAExecutor.log`"; echo
		NECHO "" "`head -1 $TMP1file`" ; echo
		ALL_BUT_FIRST $TMP1file
	fi
fi | tee -a $OUTfile

if $RESOURCESMONITOR || $SYSMONITOR
then
	MONITORdir="/var/log/caspida/monitor"
	if test -d $MONITORdir
	then
		ls -l $MONITORdir | egrep "$TODAYmmm_d |$YESTERDAYmmm_d |$DAYBEFOREmmm_d "
	fi > $TMP1file

	if test -s $TMP1file
	then
		echo
		NECHO "recent monitor logs:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	fi
fi | tee -a $OUTfile

if $RULE_REALTIME_HOST
then
	RRElog=/var/log/caspida/ruleengine/realtimeruleexecutor.log

	if test -f $RRElog
	then
		RRElist="$RRElog"
		for DAY in $TODAY $YESTERDAY
		do
			if test -f ${RRElog}.${DAY}.log.gz
			then
				RRElist="${RRElog}.${DAY}.log.gz $RRElist"
			fi	
		done
		
		zgrep -A 1 "ERROR NewAnomalyTopic-consumer c.c.ipc.messaging.KafkaTopicConsumer" $RRElist |\
		tail -30 > $TMP1file 
		if test -s $TMP1file
		then
			echo
			NECHO "realtimeruleexecutor err:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		fi
		
		zegrep -h -e ": -----> processNewAnomalies| RealtimeRuleExecutor startup " $RRElist |\
		tail -7200 > $TMP1file 
		if test -s $TMP1file
		then
			head -1 $TMP1file > $TMP2file
			SAMPLEcnt=`wc -l < $TMP1file`
			DURATIONavg=0
			DURATIONhigh=0
			set a `cat $TMP2file`
			TIMESTAMPprev="$2 $3"
			ESECSprev=`date -d "$TIMESTAMPprev" "+%s"`
			echo ". . ." >> $TMP2file
			while read LINE
			do
				set a $LINE 
				TIMESTAMPcurr="$2 $3"
				ESECScurr=`date -d "$TIMESTAMPcurr" "+%s"`
				DURATION=`expr $ESECScurr - $ESECSprev`
				DURATIONavg=`expr $DURATIONavg + $DURATION`
				case "$LINE" in
					*" RealtimeRuleExecutor startup "*)	TIMESTAMPprev="$TIMESTAMPcurr"
                                						ESECSprev=$ESECScurr
										continue;;  # reset
				esac
				if test $DURATION -ge 1320 # 120
				then
					echo "$LINE   <= prior RRE duration: $DURATION seconds (prev=$TIMESTAMPprev)"
					if test $DURATION -gt $DURATIONhigh
					then
						DURATIONhigh=$DURATION
						HIGHstart="$TIMESTAMPprev"
						HIGHstop="$TIMESTAMPcurr"
					fi	
				fi
				TIMESTAMPprev="$TIMESTAMPcurr"
				ESECSprev=$ESECScurr
			done < $TMP1file >> $TMP2file
			LASTline=`tail -1 $TMP1file`
			echo ". . ." >> $TMP2file
			SAMPLEcnt=`expr $SAMPLEcnt - 1`
			DURATIONavg=`expr $DURATIONavg / $SAMPLEcnt`
			if test $DURATIONhigh -gt 0
			then
				HIGHEST="; high: $DURATIONhigh seconds"
				zgrep -h "^2" $RRElist | sed -ne "/^$HIGHstart/,/^$HIGHstop/p" > $TMP1file
			else
				HIGHEST=""
				> $TMP1file
			fi
			echo "$LASTline   (average RRE duration: $DURATIONavg seconds${HIGHEST})" >> $TMP2file
			echo
			NECHO "realtimeruleexecutor.log:" "`head -1 $TMP2file`"; echo
			ALL_BUT_FIRST $TMP2file

			if test -s $TMP1file
			then
				echo
				NECHO "longest RRE:" "`head -1 $TMP1file`"; echo
                        	ALL_BUT_FIRST $TMP1file
			fi

		fi
	fi
fi | tee -a $OUTfile

if $CASPIDA_PROPERTIES_EXIST && $DATABASE_HOST
then
	echo
	NECHO "datasource related ..." "" ; echo

	if $uba51feature
	then
		DSmicro=`egrep -e '^splunk\.micro\.batching\.search\.' $UBA_SITE_PROPERTIES 2>/dev/null | cut -d '=' -f 1 | sed -e 's/$/ /'`
		DSmicro=`echo $DSmicro`
	fi

	for PROPerty in connector.splunk.micro.batching.backtrace connector.splunk.max.backtrace.time.in.hour splunk.indexed.realtime \
		splunk.kafka.ingestion.search.delay.seconds splunk.kafka.ingestion.search.interval.seconds\
		splunk.kafka.ingestion.search.max.lag.seconds \
		splunk.live.micro.batching \
		splunk.live.micro.batching.delay.seconds splunk.live.micro.batching.interval.seconds $DSmicro
	do
		GET_CASPIDA_PROPERTY $PROPerty
		if test "$PROPERTYvalue" = ""
		then
			case $PROPerty in
				*delay*) 	PROPERTYvalue="180   (documented default)";;
				*interval*) 	PROPERTYvalue="60   (documented default)";;
				*backtrace*) 	PROPERTYvalue="4   (documented default)";;
			esac
		fi

		if test "$PROPerty" = "connector.splunk.max.backtrace.time.in.hour"
		then
			LAGmax=`expr "$PROPERTYvalue" \* 3600 2>/dev/null`
			if test "$LAGmax" = ""
			then
				LAGmax=14400
			fi
		fi

		NECHO "" "${PROPerty}=$PROPERTYvalue"; echo
	done

	echo
	if $JOB_MANAGER
	then
		/opt/caspida/bin/CaspidaJobUtils.py -l 2>/dev/null > $TMP2file
	else
		ssh $JOB_MANAGER_host "/opt/caspida/bin/CaspidaJobUtils.py -l" 2>/dev/null > $TMP2file
	fi
	egrep  'Completed|Scheduled|Processing|DatasourceName' < $TMP2file > $TMP1file
	HEADER=`head -1 $TMP1file`
	UNDERline=`echo $HEADER | sed -e 's/\S/-/g'`
	NECHO "datasources:" "$HEADER"
	LINEcount=`wc -l < $TMP1file`
	if test "$LINEcount" -gt "1"
	then
		echo
		echo "$HEADER" | sed -e 's/[^ ]/-/g' -e "s/^/$INDENT/"
	else
		echo "   <== verify that all datasources have been started"
		INCREMENT_EXCEPTIONS 1 32
	fi
	ALL_BUT_FIRST $TMP1file


	if $uba32feature
	then
		TYPEDEFfile="/etc/caspida/local/conf/etl/setup/type_definitions.json"
		TYPEDEFfiles="$TYPEDEFfile $CASPIDAdir/conf/etl/setup/type_definitions.json"
	else
		TYPEDEFfile="$CASPIDAdir/conf/etl/setup/type_definitions.json"
		TYPEDEFfiles="$TYPEDEFfile"
	fi
	( psql -d caspidadb -t -c "select format from datasources where NOT (format='System' or format='CSV' or format='SPLUNK/DIRECT' or format='Splunk');"
	psql -d caspidadb -t -c 'select caspidatype from splunksourcetypemappings;') 2>/dev/null | sort -u | tail -n +2 |\
	while read MAPPING
	do  
		if egrep -h -e "type\":\"$MAPPING\"" $TYPEDEFfiles 
		then
			:
		else
			echo "type: $MAPPING   <= type not found in $TYPEDEFfile"
			# INCREMENT_EXCEPTIONS
		fi
	done > $TMP1file
	echo
	NECHO "type_definitions.json" "`head -1 $TMP1file`"; echo
	ALL_BUT_FIRST $TMP1file

	psql -d caspidadb -c 'select sourcetype,caspidatype from splunksourcetypemappings order by sourcetype;' 2>/dev/null | egrep -v ' rows*\)'> $TMP1file
	
	ROWSmap=`wc -l < $TMP1file`
	if test $ROWSmap -lt 29
	then
		ERROR="   <= verify that sourcetype mappings are correct" 
	else
		ERROR=""
	fi
	echo
	NECHO "mappings:" "`head -1 $TMP1file`"; echo "$ERROR"
	ALL_BUT_FIRST $TMP1file

	ERROR=""
        echo
        NECHO "hrdata stats:" ""
        psql -d caspidadb -c "SELECT datasources.name, datasources.type, datasources.status, datasources.error, datasources.format, datasources.assignedhost, \
                connectorstats.lastprocessed, date_trunc('Seconds',connectorstats.lastmodified) as lastmodified, connectorstats.numprocessed, connectorstats.epsjson::json->>'EPS' as recent_eps, \
                connectorstats.epsjson::json->>'Avg EPS' as average_eps, connectorstats.epsjson::json->>'Max EPS' as max_eps FROM connectorstats  \
                INNER JOIN datasources on connectorstats.id = datasources.id WHERE connectorstats.epsjson like '{%}' AND datasources.type='SplunkHRData' AND datasources.status=ANY(ARRAY['Scheduled','Processing','Completed']) ORDER BY name, lastmodified DESC limit 5;" 2>/dev/null |\
                egrep -v ' rows*\)'> $TMP1file

	for DAYSpast in 0 1 2 3 4 5 6 7 
	do 
		set `date -d "-$DAYSpast days" '+%Y %m %d'`
		YYYY=$1
		MONTH=$2
		DAY=$3
		if grep " ${YYYY}-${MONTH}-${DAY} " $TMP1file
		then
			break
		fi
	done > $TMP2file

	if test -s $TMP2file
	then
		ERROR=""
	else
		ERROR="   <= no recent activity; check Identity/HRData scheduling"
	fi
	
        echo  "`head -1 $TMP1file`$ERROR"
        ALL_BUT_FIRST $TMP1file

	if $RESOURCESMONITOR
	then
		if test -f /var/log/caspida/monitor/resourcesMonitor.out
		then
			zegrep -e "^20[0-9][0-9]-[0-9][0-9]-[0-9][0-9]T|sysDiskUsage" `ls -rt  /var/log/caspida/monitor/resourcesMonitor.out* 2>/dev/null` 2>/dev/null
		fi
	else
		ssh $RESOURCESMONITOR_list 'ls -rt  /var/log/caspida/monitor/resourcesMonitor.out*' > $TMP1file 2>/dev/null
		if test -s $TMP1file
		then
			ssh $RESOURCESMONITOR_list 'zegrep -e "^20[0-9][0-9]-[0-9][0-9]-[0-9][0-9]T|sysDiskUsage" `ls -rt  /var/log/caspida/monitor/resourcesMonitor.out* 2>/dev/null`' 2>/dev/null
		fi
	fi |\
	awk 'BEGIN { logTime="1972-01-01T00:00:01UTC"; } /sysDiskUsage/ { printf ("%s %s\n", logTime, $0); next; } { logTime=$0 }' |\
	egrep "$TODAY|$YESTERDAY|$DAYBEFORE" > $TMP2file
	if test -s $TMP2file
	then
		echo
		NECHO "resources monitor disk:" "`head -1 $TMP2file`"; echo
		ALL_BUT_FIRST $TMP2file
	fi


	echo
	NECHO "java_home:" "$JAVA_HOME"; echo
	KEYstore=`find -L  $JAVA_HOME -name cacerts`
	if test "$KEYstore" = ""
	then
		KEYstore=/usr/lib/jvm/default-java/jre/lib/security/cacerts
	fi
	NECHO "keystore:" "$KEYstore"; echo

	echo
	NECHO "output connectors:" ""
	psql -d caspidadb -c "select name,type,date_trunc('Seconds',creationtime) as creationtime, id from outputconnectors order by 3;" 2>/dev/null | egrep -ve ' rows*\)$' >$TMP1file 	
	head -1 $TMP1file
	ALL_BUT_FIRST $TMP1file

	cp $TMP1file $OCout		# save output on OCout for both connectors


	#
	#	Check for ES connector
	#
	if egrep '\bSplunkES\b' $OCout > $TMP3file
        then
		Cid=`cut -d '|' -f 4 < $TMP3file | sed -e 's/^\s\s*//' -e 's/\s\s*$//'`

		DISPLAY_OUTPUT_CONNECTOR $Cid | tee $TMP1file

		ApiUrl=`grep splunkApiUrl $TMP1file | sed -e 's/^[^:][^:]*: "//' -e 's/".*$//'`
		echo "$ApiUrl" > $ENDpoints
		EShostport=`echo "$ApiURL" | sed -e 's+[^/][^/]*//++'`
		echo

		if $uba32feature
		then	
			SSLprop=false
			SSLcert=false

			GET_CASPIDA_PROPERTY connectors.output.splunkes.ssl
			if test "$PROPERTYvalue" = "true"
			then
				SSLprop=true
			fi
			echo "connectors.output.splunkes.ssl=$PROPERTYvalue" > $TMP1file
			for PROPerty in uba.splunkes.integration.enabled \
				uba.sys.audit.push.splunk.enabled \
				uiServer.host
			do
				GET_CASPIDA_PROPERTY $PROPerty
				echo "$PROPerty=$PROPERTYvalue"
			done >> $TMP1file
			if test -s $TMP1file
			then
				NECHO "es-integration props:" "`head -1 $TMP1file`"; echo
				ALL_BUT_FIRST $TMP1file
				echo
			fi

			if ssh $FIRSTnode "KEYpass=changeit keytool -list -v -alias 'splunk es' -storepass:env KEYpass -keystore $KEYstore" > $TMP1file 2>$TMP2file
			then
				SSLcert=true
				NECHO "SplunkES connector:" "(keystore contains 'splunk es' alias)"; echo
				NECHO "splunk es' alias:" "`head -1 $TMP1file`"; echo
				ALL_BUT_FIRST $TMP1file
			else
				:	# $TMP2file contains error
			fi

			if $SSLprop
			then
				if ! $SSLcert
				then
					NECHO "SplunkES connector:" "`head -1 $TMP2file`"
					echo "   <== SSL cacert.pem missing"
					INCREMENT_EXCEPTIONS
					ALL_BUT_FIRST $TMP2file
				fi
				if test "$EShostport" != ""
				then
					timeout 10 openssl s_client -connect $EShostport > $TMP1file 2>/dev/null
					if test -s $TMP1file
					then
						echo
						NECHO "ES $EShostport:" "`head -1 $TMP1file`"; echo
						ALL_BUT_FIRST $TMP1file
					fi
				fi
			else
				if ! $SSLcert
				then
					NECHO "SplunkES connector:" "(SSL not configured)"; echo
				fi
			fi
		fi
	fi

	echo

	#
	#	Check for Email connector
	#
	if egrep '\bEmail\b' $OCout > $TMP3file
        then
		Cid=`cut -d '|' -f 4 < $TMP3file | sed -e 's/^\s\s*//' -e 's/\s\s*$//'`

		DISPLAY_OUTPUT_CONNECTOR $Cid | tee $TMP1file
		echo

		SMTPhost=`grep '"host":' $TMP1file | sed -e 's/^.*: "//' -e 's/".*$//'`
		SMTPport=`grep '"port":' $TMP1file | sed -e 's/^.*: //' -e 's/,.*$//'`

		if test "$SMTPhost" != ""
		then
			if test "$SMTPport" = ""
			then
				SMTPport="25"
			fi
			
			BASEsmtp=`basename $PYsmtp`
			for SMTPclient in "localhost" $HOSTname
			do
				timeout 20 ssh $JOB_MANAGER_host python3 ~/$BASEsmtp $SMTPclient $SMTPhost $SMTPport > $TMP1file 2>/dev/null
				if test -s $TMP1file
				then
					NECHO "smtp info:" "`head -1 $TMP1file`"; echo
					ALL_BUT_FIRST $TMP1file
					echo
				fi
			done
		fi

		if $uba43feature
		then	
			SSLcert=false

			if ssh $FIRSTnode "KEYpass=changeit keytool -list -v -alias 'splunk smtp' -storepass:env KEYpass -keystore $KEYstore" > $TMP1file 2>&1
			then
				SSLcert=true
			else
				:	# $TMP1file contains error
			fi

			if $SSLcert
			then
				NECHO "Email connector:" "(keystore contains 'splunk smtp' alias)"
				if test "$SMTPport" = "465"
				then
					echo
				else
					echo "   <= SSL vcert not used for port $SMTPport"
				fi
			else
				NECHO "Email connector:" "`head -1 $TMP1file`"
				if test "$SMTPport" = "465"
				then
					echo "   <== SSL cacert.pem missing"
					INCREMENT_EXCEPTIONS
				else
					echo "   <= SSL cacert.pem missing; not used for port $SMTPport"
				fi
				ALL_BUT_FIRST $TMP1file
			fi
		fi

		ssh $JOB_MANAGER_host cat /opt/caspida/conf/mail-connector.properties > $TMP1file 2>/dev/null
		if test -s $TMP1file
		then
			echo
			NECHO "mail-connector.properties:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST /opt/caspida/conf/mail-connector.properties
		fi
	fi

	for INTEGRATIONlog in /var/log/caspida/outputconnector.log /var/log/caspida/system/esnotableupdates.log 
	do
		ssh $JOB_MANAGER_host tail -30 $INTEGRATIONlog 2>&1 | fmt -s -w 100 > $TMP1file
		if test -s $TMP1file
		then
			BASElog=`basename $INTEGRATIONlog`
			echo
			NECHO "$BASElog:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		fi
	done
	
	echo 

	GET_CASPIDA_PROPERTY splunk.direct.enum.normalize
	if test "$PROPERTYvalue" = "true"
	then
		TAG="(d)"
		for NORMfile in /etc/caspida/conf/normalize.rules /etc/caspida/local/conf/normalize.rules.
		do
			if test -r $NORMfile
			then
				echo
				NECHO "normalize.rules $TAG:" "`ls -l $NORMfile`"; echo	
				NECHO "" "`head -1 $NORMfile`"; echo	
				ALL_BUT_FIRST $NORMfile
				TAG="(l)"
			fi
		done
	fi
	
	echo

	PSQLquery="select name,enumstatsbad from datasources 
		where type='Splunk' AND format='SPLUNK/DIRECT' AND enumstatsbad != '';"
	psql -d caspidadb -t -c "$PSQLquery" 2>/dev/null |\
	sed -e 's/^\s\s*//' -e 's/\s\s*|\s\s*/|/' -e 's/\s\s*$//' |\
	while read DSname_DSenum
	do
		DSname=`echo "$DSname_DSenum" | cut -d '|' -f 1`
		DSenum=`echo "$DSname_DSenum" | cut -d '|' -f 2`
		echo "$DSenum" | python -m json.tool > $TMP1file 2>/dev/null
		if test -s $TMP1file
		then
			NECHO "    $DSname:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
			echo
		fi
	done > $TMP2file
	if test -s $TMP2file
	then
		NECHO "datasource bad enum ..." ""; echo
		cat $TMP2file
	fi

	NECHO "datasource information ..." ""; echo
	
	for DSid in `psql -t -d caspidadb -c "select id from datasources where (type!='System' AND type!='SplunkSkipEtl')  order by type desc,name;" 2>/dev/null `
	do
		DSid=`echo "$DSid" | sed -e 's/^\s\s*//' -e 's/\s\s*$//' -e '/^$/d'`

		DISPLAY_DATASOURCE "$DSid"

		echo
	done
	grep '"endpoint":' $OUTfile |\
	sed -e 's/^.*endpoint": "//' -e 's/",.*$//' |\
	sort -u | grep  http >> $ENDpoints
	while read ENDpoint
	do
		echo "$ENDpoint ..."
		echo -n "    "
		if python3 $PYscript "$ENDpoint" 2>/dev/null 
		then
			:
		else
			if python $PYscript "$ENDpoint" 2>/dev/null
			then
				:
			else
				echo "(version unavailable)"
			fi
		fi 
	done < $ENDpoints > $TMP1file

	if test -s $TMP1file
	then
		NECHO "endpoint splunk version:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	fi

	if test -s $RESTapps
	then
		sort -u < $RESTapps > $TMP1file 
		if test -s $TMP1file
		then
			NECHO "endpoint app version:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		fi
	fi


	if $JOB_MANAGER
	then
		grep "Started CaspidaJobManager" /var/log/caspida/jobmanager-debug.log 2>/dev/null
	else
		ssh $JOB_MANAGER_host 'grep "Started CaspidaJobManager" /var/log/caspida/jobmanager-debug.log 2>/dev/null' 2>/dev/null
	fi |\
	tail -1 > $TMP1file
	if test -s $TMP1file
	then
		echo
		NECHO "jobmanager start:" ""; head -1 $TMP1file
	fi

	echo
	NECHO "datasource processes:" ""
	psql -d caspidadb -c "select datasources.name as name,location,CASE WHEN state='Processing' THEN to_char(latestprocessstarttime,'YYYY-MM-DD HH24:MI:SS') ELSE '' END as latestprocessstarttime,date_trunc('Minutes',lastprocessed) as lastprocessed,date_trunc('Minutes',lastmodified) as lastmodified,state from connectorstats INNER JOIN datasources on connectorstats.id = datasources.id where (state='Stopped' OR state='Processing' OR state='Scheduled') AND lastprocessed > current_date - interval '3 days' order by lastmodified desc, state, datasources.name;" 2>/dev/null |\
	egrep -v ' rows*\)'> $TMP1file 
	head -1 $TMP1file
	ALL_BUT_FIRST $TMP1file

	tail -n +3 $TMP1file | sed -e 's/\s//g' -e '/^$/d' > $TMP2file
	if test -s $TMP2file
	then
		ROLESflag=false
		VERBOSE="-v"
		NECHO "datasource logs:" ""; echo
		cut -d '|' -f 1,2,6 < $TMP2file |\
		while read LINE
		do 
			DSstate=`echo "$LINE" | cut -d '|' -f 3`
			case "$DSstate" in
				"Stopped") continue;;
				"Completed") continue;;
			esac
        		DSname=`echo "$LINE" | cut -d '|' -f 1`
        		DShost=`echo "$LINE" | cut -d '|' -f 2 | cut -d '/' -f 2`
        		DSpid=`echo "$LINE" | cut -d '|' -f 2 | cut -d '/' -f 3`
        		DSlog="/var/log/caspida/jobexecutor-${DSpid}.log"
			ssh -n $VERBOSE $DShost tail -100  $DSlog > $TMP1file 2>&1
			RETurn="$?"
			NECHO "    $DSname" "${DShost}:$DSlog  ($RETurn) $DSstate"; echo
			NECHO "" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		
			VERBOSE=""
			
			echo		
			ssh -n $DShost "eval \"zegrep -he  'SplunkRealTimeIngestionThread: intervalInSecs=|SplunkRealTimeIngestionThread: last run|Search not executed' ${DSlog}* 2>/dev/null\"" > $SEARCHmessages 2>/dev/null
			grep -v "Search not executed" $SEARCHmessages > $TMP2file
			echo >> $TMP2file
			ssh -n $DShost tail -3000  $DSlog 2>/dev/null > $TMP1file
			kafkaDelay=`awk -f $KAFKAdelay $TMP1file`
			egrep -e " currentLag=|Search completed in | fetcher: #events=" < $TMP1file |\
			tail -20 >> $TMP2file
			if test -s $TMP2file
			then
				ERROR=""
				if grep "currentLag=" $TMP2file > $TMP1file
				then
					LAGprior==`head -1 $TMP1file | sed -e 's/^.*currentLag=//' -e 's/ secs.*$//'`
					LAGprior=`expr "$LAGprior" + 0 2>/dev/null`
					LAGcurrent=`tail -1 $TMP1file | sed -e 's/^.*currentLag=//' -e 's/ secs.*$//'`
					NOWlast=`tail -1 $TMP1file | sed -e 's/^.*now=\[//' -e 's/,.*$//'`
					NOWlast=`expr \( \( $NOWlast / 60 \) \* 60 \)  2>/dev/null`
					EARLIESTlast=`tail -1 $TMP1file | sed -e 's/^.*earliest=\[//' -e 's/,.*$//'`
					LATESTlast=`tail -1 $TMP1file | sed -e 's/^.*latest=\[//' -e 's/,.*$//'`
					INTERVALlast=`expr "$LATESTlast" - "$EARLIESTlast" 2>/dev/null`
					DELAYlast=`expr "$NOWlast" - "$EARLIESTlast" 2>/dev/null`
					LAGcurrent=`expr "$LAGcurrent" + 0 2>/dev/null`
					ERROR="( delay: $DELAYlast; interval: $INTERVALlast"
					if test "$LAGcurrent" -gt "$LAGmax" 2>/dev/null
					then
						ERROR="$ERROR; lag: $LAGcurrent )   <= lag is greater than backtrace: $LAGmax seconds; restart datasource may be advised"
					else
						if test "$LAGcurrent" -gt "$LAGprior" 2>/dev/null
						then
							ERROR="$ERROR; lag: $LAGcurrent )   <= lag is increasing; review datasource"
						else
							ERROR="$ERROR )"
						fi
					fi
				fi
				NECHO "    $DSname" "... search info ...  "; echo "$ERROR"
				NECHO "" "`head -1 $TMP2file`"; echo
				ALL_BUT_FIRST $TMP2file

				echo
				grep "Search not executed" $SEARCHmessages | sort | uniq -c > $TMP2file
				if test -s $TMP2file
				then
					NECHO "    $DSname" "... search failures ...  "; echo "$ERROR"
					NECHO "" "`head -1 $TMP2file`"; echo
					ALL_BUT_FIRST $TMP2file
					echo
				fi

				if test "$kafkaDelay" != ""
				then
					echo
					NECHO "    $DSname" "... kafka producer progress ...  "; echo
					NECHO "" "$kafkaDelay"; echo
					echo
				fi

			fi

			if $ROLESflag
			then
				continue
			fi

                        ROLESinfo="\
SplunkEventProducer: ===================    User Info: Number of roles|\
SplunkEventProducer: Role Name:|\
SplunkEventProducer:     - edit_uba_settings|\
SplunkEventProducer:     - edit_forwarders|\
SplunkEventProducer:     - list_forwarders|\
SplunkEventProducer:     - rtsearch|\
SplunkEventProducer:     - edit_reviewstatuses|\
SplunkEventProducer:    Max disk space|\
SplunkEventProducer:    Max concurrent"

			ssh -n $DShost "eval \"zegrep -e  '$ROLESinfo' ${DSlog}* 2>/dev/null\"" > $TMP2file 2>/dev/null

			if test -s $TMP2file
			then
				NECHO "    $DSname" "... role info ...  "; echo
				NECHO "" "`head -1 $TMP2file`"; echo
				ALL_BUT_FIRST $TMP2file
				echo

				ROLESflag=true
			fi
		done
	fi
	
	echo
	NECHO "datasource stats:" ""

	if $uba41feature
	then
		SUCCESSsql="SELECT DISTINCT ON (d.name) d.name, d.type, d.status, d.error, d.format, date_trunc('Seconds',cstats.lastprocessed) as lastprocessed, cstats.epsjson::json->>'EPS' as recent_eps, cstats.epsjson::json->>'Avg EPS' as average_eps, cstats.epsjson::json->>'Max EPS' as max_eps, dstats.countevents, dstats.countfailedevents, dstats.countskippedevents, (100-((dstats.countskippedevents + dstats.countfailedevents)*100)/dstats.countevents) as success_perc, \
                case \
                  when d.status != 'Processing' then '' \
                  when (100-((dstats.countskippedevents + dstats.countfailedevents)*100)/dstats.countevents)=0 then '   <== utter failure; events not processed; review datasource SPL' \
                  when (100-((dstats.countskippedevents + dstats.countfailedevents)*100)/dstats.countevents)=100 then '(complete success)' \
                  when (100-((dstats.countskippedevents + dstats.countfailedevents)*100)/dstats.countevents)<=40 then '   <== significant failed/skipped events; review datasource SPL' \
                  when (100-((dstats.countskippedevents + dstats.countfailedevents)*100)/dstats.countevents)<=70 then '   <= check failed/skipped events; review datasource SPL' \
                  else '' end as observation \
                FROM connectorstats as cstats \
                INNER JOIN datasourcestats as dstats on cstats.id = dstats.id \
                INNER JOIN datasources as d on cstats.id = d.id WHERE NOT (d.type='System' OR d.type='File') AND cstats.lastprocessed IS NOT NULL AND dstats.countevents!=0 ORDER BY name, lastmodified DESC; \
		"
	else
		SUCCESSsql="SELECT DISTINCT ON (d.name) d.name, d.type, d.status, d.error, d.format, d.assignedhost, date_trunc('Seconds',cstats.lastprocessed) as lastprocessed, cstats.epsjson::json->>'EPS' as recent_eps, cstats.epsjson::json->>'Avg EPS' as average_eps, cstats.epsjson::json->>'Max EPS' as max_eps, cstats.numprocessed, cstats.numfailed, cstats.numskipped, (100-((cstats.numskipped + cstats.numfailed)*100)/cstats.numprocessed) as success_perc, \
                case when (100-((cstats.numskipped + cstats.numfailed)*100)/cstats.numprocessed)=0 then '   <== utter failure; events not processed; review datasource SPL' \
                  when (100-((cstats.numskipped + cstats.numfailed)*100)/cstats.numprocessed)=100 then '(complete success)' \
                  when (100-((cstats.numskipped + cstats.numfailed)*100)/cstats.numprocessed)<=50 then '   <== significant failed/skipped events; review datasource SPL' \
                  when (100-((cstats.numskipped + cstats.numfailed)*100)/cstats.numprocessed)<=80 then '   <= check failed/skipped events; review datasource SPL' \
                  else '' end as observation \
                FROM connectorstats as cstats \
                INNER JOIN datasources as d on cstats.id = d.id WHERE NOT (d.type='System' OR d.type='File') AND cstats.lastprocessed IS NOT NULL AND cstats.numprocessed!=0 ORDER BY name, lastmodified DESC; \
		"
	fi

	psql -d caspidadb -c "$SUCCESSsql" 2>/dev/null |\
	egrep -v ' rows*\)'> $TMP1file 


	FAILUREcnt=`grep '<== ' $TMP1file | wc -l`
	if test $FAILUREcnt -ne 0
	then
		INCREMENT_EXCEPTIONS $FAILUREcnt
	fi
	
	LINEcount=`wc -l < $TMP1file`
	NOTprocessed=`egrep -ve "\b$TOMORROW\b" $TMP1file | egrep -ve "\b$TODAY\b" | egrep -ve "\b$YESTERDAY\b" | egrep -ve "\b$DAYBEFORE\b" | wc -l`	
	NOTprocessed=`expr $NOTprocessed - 3`		# ignore headers

	if test "$NOTprocessed" -eq "0"
	then
		if test $LINEcount -ne 3
		then
			ERROR=""
		else
			ERROR="   <== no recent datasource activity"
			INCREMENT_EXCEPTIONS 1 33a
		fi
	else
		ERROR="   <== '$NOTprocessed' datasources not processed since before $DAYBEFORE"
		INCREMENT_EXCEPTIONS 1 33
	fi
	echo  "`head -1 $TMP1file`$ERROR"
        ALL_BUT_FIRST $TMP1file

	if $uba41feature
	then
		cut -d '|' -f 1,9  $TMP1file |\
		while read LINE
		do
			MAXds_eps=`echo $LINE | cut -d '|' -f 2 | tr -d ' '`

			if ! expr "$MAXds_eps" + 0 >/dev/null 2>&1
			then
				continue
			fi

			DSname=`echo $LINE | cut -d '|' -f 1 | tr -d ' '`

			if test $MAXds_eps -le 8000
			then
				continue
			fi

			if test $MAXds_eps -gt 10000
			then
				ERROR="<== max_eps greater than 10K; datasource '$DSname' should be split"
			else
				if test $MAXds_eps -gt 9000
				then
					ERROR="<= max_eps approaching 10K; datasource '$DSname' should be monitored"
				else
					ERROR=""
				fi
			fi

			if grep -q "$DSname" $KAFKAingest 2>/dev/null
			then
				ERROR="(kafka)"
			fi

			echo  "$LINE   $ERROR"
		done > $TMP2file

		FAILUREcnt=`grep '<== ' $TMP2file | wc -l`
		if test $FAILUREcnt -ne 0
		then
			INCREMENT_EXCEPTIONS $FAILUREcnt
		fi
		
		if grep -q " <=" $TMP2file
		then
			NECHO "excessive eps:" "`head -1 $TMP2file`"; echo
			ALL_BUT_FIRST $TMP2file
		fi
	fi

	psql -d caspidadb -t -c "select datasourceid, count(*) from anomalies where eventtime > '$RECENT_days' group by 1 order by 1;" 2>/dev/null |\
	sed -e 's/\s//g' -e '/^$/d'|\
	while read LINE
	do
		DSid=`echo $LINE | cut -d '|' -f 1`
		COUNT=`echo $LINE | cut -d '|' -f 2`
	
		if test "$DSid" != ""
		then
			DSname=`psql -d caspidadb -t -c "select name from datasources where id='$DSid';" 2>/dev/null | sed -e 's/\s//g' -e '/^$/d' `
			if test "$DSname" = ""
			then
				DSname="(not current datasource)"
			fi
		else
			DSname="(no specific reference)"
		fi
		echo "${DSname}|$COUNT"
	done |\
	sort -f -t '|' -k 1 -r |\
	awk 'BEGIN {FS="|"} {printf "%-25.25s | %8d\n", $1, $2}' > $TMP1file

	if test -s $TMP1file
	then
		echo
		NECHO "datasource anomalies:" "`head -1 $TMP1file`"; echo "   (last 14 days)" 
		ALL_BUT_FIRST $TMP1file
	fi

	echo
	for STATICdir in /etc/caspida/local/conf/etl/morphlines/static /opt/caspida/conf/etl/morphlines/static
	do
		ls -l $STATICdir/*.conf  2>/dev/null | tail -n +2 > $TMP1file
		if test -s $TMP1file
		then
			TOPdir=`echo $STATICdir | sed -e 's/^\(....\).*$/\1/'`
			echo
			NECHO "static morphlines $TOPdir:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		fi
	done

	for INCLUDEdir in /etc/caspida/local/conf/etl/morphlines/include /opt/caspida/conf/etl/morphlines/include
	do
		ls -l $INCLUDEdir/*.conf 2>/dev/null | tail -n +2 > $TMP1file
		if test -s $TMP1file
		then
			TOPdir=`echo $INCLUDEdir | sed -e 's/^\(....\).*$/\1/'`
			echo
			NECHO "include morphlines $TOPdir:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		fi
	done

	echo
	> $TMP5file
	if ! $uba41feature
	then
		NECHO "distributed parsers ..." ""; echo
		for DSname in `psql -t -d caspidadb -c "select name from datasources where (type!='System' AND type!='SplunkSkipEtl') AND status='Processing'  order by name;" 2>/dev/null `
		do
			DISPLAY_DATASOURCE_DISTRIBUTED "  $DSname" 
		done 

		if test -s "$TMP5file"
		then
			EXPECTEDparsers=`cut -d ':' -f 2 $TMP5file | awk '{ sum+=$1} END {print sum}'`
			ETLdistributed=`wc -l < $TMP5file`	# used to adjust total count
		else
			EXPECTEDparsers=0
			ETLdistributed=0
		fi
	else
		EXPECTEDparsers=0
                ETLdistributed=0
	fi

	echo
	ASSETdata=false
	if $uba40feature
	then
		NECHO "asset search ..." ""
		for DSid in `psql -t -d caspidadb -c "select id from datasources where type='SplunkSkipEtl' order by type desc,name;" 2>/dev/null `
		do
			DSid=`echo "$DSid" | sed -e 's/^\s\s*//' -e 's/\s\s*$//' -e '/^$/d'`

			DISPLAY_DATASOURCE "$DSid"

		done > $TMP1file

		if test -s $TMP1file
		then
			echo
			ASSETdata=true
			cat $TMP1file

        		psql -d caspidadb -c "SELECT datasources.name, datasources.type, datasources.status, datasources.error, datasources.format, datasources.assignedhost, \
                	connectorstats.lastprocessed, date_trunc('Seconds',connectorstats.lastmodified) as lastmodified, connectorstats.numprocessed, connectorstats.epsjson::json->>'EPS' as recent_eps, \
                	connectorstats.epsjson::json->>'Avg EPS' as average_eps, connectorstats.epsjson::json->>'Max EPS' as max_eps FROM connectorstats  \
                	INNER JOIN datasources on connectorstats.id = datasources.id WHERE connectorstats.epsjson like '{%}' AND datasources.type='SplunkSkipEtl' AND datasources.status=ANY(ARRAY['Scheduled','Processing','Completed']) ORDER BY name, lastmodified DESC limit 5;" 2>/dev/null |\
                	egrep -v ' rows*\)'> $TMP1file

			echo
        		NECHO "assets stats:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		else
			echo "   <== required datasource not found"
			INCREMENT_EXCEPTIONS
		fi

		echo 

		if $ASSETdata
		then
			for FILE in asset_dc_query.txt  asset_es_pull_query.txt  asset_proxy_query.txt
			do
				ERROR=""
				FILEbase=`basename $FILE .txt`
				if test -r /etc/caspida/local/conf/$FILE
				then
					FILEbase="$FILEbase (l)"
					fmt -s -w 80 /etc/caspida/local/conf/$FILE > $TMP1file
					echo
				else
					if test -r /opt/caspida/conf/$FILE
					then
						if test $FILE = "asset_dc_query.txt"
						then
							SPLerror='hostname=replace(sAMAccountName,\"$\",\"\")'
							if grep -q "$SPLerror" /opt/caspida/conf/$FILE
							then
								ERROR="   <== hostname=replace() SPL is incorrect"
								INCREMENT_EXCEPTIONS
							fi
						fi
						FILEbase="$FILEbase (d)"
						fmt -s -w 80 /opt/caspida/conf/$FILE > $TMP1file
					else
						continue
					fi
				fi
				NECHO "${FILEbase}:" "`head -1 $TMP1file`$ERROR"; echo
				ALL_BUT_FIRST $TMP1file
				echo
			done
			echo
		fi
 
		psql -d caspidadb -c "select devicetype,count(*) from assetsdata group by 1 order by 1;" 2>/dev/null  | egrep -ve ' rows*\)$' > $TMP1file
		NECHO "assetsdata device types:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	
		TOTassets=`timeout 90 psql -d caspidadb -t -c "select count(*) from assetsdata;" 2>/dev/null | sed -e '/^$/d' -e 's/ //g'` 

		BLOCKassets=`timeout 90 psql -d caspidadb -t -c "select count(*) from assetsdata where blacklistuserir='true';" 2>/dev/null | sed -e '/^$/d' -e 's/ //g'` 
		if ! test "$BLOCKassets" = ""
		then
			BLOCKirAssets="0"
			BLOCKirOther="0"
			BLOCKirUserInput="0"
			if test "$BLOCKassets" -ne "0"
			then
				BLOCKirAssets=`timeout 90 psql -d caspidadb -t -c "select count(*) from irblacklist i inner join assetsdata a on a.hostname ilike i.entity where a.blacklistuserir='true' AND i.entitytype='Device' AND source='AssetsData';" 2>/dev/null | sed -e '/^$/d' -e 's/ //g'`
				if test "$BLOCKirAssets" = ""
				then
					BLOCKirAssets="-1"
				fi
				BLOCKirUserInput=`timeout 90 psql -d caspidadb -t -c "select count(*) from irblacklist i inner join assetsdata a on a.hostname ilike i.entity where a.blacklistuserir='true' AND i.entitytype='Device' AND source='UserInput';" 2>/dev/null | sed -e '/^$/d' -e 's/ //g'`
				if test "$BLOCKirUserInput" = ""
				then
					BLOCKirUserInput="-1"
				fi
				BLOCKirOther=`timeout 90 psql -d caspidadb -t -c "select count(*) from irblacklist i inner join assetsdata a on a.hostname ilike i.entity where a.blacklistuserir='true' AND i.entitytype='Device' AND (source!='AssetsData' AND source!='UserInput');" 2>/dev/null | sed -e '/^$/d' -e 's/ //g'`
				if test "$BLOCKirOther" = ""
				then
					BLOCKirOther="-1"
				fi
			fi

			echo
			NECHO "user ir block assets:" "assetsdata=$BLOCKassets of $TOTassets; irblacklist=$BLOCKirAssets (AssetData), $BLOCKirUserInput (UserInput), $BLOCKirOther (other)" ; echo
			NECHO "user ir block devices:" ""
			SEP=""
			timeout 90 psql -d caspidadb -t -c "select source,count(*) from irblacklist where entitytype='Device' AND NOT (entityid like 'Expired%') group by 1 order by 1 desc;" 2>/dev/null |\
			sed -e '/^$/d' |\
			while read SOURCE FS COUNT
			do
				echo -n "${SEP}$SOURCE: $COUNT "
				SEP="; "
			done 
			echo 
		fi

		psql -d caspidadb -c "select blacklisttype, entitytype, count(*) from irblacklist where source='UserInput' group by 1,2 order by 1,2;" 2>/dev/null > $TMP1file
		if ! grep -q  "(0 rows)" $TMP1file
		then
			grep -v " rows)" $TMP1file > $TMP2file
			echo
			NECHO "idr exclusion:" "`head -1 $TMP2file`"; echo
			ALL_BUT_FIRST $TMP2file
		fi

                echo; NECHO "current time:" ""; date "+%T %Z"; echo

 		timeout 90 psql -d caspidadb -t -c "select b.entityid,b.source from irblacklist b where b.entitytype='Device' AND b.blacklisttype='User' AND NOT (b.entityid like 'Expired%');" 2>/dev/null |\
 		sed -e '/^$/d' -e 's/\s\s*//g' -e 's/|/ /' |\
 		while read Id Source
 		do 
 			assetDeviceType=`psql -d caspidadb -t -c "select devicetype from assetsdata where caspidaid='$Id';" 2>/dev/null | sed -e 's/^\s//'`
 			systemType=`psql -d caspidadb -t -c "select type from systems where id='$Id';" 2>/dev/null | sed -e 's/^\s//'`
 			echo "$Source=$systemType=$assetDeviceType"
 		done |\
 		sort | uniq -c |\
 		sed -e 's/^\s\s*//' -e 's/^\([0-9][0-9]*\) \(.*$\)/\2\=\1/' |\
 		sort |\
 		awk 'BEGIN {FS="="; hyphens="-------------------------"; printf("%-12.12s | %-12.12s | %-22.22s | %5.5s\n","source", "systems type", "assetsData deviceType","count"); printf("%-12.12s-+-%-12.12s-+-%-22.22s-+-%5.5s\n", hyphens, hyphens, hyphens, hyphens); } {printf("%-12.12s | %-12.12s | %-22.22s | %5d\n",$1,$2,$3,$4);}' > $TMP1file
 
		if test -s $TMP1file
                then
 			echo
 			NECHO "user ir blocklist:" "`head -1 $TMP1file`"; echo
 			ALL_BUT_FIRST $TMP1file
 			echo
		else
			NECHO "user ir blocklist:" "(timeout)"; echo
		fi

                echo; NECHO "current time:" ""; date "+%T %Z"; echo

 		timeout 90 psql -d caspidadb -t -c "select b.entityid,b.source,b.blacklisttype from irblacklist b where b.entitytype='Device' AND b.blacklisttype!='User' AND NOT (b.entityid like 'Expired%');" 2>/dev/null |\
 		sed -e '/^$/d' -e 's/\s\s*//g' -e 's/|/ /' -e 's/|/ /' |\
 		while read Id Source BlacklistType
 		do 
 			assetDeviceType=`psql -d caspidadb -t -c "select devicetype from assetsdata where caspidaid='$Id';" 2>/dev/null | sed -e 's/^\s//'`
 			systemType=`psql -d caspidadb -t -c "select type from systems where id='$Id';" 2>/dev/null | sed -e 's/^\s//'`
 			echo "$Source=$BlacklistType=$systemType=$assetDeviceType"
 		done |\
 		sort | uniq -c |\
 		sed -e 's/^\s\s*//' -e 's/^\([0-9][0-9]*\) \(.*$\)/\2\=\1/' |\
 		sort |\
 		awk 'BEGIN {FS="="; hyphens="-------------------------"; printf("%-12.12s | %-13.13s | %-12.12s | %-22.22s | %5.5s\n","source", "blacklisttype", "systems type", "assetsData deviceType","count"); printf("%-12.12s-+-%-13.13s-+-%-12.12s-+-%-22.22s-+-%5.5s\n", hyphens, hyphens, hyphens, hyphens, hyphens); } {printf("%-12.12s | %-13.13s | %-12.12s | %-22.22s | %5d\n",$1,$2,$3,$4,$5);}' > $TMP1file
 
		if test -s $TMP1file
		then
 			echo
 			NECHO "device ir blocklist:" "`head -1 $TMP1file`"; echo
 			ALL_BUT_FIRST $TMP1file
			echo
		else
			NECHO "device ir blocklist:" "(timeout)"; echo
		fi

		ACTIVEcount=`psql -d caspidadb -t -c "select count(*) from irblacklist where status='Active';" 2>/dev/null | sed -e 's/\s//g'`
		echo
		NECHO "irblocklist active:" "$ACTIVEcount"; echo

		MISMAMEDassets=`psql -d caspidadb -t -c "select count(*) from irblacklist where source='AssetsData' AND entity like '%$';" 2>/dev/null | sed -e '/^$/d' -e 's/ //g'`
		if ! test "$MISNAMEDassets" = ""
		then
			if test "$MISNAMEDassets" != "0"
			then
				NECHO "misnamed assets:" "'$MISNAMEDassets'"; echo "   <== irblacklist table contains entities ending with '$'"
				INCREMENT_EXCEPTIONS
				echo
			fi
		fi
	fi

                echo; NECHO "current time:" ""; date "+%T %Z"; echo

	PSQLquery='SELECT datasources.name, connectorstats.state FROM connectorstats INNER JOIN datasources on connectorstats.id = datasources.id ORDER BY name;'
	psql -d caspidadb -c "$PSQLquery" 2>/dev/null  | egrep ' \| Processing' > $TMP1file

	ACTIVEcount=`cat $TMP1file | wc -l`

	echo
	NECHO "datasources:" "processing: $ACTIVEcount"; echo

	# JOBMANAGERagents=`grep '^jobmanager.agents' /opt/caspida/conf/deployment/caspida-deployment.conf | sed -e 's/^.*=//' -e 's/,/ /g'`
	# for JOBMANAGERagent in $JOBMANAGERagents
	for JOBMANAGERagent in $JOB_AGENT_list
	do
		ssh $JOBMANAGERagent ps -elf
	done |\
	egrep -e '[f]lumeconnector-[0-9]*\.log' > $TMP1file

	FLUMEcount=`cat $TMP1file | wc -l`

	echo
	NECHO "flume:" "$FLUMEcount"; echo

	for JOBMANAGERagent in $JOB_AGENT_list
	do
		ssh -n $JOBMANAGERagent ps -elf
	done |\
	egrep -e '[j]obexecutor-[0-9]*\.log' > $TMP1file

	JOBcount=`cat $TMP1file | wc -l`

	echo
	NECHO "jobexecutor:" "$JOBcount"; echo

	if ! $uba41feature
	then
		for ANALYTICShost in $ANALYTICS_HOST_list
		do
			ssh $ANALYTICShost "egrep -e '.QueryProcessor. INFO  c.c.a.processor.QueryProcessor' /var/log/caspida/analytics.log 2>/dev/null" 
		done |\
		tail -1 > $TMP1file

		ANALYTICScount=`grep "No data to process" $TMP1file | wc -l`	
		echo
		if test "$ANALYTICScount" -eq "1"
		then
			NECHO "analytics:" "No data to process"; echo
		else
			STATUS=`sed -e 's/^.* //' < $TMP1file`
			NECHO "analytics:" "$STATUS"; echo
		fi
	fi

	echo

	> $TMP2file
	if $uba41feature
	then
		REDIScluster=""
		if $uba42feature
		then
			if echo "$PERSISTENCE_SERVER_list" | grep -q ","
			then
				REDIScluster="-c"
				PERSISTENCE_SERVER_list=`echo "$PERSISTENCE_SERVER_list" | cut -d ',' -f 1`	# take the first
			fi
		fi
		EPS=`(exec $RSH $REDIScli $REDIScluster -h $PERSISTENCE_SERVER_list get caspida:eps ) 2>$TMP2file | tr -d '"' | tr -d '\r'`
	else
		EPS=`redis-cli -h $PERSISTENCE_SERVER_list get caspida:eps 2>$TMP1file | tr -d '"'`
	fi

	if test -s $TMP2file
	then
		NECHO "redis error:" "`head -1 $TMP2file`"; echo
		ALL_BUT_FIRST $TMP2file
		echo
	else
		echo
	fi

	MAXeps=`cat $MAXEPSfile 2>/dev/null`

	NECHO "eps:" "'$EPS'"

	if expr "$EPS" + 0 >/dev/null 2>&1
	then
		if test "$EPS" = "0"
		then
			if test "$ACTIVEcount" -gt "0"
			then
				echo "   <== redis reports EPS is 0; verify that $ACTIVEcount datasources are processing properly"
				INCREMENT_EXCEPTIONS 1 39b
			else
				echo
			fi
		else
			if test "$EPS" -gt "$MAXeps" 2>/dev/null
			then
				echo "   <== exceeds $MAXeps maximum for cluster"
				INCREMENT_EXCEPTIONS	
			else
				echo
			fi
		fi
	else
		echo "   <= no value returned from redis '$PERSISTENCE_SERVER_list'"
	fi

	if $uba41feature
	then
        	if $JOB_MANAGER
        	then
			EPSds=`/opt/caspida/bin/status/eps_ds 2>/dev/null | grep "EPS=" | tail -1 | sed -e 's/^.*EPS=//' -e 's/ .*$//'`
			EPSetl=`/opt/caspida/bin/status/eps_etl 2>/dev/null`
			if $uba42feature
			then
				/opt/caspida/bin/status/logs_perf_parser_services identityresolver 1 2>/dev/null > $TMP1file
			else
				/opt/caspida/bin/status/logs_perf_parser_services irtrainer 1 2>/dev/null > $TMP1file
			fi
			EPSir=`grep "EPS=" $TMP1file | sed -e 's/^.*EPS=//' -e 's/ .*$//' | awk '{s += $1 } END {print s}'`	
        	else
			ssh $JOB_MANAGER_host "/opt/caspida/bin/status/eps_ds" > $TMP1file 2>/dev/null
			EPSds=`grep "EPS=" $TMP1file | tail -1 | sed -e 's/^.*EPS=//' -e 's/ .*$//'`
                	EPSetl=`ssh $JOB_MANAGER_host "/opt/caspida/bin/status/eps_etl" 2>/dev/null`
			if $uba42feature
			then
				ssh $JOB_MANAGER_host "/opt/caspida/bin/status/logs_perf_parser_services identityresolver 1" > $TMP1file 2>/dev/null
			else
				ssh $JOB_MANAGER_host "/opt/caspida/bin/status/logs_perf_parser_services irtrainer 1" > $TMP1file 2>/dev/null
			fi
			EPSir=`grep "EPS=" $TMP1file | sed -e 's/^.*EPS=//' -e 's/ .*$//' | awk '{s += $1 } END {print s}'`	
        	fi
		NECHO "  eps_ds:" "$EPSds"
		if test "$EPSds" -gt "$MAXeps" 2>/dev/null
                then
                	echo "   <= exceeds $MAXeps maximum for cluster"
                else
                        echo
                fi
		NECHO "  eps_etl:" "$EPSetl"; echo
		NECHO "  eps_ir:" "$EPSir"; echo
	fi
	
	if false       # not a problem  $uba42feature
	then
		echo
		NECHO "eps_etl (test):" ""

		NODEs=`/opt/caspida/bin/status/nodes.sh | sed -e '/^NAME/d' -e 's/\s\s*/ /g' | cut -d ' ' -f 1`

		/opt/caspida/bin/status/pods.sh | sed -e '/^NAMESPACE/d' -e 's/\s\s*/ /g' | cut -d ' ' -f 2,8 > $TMP1file

		for NODE in $NODEs
		do
			if egrep "^etl.* $NODE" $TMP1file > $TMP2file 
			then
				PODs=`cut -d ' ' -f 1 <$TMP2file | sed -e :a -e '$!N; s/\n/ /; ta'`
				ssh $NODE "for POD in $PODs; do cd /var/log/caspida/containerization/ubaparsers/; tac \${POD}.log 2>/dev/null | grep -m 1 'EPS='; done"
			fi
		done |\
		awk '/INFO/{print $7}'  | cut -d'=' -f2 | awk '{x+=$1} END{print x}'
	fi

	echo
	NODElist="`echo $CLUSTER_NODES_list | tr ',' ' '`"
	for HOST in $NODElist
	do
		ssh -o LogLevel=QUIET -n $HOST "jps -v 2>&1" | sed -s 's/$/\n/' | fmt -s -w 120 > $TMP1file	# fold long lines 
		NECHO "jps  $HOST:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	done | tee $TMP2file 


	if ! $uba41feature
	then
		DISTRIBUTEDparsers=`grep DistributedParser < $TMP2file | wc -l`
		echo
		NECHO "distributed parsers:" "$DISTRIBUTEDparsers"
		if test "$EXPECTEDparsers" != "$DISTRIBUTEDparsers"
		then
			echo "   <== expected $EXPECTEDparsers distributed parsers"
			INCREMENT_EXCEPTIONS 1 39d
		else
			echo
		fi
		ETLactual=`egrep -h 'DistributedParser|CaspidaJobExecutor' $TMP2file | wc -l`
		ETLactual=`expr $ETLactual - $ETLdistributed`
	else
		DISTRIBUTEDparsers=0
		ETLactual=` grep CaspidaJobExecutor $TMP2file | egrep '/jobexecutor-[1-9][0-9]*\.log' | wc -l`
	fi
	
	MAXetl=`cat $MAXETLfile 2>/dev/null`
	if test -z "$MAXetl"
	then
                case "$NUMnodes" in
                        1)      MAXetl=6;;    
                        3)      MAXetl=10;;
                        5)      MAXetl=12;;
                        7)      MAXetl=24;;
                        10)     MAXetl=32;;
                        20)     MAXetl=64;;
		esac
	fi

	if test $ETLactual -gt $MAXetl
	then
		echo
		NECHO "number of etl parsers:" "$ETLactual"
		#	if $uba41feature
		#	then
			#	INDopen="("
			#	INDclose=")"
		#	else
			INDopen="<== "
			INDclose=""
			INCREMENT_EXCEPTIONS 1 39e
		#	fi
		echo "   ${INDopen}exceeds $MAXetl recommended maximum for ${NUMnodes}-node cluster; monitor available memory${INDclose}"
	fi

	if ! $CONTAINERstreaming
	then
		echo
		GET_CASPIDA_PROPERTY system.realtime.workers
		if test -z "$PROPERTYvalue"
		then
			EXPECTED_WORKERcount=4
		else
			EXPECTED_WORKERcount=$PROPERTYvalue
		fi
	
		WORKERcount=`egrep "\bworker\b" < $TMP2file | wc -l`
		NECHO "workers (storm):" "$WORKERcount"
		if test "$WORKERcount" -eq "$EXPECTED_WORKERcount"
		then
			echo
		else
			echo "   <== not the expected number of $EXPECTED_WORKERcount"
			INCREMENT_EXCEPTIONS 1 34
		fi
	fi
	
	echo

	if test -z "$SPARK_WORKER_host_count"
	then
		SPARK_WORKER_host_count=1
	fi


	CHECK_TUNABLES spark-env.sh SPARK_WORKER_INSTANCES $NUMnodes
	if ! test -z "$TUNABLEvalue"
        then
		SPARK_WORKER_instances=$TUNABLEvalue
	else
		SPARK_WORKER_instances=1
	fi

	SPARK_WORKER_count=`expr $SPARK_WORKER_host_count \* $SPARK_WORKER_instances`

	WORKERcount=`egrep "\bWorker\b" < $TMP2file | wc -l`
	NECHO "Workers (spark):" "$WORKERcount"
	if test "$WORKERcount" -eq "$SPARK_WORKER_count"
	then
		echo
	else
		echo "   <== not the expected number of $SPARK_WORKER_count"
                INCREMENT_EXCEPTIONS 1 35
	fi
fi | tee -a  $OUTfile

if $DATABASE_HOST && $uba40feature
then
	echo
	NECHO "resources monitor ..." ""; echo

	if $uba41feature
	then
		IDeps="DS::datasourceEPS"
	else
		IDeps="ETL::datasourceEPS"
	fi
	psql -d caspidadb -t -c "select value  from indicators where id ilike '$IDeps';" 2>/dev/null |\
	sed -e '/^$/d' > $TMP1file
	if test -s $TMP1file
	then
		if python -m json.tool < $TMP1file > $TMP2file 2>/dev/null
		then
			egrep -v -e '\"entityId\": null,' < $TMP2file > $TMP1file
			NECHO "   datasource EPS:" ""; echo
			NECHO "" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
			echo
		fi	
	fi

	psql -d caspidadb -t -c "select id, date_trunc('Seconds', polltime) as polltime, status, value from indicators where not status::text like 'OK' order by 1;" 2>/dev/null |\
	while read LINE
	do
		echo
		echo "$LINE" | cut -d '|' -f 1
		echo "$LINE" | cut -d '|' -f 2-3
		echo "$LINE" | sed -e 's/^[^|][^|]*|[^|][^|]*|[^|][^|]*|\s\s*//' |\
		python -m json.tool 2>/dev/null |\
		egrep -v -e '\"entityId\": null,' 
	done > $TMP1file
	
	if test -s $TMP1file
	then
		if grep -q '"BAD"' $TMP1file
		then
			ERROR="<== check UI system health monitor for errors"
			INCREMENT_EXCEPTIONS
		else
			ERROR=""
		fi
		NECHO "status not OK ..." "`head -1 $TMP1file`"; echo "   $ERROR"
		ALL_BUT_FIRST $TMP1file
	fi

	psql -d caspidadb -c "select jd.job_name, jd.job_group, jd.description, to_timestamp(prev_fire_time/1000) as PrevFired, to_timestamp(next_fire_time/1000) as NextFired from jm_master.qrtz_triggers qt, jm_master.qrtz_job_details jd where jd.job_group ='System' and qt.job_name = jd.job_name;" >$TMP1file 2>/dev/null 
	if test -s $TMP1file
	then
		echo
		NECHO "system jobs:" "`head -1 $TMP1file`"; echo 
		ALL_BUT_FIRST $TMP1file
	fi
fi | tee -a  $OUTfile

if test -f /var/log/caspida/system/threatcomputation.log
then
	zegrep -A 1 -e "^$TODAY .* ERROR |^$YESTERDAY .* ERROR |^$DAYBEFORE .* ERROR " `ls -rt  /var/log/caspida/system/threatcomputation.log* 2>/dev/null` 2>/dev/null |\
	tail -60 > $TMP1file

	if test -s $TMP1file
        then
                echo
                NECHO "threatcomputation err:" "`head -1 $TMP1file`"; echo
                ALL_BUT_FIRST $TMP1file
        fi
fi | tee -a  $OUTfile

if $DATABASE_HOST
then
	if test -s "$CONTRIBlines"
	then
		sort -u < $CONTRIBlines > $TMP1file		# sort unique for datasources that may have been accessed twicw
		echo
		NECHO "contributing SPL:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	fi
fi | tee -a  $OUTfile

if $IMPALA_HOST && $uba41feature
then
	NECHO "model info from impala ..." ""; echo

	ERROR=""
	MODELsql="select modelname, eventtime from (select modelname,eventtime,row_number() over (partition by modelname order by eventtime desc) as index from offlinemodelstats where failurereason = 'n/a') t where index = 1 order by 2 desc;"
	impala-shell -d caspida --quiet -q "$MODELsql" > $TMP1file 2>$TMP2file
	if ! test -s $TMP1file
	then
		cp $TMP2file $TMP1file
		ERROR="<== impala-shell failed"
		INCREMENT_EXCEPTIONS
	fi
	if test -s $TMP1file
	then
		echo
		NECHO "  last successful run:" "`head -1 $TMP1file`   $ERROR"; echo
		ALL_BUT_FIRST $TMP1file
	fi

	MODELsql="select count(*) from (select modelname,eventtime,row_number() over (partition by modelname order by eventtime desc) as index from offlinemodelstats where failurereason = 'n/a') t where index = 1 AND eventtime >= '$LAST_4days';"
	impala-shell -d caspida --quiet -q "$MODELsql" > $TMP1file 2>/dev/null
	if test -s $TMP1file
	then
		echo
		NECHO "  recent success total:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	fi

	MODELsql="select modelname, eventtime from (select modelname,eventtime,row_number() over (partition by modelname order by eventtime desc) as index from offlinemodelstats where failurereason != 'n/a') t where index = 1 order by 2 desc;"
	impala-shell -d caspida --quiet -q "$MODELsql" > $TMP1file 2>/dev/null
	if test -s $TMP1file
	then
		echo
		NECHO "  last failed run:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	fi

	MODELsql="select eventtime,modelname,failurereason,count(*) from offlinemodelstats where failurereason != 'n/a' group by 1,2,3 order by 1 desc limit 10;"
	impala-shell -d caspida --quiet -q "$MODELsql" > $TMP1file 2>/dev/null
	if test -s $TMP1file
	then
		echo
		NECHO "  last 10 failures:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	fi

	MODELsql="select modelname,round(cast(min(durationinmilliseconds/1000) as decimal(15,5)),3) as min_duration_seconds,round(cast(avg(durationinmilliseconds/1000) as decimal(15,5)),3) as avg_duration_seconds,round(cast(max(durationinmilliseconds/1000) as decimal(15,5)),3) as max_duration_seconds from offlinemodelstats where failurereason = 'n/a' group by 1 order by 3 asc; "
	impala-shell -d caspida --quiet -q "$MODELsql" > $TMP1file 2>/dev/null
	if test -s $TMP1file
	then
		echo
		NECHO "  model run times:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	fi
fi | tee -a $OUTfile

if $ANALYTICS_HOST
then
	echo
	if ! $uba41feature
	then
		ANALYTICSlog=/var/log/caspida/analytics.log
		if test -f $ANALYTICSlog
		then
			NECHO "analytics.log:" "`ls -l $ANALYTICSlog`"
			LASTmod=`stat --format='%Y' $ANALYTICSlog`
			MINUTESago=`expr $NOW - 300`
			if test $LASTmod -gt $MINUTESago
			then
				echo
			else
				echo "   <== analytics may be stalled or stopped"
				INCREMENT_EXCEPTIONS
				grep ERR $ANALYTICSlog | tail -1 >$TMP1file
				if test -s $TMP1file
				then
					NECHO "" "`head -1 $TMP1file`": echo
				fi
			fi
			grep -n ERROR $ANALYTICSlog | tail -3 | cut -d ':' -f 1 |\
			while read NUMBER
			do 
				SNIP=`expr $NUMBER + 5`
				sed -n "$NUMBER,$SNIP p" $ANALYTICSlog
				echo
			done > $TMP1file
			if test -s $TMP1file
			then
				echo
				NECHO "last analytics errors:" "`head -1 $TMP1file`": echo
				ALL_BUT_FIRST $TMP1file
			fi
		fi
	fi
fi | tee -a  $OUTfile

if $uba41feature
then
	if ($CONTAINER_MASTER || $CONTAINER_WORKER)
	then
		ps -lf --ppid 1 2>/dev/null | egrep -e 'docker-containerd-shim' |\
		sed -e 's/\s\s\s*/ /g' | cut -d ' ' -f 4 |\
		while read SHIMpid
		do
			ps -lf --ppid $SHIMpid -p $SHIMpid 2>/dev/null
		done > $TMP1file
		if test -s $TMP1file
		then
			SHIMcount=`wc -l < $TMP1file`
			SHIMpages=`sed -e 's/\s\s*/ /g' < $TMP1file | cut -d ' ' -f 10 | egrep -e "[0-9][0-9]*" | awk '{s+=$1} END {print s}'`
			if test $SHIMpages -gt 128000
			then
				ERROR="<= detached docker shims occupy $SHIMpages pages; monitor memory"
			else
				ERROR="(detached docker shims occupy $SHIMpages pages)"
			fi

			echo
			NECHO "detached shim:" "`head -1 $TMP1file`"; echo "   $ERROR"
			ALL_BUT_FIRST $TMP1file
			echo
		fi
		if $LEADERnode && $uba41feature
		then
			NOWsecs=`date '+%s'`
			SOONsecs=`expr $NOWsecs + 1209600`
			echo
			sudo openssl x509 -in  /etc/kubernetes/pki/apiserver.crt -text -noout 2>/dev/null | grep "Not After" | sed -e 's/^\s\s*Not After : //' > $TMP1file
			CERTexpires=`head -1 $TMP1file`
			CERTsecs=`date -d "$CERTexpires" '+%s'`
			if test "$CERTexpires" = ""
			then
				CERTexpires="not available"
			fi
			NECHO "apiserver cert expires:" "$CERTexpires"
			if test $CERTsecs -gt $SOONsecs
			then
				echo
			else
				if test $NOWsecs -ge $CERTsecs
				then
					echo "   <== kubelet cert has expired"
					INCREMENT_EXCEPTIONS
				else
					echo "  <= kubelet cert will soon expire"
				fi
			fi
		fi

		if test -r /var/lib/kubelet/pki/kubelet.crt
		then
			NOWsecs=`date '+%s'`
			SOONsecs=`expr $NOWsecs + 1209600`
			echo
			sudo openssl x509 -in /var/lib/kubelet/pki/kubelet.crt  -text -noout 2>/dev/null | grep "Not After" | sed -e 's/^\s\s*Not After : //' > $TMP1file
			CERTexpires=`head -1 $TMP1file`
			CERTsecs=`date -d "$CERTexpires" '+%s'`
			if test "$CERTexpires" = ""
			then
				CERTexpires="not available"
			fi
			NECHO "kubelet cert expires:" "$CERTexpires"
			if test $CERTsecs -gt $SOONsecs
			then
				echo
			else
				if test $NOWsecs -ge $CERTsecs
				then
					echo "   <== kubelet cert has expired"
					INCREMENT_EXCEPTIONS
				else
					echo "  <= kubelet cert will soon expire"
				fi
			fi
		fi

		echo
		DOCKERregistry="${CONTAINER_MASTER_list}:5000"
		NECHO "docker registry:" "'$DOCKERregistry'"
		curl --silent --show-error --header "Accept: application/vnd.docker.distribution.manifest.v2+json" http://${DOCKERregistry}/v2/ubabase/manifests/latest > $TMP1file 2>$TMP2file
		if grep -q '"mediaType": "application/vnd.docker.distribution.manifest.v2+json",' $TMP1file
		then
			echo
		else
			curl --verbose --silent --show-error --noproxy '*' --header "Accept: application/vnd.docker.distribution.manifest.v2+json" http://${DOCKERregistry}/v2/ubabase/manifests/latest > $TMP1file 2>&1
			if grep -q '"mediaType": "application/vnd.docker.distribution.manifest.v2+json",' $TMP1file
			then 
				echo "   <= docker registry accessible by curl with noproxy; verify  NO_PROXY '$NO_PROXY' or no_proxy '$no_proxy' settings"
			else
				echo "   (docker registry not accessible by curl; noproxy attempt follows ...)"
				sed -ie 's/^/    /' $TMP1file
				NECHO "" "`head -1 $TMP1file`"; echo
				ALL_BUT_FIRST $TMP1file
			fi
		fi

		echo
		NECHO "docker cidr:" "system.docker.networkcidr=$DOCKERcidr"; echo 
 
		if which ifconfig >/dev/null 2>&1
		then
			ifconfig docker > $TMP1file  2>/dev/null
			echo
			NECHO "docker ifconfig:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		fi 

		cat /proc/net/dev 2>/dev/null | egrep -e "lo:|docker0:|flannel|cni|face|Receive" > $TMP1file
		echo
		NECHO "docker network:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file

		ip route > $TMP1file 2>/dev/null
		if test -s $TMP1file
		then
			echo
			NECHO "docker routing:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		fi

		sudo timeout 15 docker images 2>/dev/null > $TMP1file
		UBAimage=`tr -d '\r' < $TMP1file | egrep -m 1 -e '^ubabase' | sed -e 's/\s\s*/ /g' | cut -d ' ' -f 3`

		if test "$UBAimage" != ""
		then
			sudo docker run  $UBAimage stat -c '%A %U %G' /tmp > $TMP1file 2>$TMP2file & 
			sleep 4
			if ps $! >/dev/null 2>&1
			then
				kill -9 $! 2>/dev/null
			fi

			echo
                        NECHO  "docker image /tmp:" "'`tr -d '\r' < $TMP1file`'"
                        if test -s $TMP1file
                        then
                                echo
                                if egrep -q "^drwxrwxrwt" $TMP1file
                                then
                                        echo
                                else
                                        echo "   <== unexpected /tmp permission in $UBAimage; correct and rebuild containers"
                                        INCREMENT_EXCEPTIONS
                                fi
			else
				echo "   <= could not stat in $UBAimage"
				NECHO "" "`head -1 $TMP2file`"; echo
				ALL_BUT_FIRST $TMP2file
                        fi
		fi

		for COMMAND in info images "system df" "stats --no-stream" "ps --no-trunc" 
		do
			sudo timeout 30 docker $COMMAND 2>/dev/null > $TMP1file
			echo
			NECHO "docker $COMMAND:" "`head -1 $TMP1file`"
			if test "$COMMAND" = "images"
			then
				if $CONTAINER_MASTER
				then
					if grep -q  ubabase $TMP1file
					then
						COUNTimages=`wc -l < $TMP1file`
						if test $COUNTimages -le 11
						then
							echo "   <= fewer than expected docker images; re-install may be required"
						else
							echo
						fi	
					else
						echo "   <==  'ubabase' docker image is missing; re-install is required"
						INCREMENT_EXCEPTIONS
						echo >> $TMP1file
						sudo docker image history ubabase >> $TMP1file 2>&1
					fi
				else
					echo
				fi
			else
				echo
			fi
                	ALL_BUT_FIRST $TMP1file
		done

		# show Linux processes

		sudo timeout 30 docker ps --all --format 'table {{.Names}}\t{{.ID}}' 2>/dev/null |\
		grep "_splunkuba_" | grep -v "k8s_POD" |\
		sort |\
		while read LINE
		do
		
        		ID=`echo "$LINE" | sed -e 's/.*\s\(\S\S*$\)/\1/'` 
		
        		PID=`sudo timeout 30 docker inspect --format '{{ .State.Pid }}' $ID 2>/dev/null`

        		echo -n "$LINE    $PID"
        
        		if test "$PID" = ""
        		then
                		echo "   <== container PID not provided by inspection"
				INCREMENT_EXCEPTIONS
        		else
                		if test "$PID" = "0"
                		then
                        		echo "   (invalid PID '$PID'; container may have been restarted)"
                		else
                        		if ps -l $PID > $TMP2file 2>/dev/null
                        		then
                                		echo
                                		fold -s -w 160 < $TMP2file | sed -e 's/^/    /' 
                        		else
                                		echo "   <= PID '$PID' not found; container may have been restarted"
                        		fi
                		fi
        		fi
        
        		echo
		done > $TMP1file

		if test -s $TMP1file
		then
			echo
			NECHO "docker processes:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		fi

		sudo journalctl -n 100 -eu docker | grep -v 'level=info' > $TMP1file 2>/dev/null
		if test -s $TMP1file
		then
			echo
			NECHO "docker journalctl:" "`head -1 $TMP1file`"; echo
                        ALL_BUT_FIRST $TMP1file
		fi

		sudo journalctl -n 33 -eu kubelet > $TMP1file 2>/dev/null
		if test -s $TMP1file
		then
			echo
			NECHO "kubelet journalctl:" "`head -1 $TMP1file`"; echo
                        ALL_BUT_FIRST $TMP1file
		fi


	fi

	if $JOB_MANAGER
	then
                if $uba42feature
                then
                        URL="https://localhost:9002/containers/getClusterConfiguration"
                        TOKEN=`grep '^\s*jobmanager.restServer.auth.user.token=' /opt/caspida/conf/uba-default.properties | cut -d'=' -f2`
                        AUTH="-Ssk -H 'Authorization: Bearer $TOKEN'"
                else
                        URL="http://localhost:9002/containers/getClusterConfiguration"
                        AUTH=""
                fi
                eval curl --connect-timeout 30 $AUTH $URL 2>$TMP2file | tee $TMP3file | python -m json.tool  > $TMP1file 2>/dev/null
	
		echo
		NECHO "jobmanager pods:" ""

		if test -s $TMP1file
		then
			echo "`head -1 $TMP1file`"
			ALL_BUT_FIRST $TMP1file
		else
			cat $TMP3file >> $TMP2file
			echo "`head -1 $TMP2file`  <== jobmanager failed to getClusterConfiguration"
			ALL_BUT_FIRST $TMP2file

			KUBER_REST="$KUBERNETES_RESTSERVER_list"
                        URL="https://${KUBER_REST}/api/v1/pods"
                        TOKEN=`grep "^token=" /etc/caspida/local/conf/containerization/containerization.properties | cut -d '=' -f 2`
                        AUTH="-Ssk -H 'Authorization: Bearer $TOKEN'"
			
                	eval curl --connect-timeout 30 $AUTH $URL 2>$TMP2file | tee $TMP3file |  python -m json.tool  > $TMP1file 2>/dev/null

			if ! test -s $TMP1file
			then
				cat $TMP2file $TMP3file >  $TMP1file
			fi

			if test -s $TMP1file
			then
				echo
				NECHO "kubernetes rest pods:" "`head -1 $TMP1file`"; echo
                        	ALL_BUT_FIRST $TMP1file
			fi
		fi
	fi

	if $KUBERNETES_RESTSERVER
	then
		for GETcommand in nodes namespaces "pods -o wide --all-namespaces"
		do
			echo
			echo " ... get $GETcommand ..." > $TMP2file
			sudo kubectl --kubeconfig /etc/kubernetes/admin.conf get $GETcommand 2>/dev/null > $TMP1file
			if test "$GETcommand" = "pods -o wide --all-namespaces"
			then
				while read LINE
				do 
					set $LINE missing missing missing missing
					ERROR=""
					case "$4" in  
						"Running");;
						"STATUS");;
						"missing");;
						"Init:RunContainerError") ERROR="   <== pod '$2' is '$4'; not 'Running'";;
						*) 	ERROR="   (pod '$2' is '$4'; not 'Running')";;
					esac; 
					echo "${LINE}$ERROR"
				done < $TMP1file >> $TMP2file
			else
				cat $TMP1file >> $TMP2file
			fi
		done

		sed -ie '/splunkuba .* Running *[0-9]\{3,\} / s/$/   \(excessive restarts; container logs should be reviewed\)/' $TMP2file
		
		ERRORcount=`grep ' <== ' $TMP2file | wc -l`
		INCREMENT_EXCEPTIONS $ERRORcount

		NECHO "kubectl:" "`head -1 $TMP2file`"; echo
		ALL_BUT_FIRST $TMP2file 	

		grep "; not 'Running'" $TMP2file |\
		sed -e 's/\s\s*/ /g' -e 's/^ //' |\
		cut -d ' ' -f 1,2,8 |\
		while read NAMESPACE POD NODE
		do
			sudo kubectl --kubeconfig /etc/kubernetes/admin.conf describe pods $POD --namespace $NAMESPACE > $TMP1file 2>&1
			if test -s $TMP1file
			then
				echo >> $TMP1file
				echo "${POD}.log.out ..." >> $TMP1file
				echo >> $TMP1file
				ssh -n $NODE tail -50 /var/log/caspida/containerization/*/${POD}.log.out >> $TMP1file 2>/dev/null
				echo >> $TMP1file
				echo "${POD}.log ..." >> $TMP1file
				echo >> $TMP1file
				ssh -n $NODE tail -50 /var/log/caspida/containerization/*/${POD}.log >> $TMP1file 2>/dev/null
				echo
				NECHO "kubectl:" " ... describe pods $POD (not running) ..."; echo
				NECHO "" "`head -1 $TMP1file`"; echo
				ALL_BUT_FIRST $TMP1file
				echo
			fi
		done

		grep '(excessive restarts' $TMP2file |\
		sed -e 's/\s\s*/ /g' -e 's/^ //' |\
		cut -d ' ' -f 1,2,8 |\
		while read NAMESPACE POD NODE
		do
			sudo kubectl --kubeconfig /etc/kubernetes/admin.conf describe pods $POD --namespace $NAMESPACE > $TMP1file 2>&1
			if test -s $TMP1file
			then
				echo >> $TMP1file
				echo "${POD}.log.out ..." >> $TMP1file
				echo >> $TMP1file
				ssh -n $NODE tail -50 /var/log/caspida/containerization/*/${POD}.log.out >> $TMP1file 2>/dev/null
				echo >> $TMP1file
				echo "${POD}.log ..." >> $TMP1file
				echo >> $TMP1file
				ssh -n $NODE tail -50 /var/log/caspida/containerization/*/${POD}.log >> $TMP1file 2>/dev/null
				echo
				NECHO "kubectl:" " ... describe pods $POD (excessive restarts) ..."; echo
				NECHO "" "`head -1 $TMP1file`"; echo
				ALL_BUT_FIRST $TMP1file
				echo
			fi
		done

		sudo kubectl --kubeconfig /etc/kubernetes/admin.conf get pods --namespace kube-system 2>/dev/null | egrep -e "^kube-dns|^kube-flannel" | cut -d ' ' -f 1 |\
		while read KUBEpod
		do
			if test "$KUBEpod" != ""
			then
				sudo kubectl --kubeconfig /etc/kubernetes/admin.conf describe pods $KUBEpod --namespace kube-system > $TMP1file 2>/dev/null
				if test -s $TMP1file
				then
					echo
					NECHO "kubectl:" " ... describe pods $KUBEpod ..."; echo
					NECHO "" "`head -1 $TMP1file`"; echo
                        		ALL_BUT_FIRST $TMP1file
				fi
			fi
		done


		ANALYTICSpod=`sudo kubectl --kubeconfig /etc/kubernetes/admin.conf get pods -n splunkuba 2>/dev/null | grep -m 1 analyticsaggregator | cut -d ' ' -f 1`

		CONTAINERmaster="${CONTAINER_MASTER_list}"

		SECURITYjar=/opt/caspida/lib/CaspidaSecurity.jar

		if test "$ANALYTICSpod" != ""
		then
			sudo kubectl --kubeconfig /etc/kubernetes/admin.conf exec -it  $ANALYTICSpod  -n splunkuba -- stat -c '%A %U %G' /tmp 2>/dev/null |\
			tr -d '\r' > $TMP1file
		
			echo
			NECHO  "container /tmp:" "'`cat $TMP1file`'"
			if test -s $TMP1file
			then
				if egrep -q "drwxrwxrwt" $TMP1file
				then
					echo "   ($ANALYTICSpod)"
				else
					echo "   <== unexpected /tmp permission in $ANALYTICSpod; correct and rebuild containers"
					INCREMENT_EXCEPTIONS
				fi
			else
				echo "   <= could not stat in $ANALYTICSpod" 	
			fi

			sudo kubectl --kubeconfig /etc/kubernetes/admin.conf exec -it  $ANALYTICSpod  -n splunkuba -- egrep "^hosts:" /etc/nsswitch.conf 2>/dev/null |\
			tr -d '\r' > $TMP1file
			echo >> $TMP1file
			sudo kubectl --kubeconfig /etc/kubernetes/admin.conf exec -it  $ANALYTICSpod  -n splunkuba -- getent hosts $CONTAINERmaster 2>/dev/null |\
			tr -d '\r' >> $TMP1file

			echo
			NECHO "container nsswitch:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file

			sudo kubectl --kubeconfig /etc/kubernetes/admin.conf exec -it  $ANALYTICSpod  -n splunkuba -- java -XX:+PrintFlagsFinal -version 2>/dev/null |\
			tr -d '\r' > $TMP1file
			if test -s $TMP1file
			then
				egrep -e ' MaxHeapSize | ParallelGCThreads | NewRatio |bool Use.*GC | NewSize | OldSize | InitialHeapSize | MaxNewSize | CICompilerCount ' < $TMP1file > $TMP2file
				echo
				NECHO "default java flags:" "`head -1 $TMP2file`"; echo
				ALL_BUT_FIRST $TMP2file
			fi

			sudo kubectl --kubeconfig /etc/kubernetes/admin.conf exec -it  $ANALYTICSpod  -n splunkuba -- jcmd 1 VM.flags -all 2>/dev/null |\
			tr -d '\r' > $TMP1file
			if test -s $TMP1file
			then
				egrep -e ' MaxHeapSize | ParallelGCThreads | NewRatio |bool Use.*GC | NewSize | OldSize | InitialHeapSize | MaxNewSize | CICompilerCount ' < $TMP1file > $TMP2file
				echo
				NECHO "container java flags:" "`head -1 $TMP2file`"; echo
				ALL_BUT_FIRST $TMP2file
			fi

			sudo kubectl --kubeconfig /etc/kubernetes/admin.conf exec -it  $ANALYTICSpod  -n splunkuba -- jps -lvm 2>/dev/null |\
			tr -d '\r' > $TMP1file
			if test -s $TMP1file
			then
				grep '^1 ' $TMP1file | sed -e 's/ /\&nbsp;/g' -e 's/&nbsp;-/ -/g' | fold -s -w 160 | sed -e 's/\&nbsp;/ /g' > $TMP2file
				echo
				NECHO "container jps:" "`head -1 $TMP2file`"; echo
				ALL_BUT_FIRST $TMP2file
			fi

			sudo kubectl --kubeconfig /etc/kubernetes/admin.conf exec -it  $ANALYTICSpod  -n splunkuba -- ps -Tew -o uid,pid,spid,c,sz,rss,psr,pcpu,stime,tty,time,cmd  2>/dev/null |\
			tr -d '\r' > $TMP1file
			if test -s $TMP1file
			then
				fold -s -w 120 $TMP1file > $TMP2file
				echo
				NECHO "container ps:" "`head -1 $TMP2file`"; echo
				ALL_BUT_FIRST $TMP2file
			fi

			echo
			sudo kubectl --kubeconfig /etc/kubernetes/admin.conf exec -it $ANALYTICSpod -n splunkuba --  stat -c '%A %U %G %s %y' $SECURITYjar 2>$TMP2file |\
			tr -d '\r' > $TMP1file
                        if test -s $TMP1file
                        then
				NECHO "container Caspida jar:" "`head -1 $TMP1file`"; echo
				stat -c '%A %U %G %s %y' $SECURITYjar > $TMP2file 2>/dev/null
				SIZEcontainer=`cut -d ' ' -f 4 < $TMP1file`
				SIZElinux=`cut -d ' ' -f 4 < $TMP2file`
				if test "$SIZEcontainer" != "$SIZElinux"
				then
					if ! diff -y $TMP1file $TMP2file >  $TMP3file 2>/dev/null
					then
						NECHO "container Caspida jar:" "`sed -e 's/\s\s*/ /g' <$TMP3file`" 
						echo "   <== stat mismatch in $ANALYTICSpod; correct CaspidaSecurity.jar and rebuild containers"
						INCREMENT_EXCEPTIONS
					else
						NECHO "container Caspida jar:" "(stat match)"; echo
					fi
				else
					sudo kubectl --kubeconfig /etc/kubernetes/admin.conf exec -it $ANALYTICSpod -n splunkuba -- md5sum $SECURITYjar  2>/dev/null |\
	 				tr -d '\r'> $TMP1file
					if test -s $TMP1file
					then
						if md5sum -c $TMP1file  >$TMP2file 2>&1
						then
							NECHO "container Caspida jar:" "`head -1 $TMP1file`"; echo "   (md5 match)"
						else
							NECHO "container Caspida jar:" "`head -1 $TMP2file`"
							echo "   <= md5 mismatch in $ANALYTICSpod; remove/setup containerization advised"
							ALL_BUT_FIRST $TMP2file
						fi
					fi
				fi
			else
				NECHO "container Caspida jar:" "`head -1 $TMP2file`"; echo "   <= stat failed"
				ALL_BUT_FIRST $TMP2file
			fi
		fi
	fi
fi | tee -a  $OUTfile	

if $HADOOP_NAMENODE
then
	sudo -u hdfs hdfs fsck / 2>/dev/null | sed -n '/^Status:/,$ p' > $TMP1file
	if test -s $TMP1file
	then
		echo
		NECHO "hdfs fsck:" "`head -1 $TMP1file`"
		if grep -qi "healthy" $TMP1file
		then
			echo
		else
			echo "   <== issues with hdfs filesystem need to be addressed"
			INCREMENT_EXCEPTIONS
		fi
		ALL_BUT_FIRST $TMP1file
	fi
	echo
	if hdfs dfsadmin -report 2>/dev/null > $TMP1file
	then
		: # success
	else
		sudo -u hdfs hdfs dfsadmin -report 2>&1 > $TMP1file
	fi
	EXPECTEDcount=`echo $HADOOP_DATANODE_list | tr ',' ' ' | wc -w`
	LIVE="Live datanodes ($EXPECTEDcount)"
	if grep -q "$LIVE" $TMP1file
	then
		ERROR=""
	else
		ERROR="   <== expected $EXPECTEDcount live datanodes '$HADOOP_DATANODE_list'"
		INCREMENT_EXCEPTIONS 1 39
		sed -i -e "/Live datanodes/ s/$/$ERROR/"  $TMP1file
	fi

	if grep -q "Safe mode is ON" $TMP1file
	then
		ERROR="   <== Safe Mode unexpected; fsck required"
		INCREMENT_EXCEPTIONS 1 39a
		sed -i -e "/Safe mode is ON/ s/$/$ERROR/" $TMP1file
	fi

	DFSused=`grep 'DFS Used%:' $TMP1file | head -1 | cut -d ':' -f 2 | tr -d ' .%'`

	HIGHuse=false

	if expr "$DFSused" + 0 >/dev/null 2>&1
	then
		if test $DFSused -le 7000
		then
			:
		else
			HIGHuse=true
			if test $DFSused -gt 8500
			then
				ERROR="  <== exceeds 85%; more space may be needed"
				INCREMENT_EXCEPTIONS
			else
				ERROR="   <= exceeds 70%; monitor space"
			fi
			LINEnum=`grep -n 'DFS Used%:' $TMP1file | head -1 | cut -d ':' -f 1`
			sed -i -e "$LINEnum s/$/$ERROR/"  $TMP1file
		fi
	fi
			

	echo "five largest directories:" >> $TMP1file
	hadoop fs -du -s -h /user/caspida/analytics/caspida.db/* 2>/dev/null |\
	sed -e 's/ \([TGMK]\) /\1 /g' | sort -rh  | head -5 >> $TMP1file
	echo  >> $TMP1file
	echo "total:" >> $TMP1file
	hadoop fs -du -s -h /user/caspida/analytics/caspida.db 2>/dev/null >> $TMP1file


	ERROR=""
	if test -s $TMP1file
	then
		grep "DFS Used%:" $TMP1file | sed -e 's/[% \.]//g' -e 's/^[^:]*://' > $TMP2file

		if test -s $TMP2file
		then
	
			DFSmax=`head -1 $TMP2file`
			DFSmax=`expr $DFSmax + $DFSmax / 12 2>/dev/null`	# adjusting to 8% instead on default 10%

			DFSbalanced=true

			while read USEDperc
			do
				if test "$USEDperc" -gt "$DFSmax" 2>/dev/null
				then
					DFSbalanced=false
					break
				fi
			done < $TMP2file

			if $DFSbalanced
			then
				ERROR="   (balanced)"
			else
				if $HIGHuse
				then
					ERROR="   <= HDFS may need to be balanced"
				else
					ERROR="   (HDFS balance should be monitored)"
				fi
			fi
		fi
	fi

	NECHO "hdfs dfsadmin:"  "`head -1 $TMP1file`"; echo "$ERROR"
        ALL_BUT_FIRST $TMP1file

	echo
	ERROR=""
	for DAYSpast in 0 1 2 3 4 5 6 7 8 9 10 11 12 13
	do 
		set `date -d "-$DAYSpast days" '+%Y %m %d'`
		YYYY=$1
		MONTH=$2
		DAY=$3
		echo -n "$YYYY $MONTH $DAY: " 
		if $uba42feature
		then
			hadoop fs -ls hdfs:/user/caspida/analytics/caspida.db/semiaggr_s/tspartition=${YYYY}${MONTH}${DAY}/* 2>/dev/null | wc -l | tee $TMP2file
		else
			hadoop fs -ls hdfs:/user/caspida/analytics/caspida.db/semiaggr_s/eventyear=$YYYY/eventmonth=$MONTH/eventday=$DAY/* 2>/dev/null | wc -l | tee $TMP2file
		fi
		if egrep -v -q -e " *0\$" $TMP2file
		then
			break
		fi
	done > $TMP1file

	sed -ie '/: *0/ s/$/   <= hdfs aggregation expected/' $TMP1file
	LINEcount=`wc -l < $TMP1file`
	if test $LINEcount -eq 14
	then
		ERROR="   <== no hdfs aggregation in past two weeks"
		INCREMENT_EXCEPTIONS
	else
		if test $LINEcount -gt 2
		then
			ERROR="   <== no hdfs aggregation today"
			INCREMENT_EXCEPTIONS
		fi
	fi
	NECHO "hdfs aggregation:" "`head -1 $TMP1file`" ;echo "$ERROR"
        ALL_BUT_FIRST $TMP1file
fi | tee -a  $OUTfile

if $HIVE_HOST || $SPARK_MASTER || $SPARK_WORKER
then
	echo
	hive -e 'set' 2>/dev/null | grep "^hive.metastore.uris" > $TMP1file
	NECHO "hive uris:" "`head -1 $TMP1file`"
	if egrep -qie "localhost|127\.0\.0\.1" $TMP1file
	then
		if $MULTInode
		then
			echo "   <== 'localhost' unexpected; hive is not configured properly"
			INCREMENT_EXCEPTIONS
		else
			echo
		fi
	else
		echo
	fi

	if $uba41feature
	then
		hive -e 'show databases;' 2> $TMP2file > $TMP1file
		sed -e '/^\s*$/d' <  $TMP1file >> $TMP2file
		NECHO "hive databases:" "`head -1 $TMP2file`"; echo
		ALL_BUT_FIRST $TMP2file
	fi
fi | tee -a  $OUTfile

if $PERSISTENCE_SERVER
then
	echo
	NECHO "redis servers (uba-site):" "$PERSISTENCE_SERVER_list"; echo
	echo
	redis-cli -h localhost cluster nodes > $TMP1file 2>&1
	NECHO "redis nodes (localhost):" "`head -1 $TMP1file`"; echo
	ALL_BUT_FIRST $TMP1file
	echo
	redis-cli -h localhost config get maxclients > $TMP1file 2>&1
	NECHO "redis maxclients (localhost):" "`head -1 $TMP1file`"; echo
	ALL_BUT_FIRST $TMP1file
	echo
        redis-cli -h localhost info | sed -e 's/\r$//' > $TMP1file 2>&1
        NECHO "redis info (localhost):" "`head -1 $TMP1file`"; echo
        ALL_BUT_FIRST $TMP1file


	> $TMP2file
	if $uba41feature
	then
		REDIScluster=""
		if $uba42feature
		then
			if echo "$PERSISTENCE_SERVER_list" | grep -q ","
			then
				REDIScluster="-c"
				PERSISTENCE_SERVER_list=`echo "$PERSISTENCE_SERVER_list" | cut -d ',' -f 1`	# take the first
			fi
		fi
		(exec $RSH $REDIScli $REDIScluster -h $PERSISTENCE_SERVER_list -r 1 cluster info) 2>$TMP2file | tr -d '"' | tr -d '\r' > $TMP1file
	else
		redis-cli -h $PERSISTENCE_SERVER_list -r 1 cluster info  2>$TMP2file | tr -d '"' > $TMP1file
	fi
	cat $TMP2file >> $TMP1file

	echo
	NECHO "redis cluster ($PERSISTENCE_SERVER_list):" "`head -1 $TMP1file`"; echo
	ALL_BUT_FIRST $TMP1file


fi | tee -a  $OUTfile

if $PERSISTENCE_GRAPHDB_SERVER
then
	echo
	NEO4Jbin=/usr/bin/
	if test -x $NEO4Jbin/neo4j-shell 2>/dev/null
	then
		:
	else
		NEO4Jbin=`grep '^exec' /etc/init.d/neo4j 2> /dev/null | sed -e 's/^exec="//' -e 's/ .*$//'`
		NEO4Jbin=`dirname "$NEO4Jbin"`
	fi
	if $NEO4Jbin/neo4j-shell -c 'index --indexes' > $TMP1file 2>&1
	then
		NECHO "neo4j indexes:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	else
		NECHO "neo4j indexes:" "   <== $NEO4Jbin/neo4j-shell not found"; echo
		INCREMENT_EXCEPTIONS 1 40
	fi
fi | tee -a  $OUTfile

if $HBASE_MASTER
then
	echo
	echo "status 'simple'" | hbase shell -n 2>&1 | fold -s -w 80 > $TMP1file
	NECHO "hbase status:" "`head -1 $TMP1file`"; echo
	ALL_BUT_FIRST $TMP1file
fi | tee -a  $OUTfile

if $KAFKA_BROKER
then
	echo
	ZOOKEEPERhost=$ZOOKEEPER_SERVER_list
	if test "$ZOOKEEPERhost" = ""
	then
		ZOOKEEPERhost="localhost"
	fi
	/usr/share/kafka/bin/kafka-topics.sh --zookeeper ${ZOOKEEPERhost}:2181 --describe > $TMP1file 2>&1

	NECHO "kafka topics:" "`head -1 $TMP1file`"; echo
	ALL_BUT_FIRST $TMP1file
fi | tee -a  $OUTfile

if $STORM_SUPERVISOR
then
	echo
	storm list 2>&1 | sed -e 's/,/, /g' |  fold -s -w 80 | sed -e 's/, /,/g' > $TMP1file
	NECHO "storm list:" "`head -1 $TMP1file`"; echo
	ALL_BUT_FIRST $TMP1file
fi | tee -a  $OUTfile

if $STORM_UI_HOST
then
	if ! $uba41feature
	then
		echo
		MASTER=$STORM_NIMBUS_HOST_list

		if $uba42feature
        	then
			URL="https://${MASTER}:8090/api/v1/topology/summary"
                	TOKEN=`grep '^\s*jobmanager.restServer.auth.user.token=' /opt/caspida/conf/uba-default.properties | cut -d'=' -f2`
                	AUTH="-Ssk -H 'Authorization: Bearer $TOKEN'"
        	else
                	URL="http://${MASTER}:8090/api/v1/topology/summary"
                	AUTH=""
        	fi
		eval curl --connect-timeout 60 $AUTH $URL -4 > $TMP1file 2>$TMP2file
		if python -m json.tool < $TMP1file >$TMP3file 2>/dev/null 
		then
			ERROR=""
		else
			ERROR="   <== failed to connect"
			INCREMENT_EXCEPTIONS 1 60
		fi		
		echo
		echo >> $TMP3file
		cat $TMP2file >> $TMP3file
		NECHO "storm topology $MASTER:" "`head -1 $TMP3file`$ERROR"; echo 
        	ALL_BUT_FIRST $TMP3file
	fi
fi | tee -a $OUTfile


(
	cd /var/log/caspida

	find . -type f -mtime -1 -name 'jobexecutor-*' -exec grep -i "Sending ZK notification to update SplunkHRData Cache" {} \; | sort | tail -5 > $TMP1file

	find . -type f -mtime -1 -name 'jobexecutor-*' |\
	sed  ':a; N; s/\n/ /; ta' > $TMP2file

	LOGfiles=`cat $TMP2file`

	while read LOGentry 
	do
		grep -H "$LOGentry" $LOGfiles 
	done < $TMP1file > $TMP2file

	if test -s $TMP2file
	then
		echo
		NECHO "zk hrdata notification:" "`head -1 $TMP2file`"; echo
		ALL_BUT_FIRST $TMP2file
	fi

) | tee -a $OUTfile	

if $ZOOKEEPER_SERVER
then
	eval ls -1 /var/log/zookeeper/zookeeper.log /var/log/zookeeper/zookeeper.log.? /var/log/zookeeper/zookeeper.log.?? > $TMP1file 2>/dev/null
	while read LOG 
	do
		if test -r "$LOG"
		then
			if grep  -m 1 -hi 'starting server' "$LOG" 2>/dev/null
			then
				break
			fi
		fi
	done > $TMP2file < $TMP1file
	if test -s $TMP2file
	then
		ZKstart=`head -1 $TMP2file | sed -e 's/ \[.*$//'`
		ZKsecs=`date --date="$ZKstart" +'%s' 2>/dev/null`
		echo
		NECHO "zookeeper start:" "$ZKstart - $ZKsecs   ($THISnode)"; echo
	fi

	echo "stat /caspida/watchers/hrdata" | zookeeper-client 2>&1 | egrep "^mtime|^dataVersion" > $TMP1file
	if test -s $TMP1file
	then
		echo
		NECHO "zk hrdata_watcher" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	fi

	ZOOKEEPERleader=false
	for FLW in ruok stats cons mntr
	do
		if $ZOOKEEPERleader
		then
			if test $FLW = "stats"
			then
				continue
			fi
		else
			# follower
			if test $FLW = "cons"
			then
				continue
			fi
		fi

		echo
		echo $FLW |nc localhost 2181 2>&1 | sed -e 's/,/, /g' | fold -s -w 80 | sed -e 's/, /,/g' > $TMP1file
		ERROR=""
		if test $FLW = "mntr"
		then
			if egrep -q "zk_server_state	leader|zk_server_state	standalone" $TMP1file
			then
				ZOOKEEPERleader=true
			else
				ZOOKEEPERleader=false
			fi
			AVGlatency=`grep zk_avg_latency $TMP1file | sed -e 's/^.*zk_avg_latency\s*//'`
			if expr "$AVGlatency" + 0 >/dev/null 2>&1
			then
				if test $AVGlatency -gt 10 
				then
					sed -i -e '/zk_avg_latency/ s/$/   <= average latency has exceeded 10ms threshold; zookeeper monitoring advised/'  $TMP1file 
				fi
			fi
			MAXlatency=`grep zk_max_latency $TMP1file | sed -e 's/^.*zk_max_latency\s*//'`
 			if expr "$MAXlatency" + 0 >/dev/null 2>&1
 			then
 				if test $MAXlatency -gt 1500
 				then
 					sed -i -e '/zk_max_latency/ s/$/   (high water mark)/' $TMP1file 
 					# echo srst | nc localhost 2181 >/dev/null 2>&1
 				fi
 			fi
			OUTrequests=`grep zk_outstanding_requests  $TMP1file | sed -e 's/^.*zk_outstanding_requests\s*//'`
			if expr "$OUTrequests" + 0 >/dev/null 2>&1
                        then
                                if test $OUTrequests -gt 10
                                then
                                        sed -i -e '/zk_outstanding_requests/ s/$/   <= outstanding requests has exceeded 10; zookeeper monitoring advised/' $TMP1file 
                                fi
                        fi
			if $ZOOKEEPERleader
			then
				PENDINGsyncs=`grep zk_pending_syncs  $TMP1file | sed -e 's/^.*zk_outstanding_requests\s*//'`
				if expr "$PENDINGsyncs" + 0 >/dev/null 2>&1
                        	then
                                	if test $PENDINGsyncs -gt 10
                                	then
                                        	sed -i -e '/zk_pending_syncs/ s/$/   <= pending syncs has exceeded 10; zookeeper monitoring advised/' $TMP1file 
                                	fi
                        	fi
			fi
		fi
		NECHO "zookeeper $FLW:" "`head -1 $TMP1file`"; echo 
		ALL_BUT_FIRST $TMP1file
	done

	grep -A 3 ERROR /var/log/zookeeper/zookeeper.log 2>/dev/null | tail -12 > $TMP1file	
	if test -s $TMP1file
	then
		echo
		NECHO "zookeeper last errors:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	fi

	for PORT in 2181 2888 3888
	do
		sudo netstat -antp 2>/dev/null | egrep -e ":${PORT}\s"
	done > $TMP1file
	if test -s $TMP1file
	then
		echo
		NECHO "zookeeper ports:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	fi

	sudo lsof -n -P 2>/dev/null 2>/dev/null | egrep -e ":2181\s|:2888\s|:3888\s" | egrep -ve "zookeeper|caspida" > $TMP1file
	if test -s $TMP1file
	then
		echo
		NECHO "zookeeper port usage:" "`head -1 $TMP1file`"; echo "   <== unexpected user of zookeeper ports"
		INCREMENT_EXCEPTIONS
		ALL_BUT_FIRST $TMP1file
	fi
	

	if ! $uba32feature
	then
		for TSDB in $PERSISTENCE_DATASTORE_list
		do
			if $uba42feature
        		then
				URL="https://${TSDB}/api/version"
                		TOKEN=`grep '^\s*jobmanager.restServer.auth.user.token=' /opt/caspida/conf/uba-default.properties | cut -d'=' -f2`
                		AUTH="-Ssk -H 'Authorization: Bearer $TOKEN'"
        		else
                		URL="http://${TSDB}/api/version"
                		AUTH=""
        		fi
			eval curl --connect-timeout 30 $AUTH $URL -4 > $TMP1file 2>$TMP2file
			if python -m json.tool < $TMP1file >$TMP3file 2>/dev/null
			then
				ERROR=""
			else
				ERROR="   <== failed to connect"
				cat $TMP2file >> $TMP3file
				INCREMENT_EXCEPTIONS 1 60
			fi		
			echo
			NECHO "tsdb $TSDB:" "`head -1 $TMP3file`$ERROR"; echo 
                	ALL_BUT_FIRST $TMP3file
		done
	fi

	if which influx >/dev/null 2>&1
	then
		for TSDB in $PERSISTENCE_DATASTORE_list
		do
			TSDBhost=`echo "$TSDB" | cut -d ':' -f 1`	
			TSDBport=`echo "$TSDB" | cut -d ':' -f 2`
			if $PERSISTENCE_DATASTORE
			then
				COMMAND="show diagnostics; show stats for 'httpd'; show stats for 'database'; show retention policies on caspida; show shards"
			else
				COMMAND="show diagnostics"
			fi
			influx -host $TSDBhost -port $TSDBport -database caspida -execute "$COMMAND" > $TMP1file 2>&1
			echo
			NECHO "influxdb $TSDBhost:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		done
	fi
fi | tee -a  $OUTfile

if $SPARK_MASTER || $SPARK_WORKER
then
	/var/vcap/packages/spark/bin/spark-submit --version 2>&1 | grep version > $TMP1file
	if test -s $TMP1file
	then
		echo
		NECHO "spark version:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	fi
fi | tee -a  $OUTfile
	
if $SPARK_MASTER
then
	echo
	GET_CASPIDA_PROPERTY offline.workflow.cores
        OFFLINEcores=$PROPERTYvalue 

	ps -elf | egrep -e "[s]park.deploy.master.Master" | fmt -s -w 120 > $TMP2file           # is spark running on this node
	if test -s $TMP2file
	then
		ERROR=""
	else
		ERROR="   <== spark master not running"
		INCREMENT_EXCEPTIONS
	fi
	NECHO "spark master:"  "`head -1 $TMP2file`"; echo $ERROR 
	ALL_BUT_FIRST $TMP2file
	echo
	
	SPARK_MASTER_HOST=`echo "$SPARK_MASTER_list" | cut -d ":" -f 1`
	if test "$SPARK_MASTER_HOST" = ""
	then
		SPARK_MASTER_HOST="localhost"
	fi

	URL="http://${SPARK_MASTER_HOST}:8080"
        if eval curl --connect-timeout 30 -o $TMP2file $URL 2>$TMP1file
        then
		TOTALcores=`grep -i "Cores in use:" $TMP2file | sed -e 's/^.*> //' -e 's/ Total.*$//'`
		NECHO "spark total cores:" "'$TOTALcores'"
		if test "$TOTALcores" -ge "$OFFLINEcores"
		then
			echo
		else
			echo "   <== expecting '$OFFLINEcores'"
			INCREMENT_EXCEPTIONS 1 42
		fi
	else
		fmt -s -w 120 < $TMP1file > $TMP2file
		NECHO "spark total cores:" "`head -1 $TMP2file`"; echo "   <== failed to connect"
		ALL_BUT_FIRST $TMP2file
		INCREMENT_EXCEPTIONS 1 42b
	fi

	if eval curl --connect-timeout 30 -o $TMP1file http://${SPARK_MASTER_HOST}:8080/api/v1/applications 2>/dev/null
	then
		if which w3m >/dev/null 2>&1
		then
			w3m -T text/html -dump -cols 220 < $TMP1file 2>/dev/null
		else
			sed -e 's/>/>\n/g' -e 's/</\n</g' < $TMP1file |\
			sed  -e '/^\s*$/d' |\
			sed -e '/<div/d' -e '/<\/div/d' |\
			sed -e '/<a/d' -e '/<\/a/d' |\
			sed -e '/<span/d' -e '/<\/span/d' |\
			sed -e '/<strong/d' -e '/<\/strong/d' |\
			sed -e '/<script/d' -e '/<\/script/d' |\
			sed -e '/<link/d' -e '/<\/link/d' |\
			sed -e '/<title/d' -e '/<\/title/d' |\
			sed -e '/<html/d' -e '/<\/html/d' |\
			sed -e '/<body/d' -e '/<\/body/d' |\
			sed -e '/<meta/d' -e '/<\/meta/d' |\
			sed -e '/<h3/d' -e '/<\/h3/d' |\
			sed -e '/<img/d' -e '/<\/img/d' |\
			sed -e '/<ul/d' -e '/<\/ul/d' |\
			sed  -e '/<table/d'  -e '/<\/table/d' |\
			sed -e '/<tbody/d' -e '/<\/tbody/d' |\
			sed -e '/<td/d' -e 's/<\/td>/|/' |\
			sed -e '/<th /d' -e 's/<\/th>/|/' |\
			sed -e '/<\!DOCTYPE/d' |\
			sed -e 's/\s\s*/ /'  |\
			awk 'BEGIN {line=""} /<\// {print line; next;} /</ {line=""; next;} {line=line $0;}' |\
			sed -e 's/^ //' -e 's/| /|/g' -e 's/|$//' |\
			awk 'BEGIN {FS="|";} NF==1 {print $0; next;} {spaces="    "; for (i=1; i<=NF; i++) {if (i == 1) {print spaces $1 "    " $2; i++;} else printf("%s%s", spaces, $i); spaces=spaces "   ";}; printf("\n");}'
		fi > $TMP2file
		if test -s $TMP2file
        	then
        		echo
        		NECHO "spark current state:" "`head -1 $TMP2file`"; echo
        		ALL_BUT_FIRST $TMP2file
		fi
	else
		NECHO "spark current state:" "timeout   <== no response from Spark Master"; echo
		INCREMENT_EXCEPTIONS 
        fi

	if test "$DATABASE_HOST_list" = ""
	then
		DATABASE_HOST_list="localhost"
	fi
	ssh $DATABASE_HOST_list 'psql -d caspidadb -c "select host,pid,start,update from sparkserverregistry;"' 2>/dev/null | egrep -ve ' rows*\)$' > $TMP1file
	if test -s $TMP1file
	then
		echo
		NECHO "spark registry:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	fi

fi | tee -a  $OUTfile

if $SPARK_WORKER
then
	echo
	jps | grep Worker |\
	cut -d ' ' -f 1 > $TMP1file
	if test -s $TMP1file
	then
		ps -l `cat $TMP1file` 2>/dev/null |  fmt -s -w 120 > $TMP2file
		NECHO "spark workers:" "`head -1 $TMP2file`"; echo
		ALL_BUT_FIRST $TMP2file
	fi
fi | tee -a  $OUTfile

if test -f  /var/log/caspida/spark/uba-only/spark-caspida-local.log
then
	egrep "Initializing model|Completed|failureReason" /var/log/caspida/spark/uba-only/spark-caspida-local.log | grep -v "n/a" | tail -80 > $TMP1file
	if test -s $TMP1file
	then
		echo
		NECHO "spark local log:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	fi
fi | tee -a  $OUTfile

if test -f /var/log/caspida/spark/spark-server.log 
then
        zegrep -A 4 -e "^$TODAY .* ERROR |^$YESTERDAY .* ERROR |^$DAYBEFORE .* ERROR " `ls -rt  /var/log/caspida/spark/spark-server.log * 2>/dev/null` 2>/dev/null |\
        tail -40 > $TMP1file

        if test -s $TMP1file
        then
                echo
                NECHO "spark server err:" "`head -1 $TMP1file`"; echo
                ALL_BUT_FIRST $TMP1file
        fi
fi | tee -a  $OUTfile

if test -f /var/log/caspida/spark/uba-only/spark-caspida-local.log
then
        zegrep -A 2 -e "^$TODAY .* ERROR |^$YESTERDAY .* ERROR |^$DAYBEFORE .* ERROR " `ls -rt  /var/log/caspida/spark/uba-only/spark-caspida-local.log* 2>/dev/null` 2>/dev/null |\
        tail -40 > $TMP1file

        if test -s $TMP1file
        then
                echo
                NECHO "spark caspida err:" "`head -1 $TMP1file`"; echo
                ALL_BUT_FIRST $TMP1file
        fi
fi | tee -a  $OUTfile

if $DATABASE_HOST
then
	psql -d caspidadb -c 'select table_name,total_size from v_pg_stats;' 2>/dev/null | egrep -i -e "[8-9][0-9][0-9][0-9] mb| gb|total_size|------" > $TMP1file
	LINEcount=`wc -l < $TMP1file`
	if test $LINEcount -gt 4 
	then
		echo
		NECHO "database table size:" "`head -1 $TMP1file`"; echo
		sed -e '3,3 s/$/   <== one (or more) tables are large; maintenance may be advised/' -e '$d' < $TMP1file | sed -e '$d' >$TMP2file
		ALL_BUT_FIRST $TMP2file
		INCREMENT_EXCEPTIONS 1 43
	fi
	
	if ! $uba40feature
	then
		psql -d caspidadb -f $BLOATviews >/dev/null 2>&1			# create views, if they do not exist
		PSQLquery="select * from v_pg_stats_bloat where rebuild_required='t';"
	else
		PSQLquery="select * from v_pg_stats where rebuild_required='t';"
	fi
	psql -d caspidadb -c "$PSQLquery" 2>&1 | egrep -ve ' rows*\)$' > $TMP1file
	COUNT=`wc -l <$TMP1file | tr -d ' '`
	if test $COUNT -ge 4
	then
		echo
		NECHO "database bloat:" "`head -1 $TMP1file`"; echo 
#		NECHO "database vacuum:" "`head -1 $TMP1file`"; echo "   <== database vacuum may be advised"
#		INCREMENT_EXCEPTIONS 1 43b
		ALL_BUT_FIRST $TMP1file
	fi
		

	echo
	psql -d caspidadb -c 'select storetype,metrictype,metricjson,lastmodified from backendstats;' 2>/dev/null | sed -e 's/"/ /g' | tr -d '}{' | tee $TMP1file | egrep -ve 'Table Size : 0B , Total Files : 0 , Row Count : 0[^0-9]' >$TMP2file
	NECHO "backend stats:" "`head -1 $TMP2file`"; echo
	ALL_BUT_FIRST $TMP2file

fi | tee -a  $OUTfile

if $DATABASE_HOST
then
	AGGRcount=`psql -d caspidadb -t -c "select count(aggrname) from analyticsinfo where partitionapplied is null;" 2>/dev/null | sed -e 's/\s//g'`
	if ! test -z "$AGGRcount"
	then
		NECHO "last applied:" "count: $AGGRcount"; echo
		if test $AGGRcount -ne 0
		then
			timeout 600 impala-shell -d caspida --quiet -q "select eventtime,sum(numevents) from semiaggr_s where eventtime='$LASTapplied' group by 1 order by 1 desc limit 1;" 2>/dev/null >$TMP1file
			echo
			if test -s $TMP1file
			then
				NECHO "" "`head -1 $TMP1file`"; echo
				ALL_BUT_FIRST $TMP1file
			else
				NECHO "" "(timeout)"; echo
			fi
		fi
	fi

	psql -d caspidadb -t -c "select distinct aggrname from analyticsinfo where partitionapplied is null AND current_date > created + interval '2 hours';" 2>/dev/null |\
	sed -e '/^$/d' | sort > $TMP1file 
	if test -s $TMP1file
	then
		echo
		NECHO "partition not applied:" "`head -1 $TMP1file`"; echo "   <== impala recover partitions needed"
		ALL_BUT_FIRST $TMP1file
		INCREMENT_EXCEPTIONS
	fi
	
	GET_CASPIDA_PROPERTY analytics.store.data.retention
	if test -z "$PROPERTYvalue"
        then
                RETENTIONdays=31
		RETENTIONunit='d'
        else
                RETENTIONunit=`echo $PROPERTYvalue | sed -e 's/[ 0-9]//g'`
                RETENTIONdays=`echo $PROPERTYvalue | sed -e 's/[^0-9]//g'`
        fi

	case $RETENTIONunit in
		y|Y) RETENTIONdays=`expr $RETENTIONdays \* 365`;;
		m|M) RETENTIONdays=`expr $RETENTIONdays \* 31`;;
		w|W) RETENTIONdays=`expr $RETENTIONdays \* 7`;;
	esac

	echo
	NECHO "analytics retention:" "$PROPERTYvalue (${RETENTIONdays}d)"; echo
	echo
	GET_CASPIDA_PROPERTY executor.offline.workersPerInstance
        NECHO "workersPerInstance:" "executor.offline.workersPerInstance=$PROPERTYvalue"; echo
	echo


#	echo
#	psql -d caspidadb -c "select partitioninfo,lastupdated from analyticsinfo where aggrname='semiaggr_s' order by 1 limit $RETENTIONdays;" > $TMP1file 2>&1
#	NECHO "psql semiaggr_s:" "`head -1 $TMP1file`"; echo
#	ALL_BUT_FIRST $TMP1file
	
	set `echo "$TODAY" | tr '-' ' '`
	YEAR=$1
	MONTH=$2
	DAY=$3
	TODAYevent="eventYear=\"$YEAR\",eventMonth=\"$MONTH\",eventDay=\"$DAY\""

	psql -d caspidadb -t -c "select lastupdated from analyticsinfo where aggrname='semiaggr_s' AND partitioninfo='$TODAYevent';" > $TMP1file 2>/dev/null
	NECHO "semiaggr_s lastupdated:" "$TODAYevent `head -1 $TMP1file`"
	if ! test -s $TMP1file
	then
		echo "   <== today's partition is missing; check analytics"
		INCREMENT_EXCEPTIONS
	else
		echo
	fi

fi | tee -a  $OUTfile 

if $HADOOP_NAMENODE
then
	hadoop fs -ls hdfs:/user/caspida/analytics/caspida.db/*/*/eventyear* 2>/dev/null | egrep -e '^-r' > $TMP1file
	if test -s $TMP1file
	then
		NECHO  "analytics directories:" "`head -1 $TMP1file`"; echo "   <== unexpected analytics sub-directories; may impact future upgrades"
		ALL_BUT_FIRST $TMP1file
		INCREMENT_EXCEPTIONS
	fi
	hadoop fs -ls hdfs:/user/caspida/analytics/caspida.db/*/_impala_insert_staging/* 2>/dev/null | egrep -e '^-r' > $TMP1file
	if test -s $TMP1file
	then
		HOURago=`expr $NOW - 3600`
		while read LINE
		do
			TIMEstamp=`echo "$LINE" | sed -e 's/\s\s*/ /g' | cut -d ' ' -f 6,7`
			TIMEstamp=`date -d "$TIMEstamp" "+%s" 2>/dev/null`
			if test "$TIMEstamp" -lt "$HOURago" 2>/dev/null
			then
				ERROR="<= unexpected 'insert' files; may impact future upgrades"
			else
				ERROR="(unexpected 'insert' files; may impact future upgrades)"
			fi
			echo "${LINE}   $ERROR"
		done < $TMP1file > $TMP2file
		NECHO  "analytics insert files:" "`head -1 $TMP2file`"; echo 
		ALL_BUT_FIRST $TMP2file
	fi
fi | tee -a  $OUTfile

if ! $uba50feature
then
	if $uba431feature && $IMPALA_HOST
	then
		sudo -u hdfs hadoop fs -ls -R /user/caspida/analytics/caspida.db/UserProfiles /user/caspida/analytics/caspida.db/DeviceProfiles /user/caspida/analytics/caspida.db/AuditEvents /user/caspida/analytics/caspida.db/DataSourceValidations 2>/dev/null | egrep -v -e "\simpala\s\s*caspida\s" > $TMP1file
		if test -s $TMP1file
		then
			NECHO "hdfs owner:" "`wc -l < $TMP1file` files not owned by 'impala  caspida'"
			echo "   <== UBA-12756 'Known Issue' in UBA 4.3.1 Release Notes"
			INCREMENT_EXCEPTIONS
			head -10 $TMP1file > $TMP2file
			NECHO "" "`head -1 $TMP2file`"; echo
			ALL_BUT_FIRST $TMP2file
		fi
	fi
fi

if $IMPALA_HOST && $DATABASE_HOST
then
	case "$NUMnodes" in
		1) IMPALAtimeout=150;;
		3) IMPALAtimeout=300;;
		5) IMPALAtimeout=600;;
		7) IMPALAtimeout=1200;;
		10) IMPALAtimeout=1800;;
		*) IMPALAtimeout=3600;;
	esac
	if ! $MULTInode
	then
		if $uba41feature
		then
			SUPERnode=`cat $SNODEfile`
			if $SUPERnode
			then
				IMPALAtimeout=1500
				NECHO "super node" ""
				echo "timeout: $IMPALAtimeout seconds"
			fi
		fi
	fi
	echo

	DURATIONsec=0
        STARTsecs=`date "+%s"`

	NECHO "UTC time:" ""; TZ=UTC date
	echo
	NECHO "impala" "(queries will timeout after $IMPALAtimeout seconds)"; echo

	for IMPALAtable in irinfo userprofiles semiaggr_s
	do
		impala-shell --quiet -d caspida -q "select min(tspartition), max(tspartition) from $IMPALAtable ;" > $TMP1file 2>/dev/null
		if test -s $TMP1file
		then
			echo
			NECHO "${IMPALAtable}:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		fi
	done

	echo

	for IMPALAtable in remodelfeatures destinationports
	do
		impala-shell --quiet -d caspida -q "refresh $IMPALAtable; show partitions $IMPALAtable;"  > $TMP1file 2>/dev/null
		if test -s $TMP1file
		then
			NECHO "$IMPALAtable partitions:" "`head -1 $TMP1file`"; echo
        		ALL_BUT_FIRST $TMP1file
		fi
	done

	ERROR=""
	if impala-shell --quiet -d caspida -q 'refresh semiaggr_s; show partitions semiaggr_s;'  > $TMP1file 2>$TMP2file
        then
                ERROR="   <== no offline activity for last few days"
		for DATE in $TOMORROW $TODAY $YESTERDAY $DAYBEFORE
		do
			if $uba42feature
			then
				EVENTdate=`echo "$DATE" | sed -e  's/-//g'`
			else
				EVENTdate=`echo "$DATE" | sed -e  's/-/[^0-9]*/g' -e 's/^/[^0-9]/' -e 's/$/[^0-9]/'`
			fi
			if egrep -qe "$EVENTdate" $TMP1file
			then
				if test "$DATE" = "$YESTERDAY" -o "$DATE" = "$DAYBEFORE"
				then
					ERROR="   <== no offline activity for today (${EVENTdate})"
				else
					ERROR=""
				fi
				break
			fi
		done
        else
                ERROR="   <== impala-shell failed to 'show partitions semiaggr_s'"
		cp $TMP2file $TMP1file
        fi

	if ! test -z "$ERROR"
	then
		INCREMENT_EXCEPTIONS 1 37
	fi
	echo
        NECHO "semiaggr_s partitions:" "`head -1 $TMP1file`$ERROR"; echo
        ALL_BUT_FIRST $TMP1file

	GET_CASPIDA_PROPERTY analytics.store.data.retention
	if test -z "$PROPERTYvalue"
        then
                RETENTIONdays=31
		RETENTIONunit='d'
        else
                RETENTIONunit=`echo $PROPERTYvalue | sed -e 's/[ 0-9]//g'`
                RETENTIONdays=`echo $PROPERTYvalue | sed -e 's/[^0-9]//g'`
        fi

	case $RETENTIONunit in
		y|Y) RETENTIONdays=`expr $RETENTIONdays \* 365`;;
		m|M) RETENTIONdays=`expr $RETENTIONdays \* 31`;;
		w|W) RETENTIONdays=`expr $RETENTIONdays \* 7`;;
	esac
	PAST=`expr $RETENTIONdays + 2`		# allow extra 'day' because of UTC

	LINES=`wc -l < $TMP1file` 
	
	if test $LINES -gt 6 
	then
		CHECKoutliers=true
	else
		CHECKoutliers=false
	fi

	> $TMP1file
	echo

	IMPALAquery="select dataformat, \
		quotient(min(timeout),1000)  as min_duration, \
		quotient(avg(timeout),1000)  as avg_duration, \
		quotient(max(timeout),1000)  as max_duration, \
		ndv(timeout) as ndv_duration \
	from irinfo \
	where eventtime >= '$LAST_4days' \
		AND eventtype='Associate' \
		AND mappedentitytype='Device' \
		AND entitytype='Device' \
		AND entity like '%.%' \
	group by 1 order by 1;"

	impala-shell --quiet -d caspida -q "$IMPALAquery" 2>/dev/null >$TMP1file
        if test -s $TMP1file
        then
                NECHO "recent irinfo lease:" "`head -1 $TMP1file`"; echo
                ALL_BUT_FIRST $TMP1file
                echo
        fi
	
	NECHO "recently populated views:" ""

	if ! test -f $LOADfile
	then
		if $uba40feature
		then 
			if timeout $IMPALAtimeout impala-shell -d caspida --quiet -B -q "refresh semiaggr_s; select dataformat,viewslist,ndv(viewslist) from semiaggr_s where eventtime >= '$LAST_4days' group by dataformat, viewslist;" 2>/dev/null > $TMP1file
			then
				sed -e '/^$/d' -e 's/\s\s*[0-9][0-9]*$//' < $TMP1file > $TMP2file 
			else
				echo "TimeOut $IMPALAtimeout seconds" > $TMP2file
			fi
			sed -e 's/^.*\s\s*//' < $TMP2file |\
			sed -e 's/|/\n/g' |\
			sort -u |\
			while read TLA
			do
				case $TLA in
					ADE) echo "Active Directory Event";;
					ATH) echo "Authentication";;
					CLD) echo "Cloud Data";;
					DAT) echo "Data";;
					DHC) echo "DHCP";;
					DLP) echo "DLP";;
					DNS) echo "DNS";;
					EAL) echo "External Alarm";;
					EAS) echo "Entity Association";;
					EML) echo "Email";;
					END) echo "Endpoint";;
					FWL) echo "Firewall";;
					HTP) echo "HTTP (Proxy)";;
					PRI) echo "Printer";;
					NET) echo "Network";;
					*)	echo $TLA;;
				esac
			done | tee $TMP3file
			echo
			VIEWade=false
			VIEWdhc=false
			VIEWdns=false
			VIEWhtp=false
			VIEWnet=false
			if grep -q "Active Directory Event" $TMP3file
			then
				VIEWade=true
			fi
			if grep -q "Network" $TMP3file
			then
				VIEWnet=true
			fi
			if grep -q "DHCP" $TMP3file
			then
				VIEWdhc=true
			fi
			if grep -q "DNS" $TMP3file
			then
				VIEWdns=true
			fi
			if grep -q "HTTP (Proxy)" $TMP3file
			then
				VIEWhtp=true
			fi
			if ! ($VIEWade && $VIEWdhc && $VIEWdns && $VIEWhtp && $VIEWnet)
			then
				echo "missing:"
				if ! $VIEWade
				then
					echo "    Active Directory Event   <= ADE view not populated"
				fi
				if ! $VIEWnet
				then
					echo "    Network   <= NET view not populated"
				fi
				if ! $VIEWdhc
				then
					echo "    DHCP   <= DHC view not populated"
				fi
				if ! $VIEWdns
				then
					echo "    DNS   <= DNS view not populated"
				fi
				if ! $VIEWhtp
				then
					echo "    HTTP (Proxy)   <= HTP view not populated"
				fi
				echo
			fi
			if test -s $TMP2file
			then
				echo "detail:"
				sed -e '/^$/d' -e 's/^/    /' < $TMP2file | sort
			fi
		fi > $TMP1file
	
		if ! test -s $TMP2file
		then 
	               	psql -d caspidadb -t -c "select  viewstatsjson from connectorstats where state='Processing' order by id ;" 2>/dev/null |\
	               	sed -e '/^$/d' -e 's/[{} "]//g' -e 's/,/\n/g' |\
	               	cut -d ':' -f 1 |\
			sed -e '/^$/d' |\
	               	sort -u >> $TMP1file
		fi 
	
		if test -s $TMP1file
		then
			head -1 $TMP1file
			ALL_BUT_FIRST $TMP1file
		else
			echo "   <== no activity detected in last few days"
			INCREMENT_EXCEPTIONS
		fi
	else
		echo "   (bypassed; system under load)"
	fi

        STOPsecs=`date "+%s"`
        ELAPSEDsecs=`expr $STOPsecs - $STARTsecs`
	NECHO "" "(elapsed: ${ELAPSEDsecs})"; echo
	DURATIONsecs=`expr $DURATIONsecs + $ELAPSEDsecs`
        IMPALAtimeout=`expr $IMPALAtimeout - $ELAPSEDsecs`
	if test "$IMPALAtimeout" -le "0"
	then
		IMPALAtimeout=30
	fi
        STARTsecs=`date "+%s"`

	echo
	ERROR=""
	NECHO "  recent events:" ""
	if timeout $IMPALAtimeout impala-shell --quiet -d caspida -q "refresh semiaggr_s; select eventtime,sum(numevents) from semiaggr_s where eventtime >= '$LAST_4days' group by 1 order by 1 desc; profile;" > $TMP1file 2>/dev/null
	then
                STOPsecs=`date "+%s"`
                ELAPSEDsecs=`expr $STOPsecs - $STARTsecs`
		echo "(elapsed: $ELAPSEDsecs)" >> $TMP1file
		DURATIONsecs=`expr $DURATIONsecs + $ELAPSEDsecs`
                IMPALAtimeout=`expr $IMPALAtimeout - $ELAPSEDsecs`
		if test "$IMPALAtimeout" -le "0"
		then
			IMPALAtimeout=30
		fi
		if test -s $TMP1file
		then
			if grep -q "$TODAY" $TMP1file
			then
				ERROR=""
			else
				ERROR="   <== no offline events for today"
			fi
		fi

                STARTsecs=`date "+%s"`
		timeout $IMPALAtimeout impala-shell --quiet -d caspida -q "refresh semiaggr_s; select dataformat, sum(numevents), count(distinct source) from semiaggr_s where eventtime >= '$LAST_4days' group by 1 order by 1;" >> $TMP1file 2>/dev/null
                STOPsecs=`date "+%s"`
                ELAPSEDsecs=`expr $STOPsecs - $STARTsecs`
		echo "(elapsed: $ELAPSEDsecs)" >> $TMP1file
		DURATIONsecs=`expr $DURATIONsecs + $ELAPSEDsecs`
                IMPALAtimeout=`expr $IMPALAtimeout - $ELAPSEDsecs`
		if test "$IMPALAtimeout" -le "0"
		then
			IMPALAtimeout=30
		fi
                STARTsecs=`date "+%s"`
	else
		ERROR="   <== there may be an issue with offline models (or size)"
		ELAPSEDsecs=0
	fi

	if ! test "$ERROR" = ""
	then
		INCREMENT_EXCEPTIONS
	fi
	echo "`head -1 $TMP1file`$ERROR"
	ALL_BUT_FIRST $TMP1file

	if test -f $SEQUENTIALsubnets
	then
		COUNT=`wc -l < $SEQUENTIALsubnets`
		if test "$COUNT" -gt "64"
		then
			echo
			NECHO "sequential ip source:" ""
			DESTINATIONnetFirst=`head -1 $SEQUENTIALsubnets | sed -e 's/ //g' -e 's+\.0/24.*$+\.+'`
			MIDDLE=`expr 1 + $COUNT / 2`
			MIDDLEcidr=`sed -n "${MIDDLE}p" < $SEQUENTIALsubnets`
			DESTINATIONnetMiddle=`echo "$MIDDLEcidr" | sed -e 's/ //g' -e 's+\.0/24.*$+\.+'`
			DESTINATIONnetLast=`tail -1 $SEQUENTIALsubnets | sed -e 's/ //g' -e 's+\.0/24.*$+\.+'`

			STARTsecs=`date "+%s"`
			if ! test -f $LOADfile
			then
				timeout $IMPALAtimeout impala-shell --quiet -d caspida -q "select  distinct source, eventtime, destination, dataformat from semiaggr_s where destinationscope='Internal' and (destination='${DESTINATIONnetFirst}1' OR destination='${DESTINATIONnetFirst}126' OR destination='${DESTINATIONnetFirst}254' OR destination='${DESTINATIONnetMiddle}1' OR destination='${DESTINATIONnetMiddle}126' OR destination='${DESTINATIONnetMiddle}254' OR destination='${DESTINATIONnetLast}1' OR destination='${DESTINATIONnetLast}126' OR destination='${DESTINATIONnetLast}254') order by 2,3,4 limit 45;" > $TMP1file 2>/dev/null
        			STOPsecs=`date "+%s"`
        			ELAPSEDsecs=`expr $STOPsecs - $STARTsecs`
        			DURATIONsecs=`expr $DURATIONsecs + $ELAPSEDsecs`
        			IMPALAtimeout=`expr $IMPALAtimeout - $ELAPSEDsecs`
        			if test "$IMPALAtimeout" -le "0"
        			then
        				IMPALAtimeout=30
        			fi

				head -1 $TMP1file
				ALL_BUT_FIRST $TMP1file
				NECHO "" "(elapsed: ${ELAPSEDsecs})"; echo
			else
				echo "   (bypassed; system under load)"
			fi
		fi
	fi

	if $CHECKoutliers
	then
		TZlocal=`date +'%Z'`
		for OFFSET in 0 -$PAST
		do
			DATE_compare=`impala-shell -d caspida -B --quiet -q "select adddate(to_utc_timestamp(now(), '$TZlocal'), $OFFSET);" 2>/dev/null`
			if test $OFFSET -eq 0
			then
				WHEN="future"
				COMPARE=">"
			else
				WHEN="past"
				COMPARE="<"
			fi

			echo
			
                	STARTsecs=`date "+%s"`
        		NECHO "impala $WHEN events:" ""
        		if (timeout $IMPALAtimeout impala-shell --quiet -d caspida -q "refresh semiaggr_s; select dataformat,sum(numevents) from semiaggr_s where eventtime $COMPARE '$DATE_compare' group by 1;") 2>$TMP2file | sed '/^$/d' > $TMP1file
        		then
				LINES=`wc -l < $TMP1file`
				if test "$LINES" -gt "2"
				then
                			ERROR="   <== data source formats with $WHEN events"
				else
					ERROR=""
				fi
        		else
                		ERROR="   <== there may be an issue with offline models (or size)"
				cat $TMP2file >> $TMP1file
        		fi

                	if ! test -z "$ERROR"
			then
				INCREMENT_EXCEPTIONS 1 38
	
        			echo "`head -1 $TMP1file`$ERROR"
        			ALL_BUT_FIRST $TMP1file
			else
				echo
			fi
                	STOPsecs=`date "+%s"`
                	ELAPSEDsecs=`expr $STOPsecs - $STARTsecs`
			NECHO "" "(elapsed: ${ELAPSEDsecs})"; echo
			DURATIONsecs=`expr $DURATIONsecs + $ELAPSEDsecs`
                	IMPALAtimeout=`expr $IMPALAtimeout - $ELAPSEDsecs`
			if test "$IMPALAtimeout" -le "0"
			then
				IMPALAtimeout=30
			fi
                	STARTsecs=`date "+%s"`
		done
	fi

	echo
	STARTsecs=`date "+%s"`
	DESTports="'1','2','1023','1024','24832','24833','49151','49152','57358','57359','65534','65535'"
	NECHO "sampled ports:" "$DESTports"; echo

        NECHO "recent port scanners:" ""
        if timeout $IMPALAtimeout impala-shell --quiet -d caspida -q "select source, destinationport, count(*) as destinations from destinationports where eventtime >= '$LAST_4days' AND destinationport IN ( $DESTports )  group by 1,2 order by 1,2;" 2>$TMP2file >$TMP1file
	then
         	STOPsecs=`date "+%s"`
		sed '/^$/d' <$TMP1file  | egrep -ve '^.*\|.*\|.*\| [0-9][ 0-9][ 0-9][^0-9].*\|.*$' > $TMP2file
		LINES=`wc -l < $TMP2file`
		cp $TMP1file $TMP3file		# save detailed ports`
		head -3 $TMP2file > $TMP1file
		egrep -ve '^\+--|^\| sou' < $TMP2file >> $TMP1file
                if test "$LINES" -gt "3"
                then
                        ERROR=""
                else
                	ERROR="   (no recent destinationports samples)"
                fi
        else
        	STOPsecs=`date "+%s"`
                ERROR="   <= timeout or other error $?" 
                cat $TMP2file >> $TMP1file
		> $TMP3file
        fi

        echo "`head -1 $TMP1file`$ERROR"
	ALL_BUT_FIRST $TMP1file

        ELAPSEDsecs=`expr $STOPsecs - $STARTsecs`
        NECHO "" "(elapsed: ${ELAPSEDsecs})"; echo
        DURATIONsecs=`expr $DURATIONsecs + $ELAPSEDsecs`

	sed -e '/source/d' -e '/+/d' -e 's/\s//g' -e 's/^|//' -e 's/|.*$//' < $TMP3file|\
	sort | uniq -c | sort -nr | head -9 |\
	sed -e "s/^\s*[^\s][^\s]*\s//" -e "s/^/'/" -e "s/$/',/" |\
	sed -e :a -e '$!N; s/\n//; ta' |\
	sed -e 's/,$//' > $TMP1file    # list of top scanners
	TOPscanners="`head -1 $TMP1file`"

	> $TMP1file
	if ! test "$TOPscanners" = ""
	then
		NECHO "sampled sources:" "$TOPscanners"; echo
		STARTsecs=`date "+%s"`
        	if timeout $IMPALAtimeout impala-shell --quiet -d caspida -q "select source, sourcescope, destinationscope, protocol, externalaction, count(distinct destinationport) as distinct_ports from destinationports where eventtime >= '$LAST_4days' AND source IN ( $TOPscanners ) group by 1,2,3,4,5 order by 1,2,3,4,5;" 2>$TMP2file >$TMP1file
		then
			STOPsecs=`date "+%s"`
		else
			STOPsecs=`date "+%s"`
			cat $TMP2file >> $TMP1file
		fi	
	fi
        ELAPSEDsecs=`expr $STOPsecs - $STARTsecs`
	if test -s $TMP1file
	then
		NECHO "recent distinct ports:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	fi	
	
        NECHO "" "(elapsed: ${ELAPSEDsecs})"; echo
        DURATIONsecs=`expr $DURATIONsecs + $ELAPSEDsecs`

	echo
	NECHO "impala queries:" "$DURATIONsecs seconds duration"; echo	

	if $uba42feature
        then
		URL="https://${JOB_MANAGER_host}:25000/queries"
                TOKEN=`grep '^\s*jobmanager.restServer.auth.user.token=' /opt/caspida/conf/uba-default.properties | cut -d'=' -f2`
                AUTH="-Ssk -H 'Authorization: Bearer $TOKEN'"
        else
                URL="http://${JOB_MANAGER_host}:25000/queries"
                AUTH=""
        fi

	echo
	NECHO "jobmanager queries:" ""
	IMPALAflight=`eval curl $AUTH $URL 2>/dev/null | grep flight | sed -e 's+</*h3>++g'`
	if ! test -z "$IMPALAflight"
	then
		echo -n "impala $IMPALAflight"
		IMPALAcount=`echo "$IMPALAflight" | cut -d ' ' -f 1`
		if ! test -z "$IMPALAcount"
		then
			if test "$IMPALAcount" -eq "$IMPALAcount" -a  "$IMPALAcount" -gt "500"  2>/dev/null
			then
				echo "   <== need to run free-impala-connxs.sh"
				INCREMENT_EXCEPTIONS 1 35b
				IMPALApid=`ps -C impalad --no-headers | sed -e 's/^ *//' | cut -d ' ' -f 1`
				if ! test -x "$IMPALApid"
				then
					top -bn 1 -p $IMPALApid 2>&1 | egrep -e "PID USER|$IMPALApid" | sed -e "s/^/$INDENT/"
				fi
				if test -s /tmp/iqueries.log
				then
					tail -3 /tmp/iqueries.log | sed -e "s/^/$INDENT/"
				fi
			else
				echo
			fi
		else
			echo
		fi
	fi
	echo
fi | tee -a  $OUTfile


if $DATABASE_HOST
then
        #
        #       if Email output connector has been set, then send health check as an attachment
        #       otherwise, exit
        #

        GET_CASPIDA_PROPERTY alert.email.lists
        if test -z "$PROPERTYvalue"
        then
                SENDemail=false                    # cannot sent email
        else
                EMAILrecipients=$PROPERTYvalue
		echo
		if test "$DATABASE_HOST_list" = ""
		then
			DATABASE_HOST_list="localhost"
		fi
                ssh $DATABASE_HOST_list "psql -d caspidadb -c 'select name,type,creationtime from outputconnectors order by 3;' 2>/dev/null" | egrep -ve ' rows*\)$' >$TMP1file

		NECHO "EMail Output:" ""
                if egrep -q '\bEmail\b' $TMP1file
                then
			echo "connector exists"
                        SENDemail=true		# can send email
                else
			echo "connector does not exist"
                	SENDemail=false		# cannot sent email
                fi
		echo
        fi
fi > $TMP2file		# save output temporarily to prevent sub-shell
tee -a  $OUTfile < $TMP2file	# display it now

( 
	#
	#	if this is multi-node cluster, then current script should be copied to other nodes:
	#
	
	if $LEADERnode
	then
		# FIRSTnode is responsible for checking other nodes
		if test -s "$MEMBERwait"
		then
                        echo
			ITERcount=0
			if test "$NUMnodes" = "20"
			then
				ITERmax=90
				ITERsleep=90
			else
				ITERmax=`expr $NUMnodes \* 8`
				ITERsleep=15
			fi
                        while ps `cat $BGjobs` > $TMP1file 2>/dev/null
                        do
                                JOBcount=`wc -l < $TMP1file`
                                JOBcount=`expr $JOBcount - 1`

				if test $ITERcount -lt $ITERmax
				then
                                	NECHO "waiting:" "$JOBcount of other check scripts still executing; sleeping $ITERsleep seconds"; echo
					ITERcount=`expr $ITERcount + 1`
                                	sleep $ITERsleep                                    # wait for all members to complete
				else
					NECHO "gave up:" "$JOBcount of other check scripts still executing after $ITERcount iterations"; echo
					break
				fi
                        done
                        echo
			while read MEMBERoutfile
			do
				MEMBERname=`echo $MEMBERoutfile | sed -e "s+^$MEMBERout++" -e 's/.txt$//'`	
				
				echo
				echo "Checking $MEMBERname ..."
				echo

				if ! test -s $MEMBERoutfile
				then

					NECHO "missing output:" "$MEMBERoutfile   <== missing or empty"; echo
					INCREMENT_EXCEPTIONS 1 49z
					continue
				fi

                                if ! grep -q '(check completed)' $MEMBERoutfile
                                then
                                        NECHO "partial output:" "review $MEMBERname   <== check did not complete"; echo
                                        INCREMENT_EXCEPTIONS
                                fi

				head -n -4  $MEMBERoutfile | tr -d '\r'		# all but the last few lines
				tail -4 $MEMBERoutfile | grep concerns > $TMP2file

				NODEexceptions=`sed -e 's/^.*remediate //' -e 's/ concerns$//' < $TMP2file`

				NECHO "$MEMBERname:" "'$NODEexceptions' concerns"; echo

                                if expr "$NODEexceptions" + 0 > /dev/null 2>&1
                                then
                                        # echo "$NODE: `cat $TMP2file`"
                                        INCREMENT_EXCEPTIONS $NODEexceptions 49
                                fi
			done < $MEMBERwait
		fi
        else
                echo; NECHO "${THISnode}:" "(check completed)"; echo; echo; echo; echo; echo
	fi
	echo
	
	EXCEPTIONS=`cat $EXCfile`
	
	if test -z "$EXCEPTIONS"
	then
		EXCEPTIONS=0
	fi
	
	echo
	if test "$EXCEPTIONS" !=  "0"
	then
		# cat $EXCref
		echo "Please review and remediate $EXCEPTIONS concerns"
	else
		echo "No concerns raised"
	fi
	echo
	echo
) | tee -a $OUTfile

if $LEADERnode
then
	egrep -e "uptime:.*load average" $OUTfile |\
	sed -e 's/^.*uptime:\s\s*//' > $TMP1file
	if test -s $TMP1file
	then
		NECHO "uptimes:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
		echo
	fi >> $OUTfile

	egrep -e "psql start:|zookeeper start:|last boot:" $OUTfile |\
	sed -e 's/^\s\s*//' > $TMP1file
	if test -s $TMP1file
	then
		NECHO "start times:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
		echo
		echo
	fi >> $OUTfile

	egrep -e "^Checking |   <==" $OUTfile > $EXLfile
	sed -i '/^\s\s*Checking /i \\n' $EXLfile
	egrep -e "   <= |^Checking " $OUTfile > $OBLfile
	sed -i '/^\s\s*Checking /i \\n' $OBLfile
	if test -s $EXLfile
	then
		echo
		NECHO "note:" "'<='  is an observation that may never be a concern"; echo
		NECHO "" "'<==' is a concern that should be reviewed; it may not be relevant in all deployments"; echo
		NECHO "" "'*'   PII has been obfuscated"; echo
		echo
		NECHO "concern summary:" "`head -1 $EXLfile`"; echo
		echo
		ALL_BUT_FIRST $EXLfile
		echo
		echo
	fi | tee -a $OUTfile

	if test -s $OBLfile
	then
		echo
		echo
		NECHO "observation summary:" "`head -1 $OBLfile`"; echo
		echo
		ALL_BUT_FIRST $OBLfile
		echo
		echo
	fi | tee -a $OUTfile

        (
                echo
                OSrelease="`lsb_release -ds 2>/dev/null | tr -d '\"'| sed -e 's/ *$//'`"
                NECHO "current os:" "'$OSrelease'"; echo
                UBAversion=`grep release-version $CASPIDAdir/conf/version.properties 2>/dev/null | cut -d "=" -f 2`
                NECHO "current uba:" "'$UBAversion'"; echo
        	CONTENT=`egrep -e '"version"' /opt/caspida/content/*/content-descriptor.json 2>/dev/null | sort | tail -1 | cut -d '"' -f 4`
        	if ! test -z "$CONTENT"
        	then
                	NECHO "current content:" "'$CONTENT'"; echo
        	fi
		if $MULTInode
		then
			if test "$DATABASE_HOST_list" = ""
			then
				DATABASE_HOST_list="localhost"
			fi
			DBversion=`ssh $DATABASE_HOST_list "psql -d caspidadb -t -c 'select currentversion from dbinfo;' 2>/dev/null" 2>/dev/null | tr -d ' '`
		else
			DBversion=`psql -d caspidadb -t -c 'select currentversion from dbinfo;' 2>/dev/null | tr -d ' '`
		fi
                NECHO "current db:" "'$DBversion'"; echo
                echo
		NECHO "${BASEname}:" "'$VERSion'"; echo
		echo
        ) | tee -a $OUTfile

	STARTsecs=$NOW					# that was then
	NOW=`date '+%s'`				# this is now
	TIMEnow=`date "--date=@$NOW" "+%F_%R:%S"`	# use end time

	ELAPSEDsecs=`expr $NOW - $STARTsecs`
	ELAPSEDmins=`expr $ELAPSEDsecs / 60`
	ELAPSEDsecs=`expr $ELAPSEDsecs \% 60`
	(echo "${PROGname}: duration $ELAPSEDmins minutes and $ELAPSEDsecs seconds"; echo) | tee -a $OUTfile
	
	if test -d /var/log/caspida
	then
		SAVEdir=/var/log/caspida/check
	else
		SAVEdir=/home/caspida/check
	fi

	case $BASEname in
		uba_health_check)	SAVEname="uhc";;
		uba_pre_check)		SAVEname="upc";;
		*)			SAVEname=$BASEname;;
	esac

	if mkdir -p $SAVEdir >/dev/null 2>&1
	then
		SAVEfile="${SAVEname}_${THISnode}_${OUTnow}_v${VERSion}.txt"

		if cp $OUTfile $SAVEdir/$SAVEfile 2>/dev/null
		then
			echo "${PROGname}: output saved as    $SAVEdir/$SAVEfile" | tee -a $OUTfile
			if ln -sf $SAVEdir/$SAVEfile $SAVEdir/${BASEname}.txt 2>/dev/null
        		then
                		echo "${PROGname}: output linked to $SAVEdir/${BASEname}.txt"
			else
				echo "${PROGname}: could not link to $SAVEdir/${BASEname}.txt"
        		fi
		else
			echo "${PROGname}: could not copy output to $SAVEdir/$SAVEfile"
		fi
	else
		echo "${PROGname}: could not create $SAVEdir"
		echo "${PROGname}: could not save output"
	fi
	
	echo
	echo
else
	if ! cp $OUTfile /var/log/caspida/${BASEname}_${THISnode}_member.txt 2>/dev/null	
	then
		cp $OUTfile $TMP/${BASEname}_${THISnode}_member.txt 2>/dev/null
	fi
fi

if $LEADERnode
then

	if ! $SENDemail
	then
		exit
	fi

	if test "$EMAILto" = "noBody"
	then
		exit
	fi

	if test "$EMAILto" != "alerts"
	then
		EMAILrecipients="$EMAILto"	# use provided addresses and not caspida-site.properties
	fi

	if ! test -f $SUMMfile
	then
		> ${SUMMfile}.prev			# first time ... all concerns will be sent
	else
		mv ${SUMMfile} ${SUMMfile}.prev		# otherwise, we'll compare to previous	
	fi

	sed -ne '/summary:/,$p' < $SAVEdir/$SAVEfile | grep -v "^$PROGname" | sed -e "s/^.* <=/${INDENT}<=/" > ${SUMMfile}

	if diff -b ${SUMMfile} ${SUMMfile}.prev >/dev/null 2>&1
	then
		exit                    # nothing more to do 
	else
		NECHO "email alert:" "To: $EMAILrecipients"; echo
	fi

	(
		echo "To: ${EMAILrecipients}"
		echo "Reply-To: no-reply@splunk.com"
		echo "Subject: $BASEname - unresolved concerns"
		echo "Mime-Version: 1.0"
		echo "Content-Type: multipart/mixed; boundary=\"$BOUNDARY\""
		echo
	) > $HEADERfile

	(
		echo "--$BOUNDARY"
		echo "Content-Type: text/plain; charset=\"US-ASCII\""
		echo "Content-Transfer-Encoding: 7bit"
		echo "Content-Disposition: inline"
		echo
		echo "Please review attached UBA health check"
		echo
		cat $SUMMfile
		echo
	) > $BODYfile

	(
		echo "--$BOUNDARY"
		echo "Content-Type: `file  --brief --mime-type $FILEname`"
		echo "Content-Transfer-Encoding: base64"
		echo "Content-Disposition: attachment; filename=\"`basename $SAVEfile`\""
		base64 $SAVEdir/$SAVEfile
		echo
       	 	echo "--$BOUNDARY"
		echo
	) > $ATTACHfile

	cat $HEADERfile $BODYfile $ATTACHfile |\
	tee $MAILfile |\
	/usr/sbin/sendmail -t -oi						# send the message with file attached (save the message)
else
	sleep 5
fi 

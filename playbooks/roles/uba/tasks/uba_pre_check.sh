#! /bin/sh

##
# Copyright (C) 2005-2020 Splunk Inc. All rights reserved.
# All use of this Software is subject to the terms and conditions accepted upon installation of
# and/or purchase of license for the Software.
##

#	uba_pre_check.sh
#
# 	The infrastructure is the foundation for an successful implementation.  The software makes assumptions
#	that everything was done properly; it does not validate ... 
#
#	... so, the intent of script is to verify that infrastructure requirements have been met before
#	installing or upgrading Splunk>UBA software.
#
#	Exceptions are noted by a <== and requires some remediation

VERSion="20.08.007"
 
WHOIAM=`whoami 2>/dev/null`

echo 
echo -n "    whoami:                  '$WHOIAM'"

CASPIDAhome="/home/caspida/"

case "$WHOIAM" in
	caspida)	echo
			CASPIDAuser=true
			CASPIDAhome="$HOME";;
	*)		echo
			echo "   <= script must be run by user 'caspida' to properly complete =>";
			echo 
			CASPIDAuser=false;;
esac

PROGname="`basename $0`"
PROGdir="`dirname $0`"
PROGpwd="`pwd`"
BASEname="`basename $PROGname .sh`"

case "$PROGdir" in
	.) PROGdir="$PROGpwd";;			# must be absolute
	.*) PROGdir="$PROGpwd/$PROGdir";;	# must be absolute
esac

GETmd5=false
GETfile=false
GOTfile=false

export LEADERnode=false
export MEMBERnode=false
export MEMBERlist=""

JAVA_HOME=`echo '. /opt/caspida/bin/CaspidaCommonEnv.sh >/dev/null 2>&1; echo $JAVA_HOME' | bash `

CLASSPATH=/opt/caspida/lib/CaspidaSecurity.jar

export JAVA_HOME CLASSPATH

CASPIDA_PROPERTIES_EXIST=false				# not 'true', until 'setup' has been run

CASPIDAdir=/opt/caspida

UBA_DEFAULT_PROPERTIES=$CASPIDAdir/conf/caspida-default.properties	# defaults
UBA_ENV_PROPERTIES=$CASPIDAdir/conf/uba-env.properties			# blueprint deployment in 2.4
UBA_SITE_PROPERTIES=$CASPIDAdir/conf/caspida-site.properties		# customization

LISTENpid=""
LISTENport="43210"
LISTENprog="upc_sym_nc"
LISTENresp="Hello, UBA World!"
LISTENmode=false

ZOOKEEPERdir=/var/lib/zookeeper

minCPUs=16		# 16 processors per node
minMEM=62		# GB  - 64 GB less overhead
minDISK1r=50		# at least 50 GB
reqDISKs=2		# 2 disks are required
minDISK2r=500		# at least 500 GB
minDISK1f=45		# GB about the formatted size of a 50 GB drive
minDISK2f=488		# GB about the formatted size of a 500 GB drive
minSPEED=1000		# Mb/s
minAVAILdisk1=20	# GB space available for /opt/caspida
minAVAILdisk2=120	# GB space available for /var/vcap

minCONNECTIONS=200	# postgresql 'min' maximum connections 
maxCONNECTIONS=500	# postgresql 'max' maximum connections

uba24feature=false		# set true, if UBA version 2.4 or greater
uba24version="2.4"

uba30feature=false		# set true, if UBA version 3.0 or greater
uba30version="3.0"		# Chicago

uba301feature=false		# set true, if UBA version 3.0.1 or greater
uba301version="3.0.1"

uba32feature=false		# set true, if UBA version 3.2 or greater
uba32version="3.2"		# Dubai

uba321feature=false		# set true, if UBA version 3.2.1 or greater
uba321version="3.2.1"		# Dubai

uba33feature=false		# set true, if UBA version 3.3 or greater
uba33version="3.3"		# Edinburgh

uba34feature=false		# set true, if UBA version 3.4 or greater
uba34version="3.4"		# Florence

uba40version="4.0"			# new features with florence
uba40feature=false

uba4001version="4.0.0.1"			# new features with florence
uba4001feature=false

uba4002version="4.0.0.2"			# new features with florence
uba4002feature=false

uba41version="4.1"			# new features with geneva
uba41feature=false

uba411version="4.1.1"			# new features with geneva
uba411feature=false

uba412version="4.1.2"			# new features with geneva
uba412feature=false

uba413version="4.1.3"			# new features with geneva
uba413feature=false

uba42version="4.2"			# new features with hong kong
uba42feature=false

uba421version="4.2.1"			# new features with hong kong
uba421feature=false

uba422version="4.2.2"			# new features with hong kong
uba422feature=false

uba423version="4.2.3"			# new features with hong kong
uba423feature=false

uba43version="4.3"			# new features with indianapolis
uba43feature=false

uba431version="4.3.1"			# new features with indianapolis
uba431feature=false

uba4311version="4.3.1.1"		# new features with indianapolis
uba4311feature=false

uba432version="4.3.2"			# new features with indianapolis
uba432feature=false

uba433version="4.3.3"			# new features with indianapolis
uba433feature=false

uba434version="4.3.4"			# new features with indianapolis
uba434feature=false

uba50version="5.0"			# new features with jakarta
uba50feature=false

uba501version="5.0.1"			# new features with jakarta
uba501feature=false

uba502version="5.0.2"			# new features with jakarta
uba502feature=false

uba503version="5.0.3"			# new features with jakarta
uba503feature=false

uba504version="5.0.4"			# new features with jakarta
uba504feature=false

uba505version="5.0.5"                   # new features with jakarta
uba505feature=false

uba506version="5.0.6"                   # new features with jakarta
uba506feature=false

uba51version="5.1"                       # new features with key west
uba51feature=false


reqKAFKA="2.10-0.9.0.1"
reqSPARK="1.4.1"
reqDB="2.3"		# will be changed to match uba_release
reqODBC="2.5.30.1011-2"
reqJDBC="2.5.24.1043"


reqSYMlog=14		# there should be 14 symlinks in /var/log to /var/vcap/sys/log
reqSYMLINKlog=" zookeeper| storm| redis| neo4j| kafka| impala| hive\-hcatalog| hive| hadoop\-yarn| hadoop\-mapreduce| hadoop\-hdfs| flume\-ng| caspida| postgresql"
reqSYMlib=7		# there should be 7 symlinks in /var/lib to /var/vcap/store
reqSYMLINKlib=" hadoop\-yarn| hadoop\-mapreduce| hadoop\-hdfs| kafka| storm| redis| postgresql" # could change to pgsql 

reqSYMLINK="/usr/lib/jvm/java-1.7.0-openjdk.x86_64"
reqSYMREF="/etc/alternatives/java_sdk_1.7.0_openjdk"

reqMAPkey="AIzaSyCjjwE-XL1JOQLYep8xvk3WFL0Ja-DSR20"

reqSECUREPATH="/sbin:/bin:/usr/sbin:/usr/bin"

reqTHREATScols="Column id starttime updatetime threattype ruleid threattypeext summary description recommendation score threatdata computedby rootsessionid patternid tagids isactive category" 

reqTHREATScols_32="Column id starttime updatetime threattype ruleid threattypeext summary description recommendation score threatdata computedby rootsessionid patternid tagids isactive threatcategory" 

recDBthreads=1		# 2.4 is '1' and 3.0 is '10'

defSEARCHhead="http://localhost:8000/"
defLATITUDE="37.37"
defLONGITUDE="-121.92"
GEOlat=""
GEOlong=""

TIMEzone=""

PARTvcap=""	# partition for /var/vcap
DEVvcap=""

PARTlib=""	# partition for /var/lib
DEVlib=""

IMPALA_SCRATCH_DIRS="/var/vcap/sys/tmp/impala"
IMPALA_MEM=20		# percentage

INDENT="                             "		# 29 spaces
#      "12345678901234567890123456789"

NSLOOKUP=true					# assume that nslookup is available

						# files that may have been modified that need to be identical on all nodes in cluster
CHECKlist="\
/etc/hadoop/conf.empty/core-site.xml \
/etc/hadoop/conf.empty/hdfs-site.xml \
/etc/hbase/conf.dist/hbase-site.xml \
/etc/storm/conf/storm.yaml \
$CASPIDAdir/bin/CaspidaCommonEnv.sh \
$CASPIDAdir/conf/attribution/*.json \
$CASPIDAdir/conf/competitorDomains.txt \
$CASPIDAdir/conf/deployment/caspida-deployment.conf \
$CASPIDAdir/conf/jobmgr.yml \
$CASPIDAdir/conf/quartz.properties \
$CASPIDAdir/conf/workflows/*.json \
$CASPIDAdir/etc/default/impala \
"

# /etc/zookeeper/conf/zoo.cfg threat\
# /var/vcap/packages/spark/conf/slaves \
# /var/vcap/packages/spark/conf/spark-env.sh
# $CASPIDAdir/lib/CaspidaSecurity.jar \
# $CASPIDAdir/conf/rules/generated/* \
# $CASPIDAdir/conf/rules/rule-site.properties \
# $CASPIDAdir/conf/rules/users/* \

TIMESTAMP_OptCaspida="0"

TMP=/var/vcap/sys/tmp     # UBA tmp
if touch $TMP/uhc >/dev/null 2>&1
then
        rm $TMP/uhc
else
        TMP="/tmp"
fi

TMP1file=$TMP/upc.check1.$$
TMP2file=$TMP/upc.check2.$$
TMP3file=$TMP/upc.check3.$$
TMP4file=$TMP/upc.check4.$$
SIZINGfile=$TMP/upc.sizing.$$
EXCfile=$TMP/upc.except_count.$$
EXLfile=$TMP/upc.except_list.$$
OBLfile=$TMP/upc.observation_list.$$
OUTfile=$TMP/upc.output.$$
LASTfile=$TMP/${BASEname}_last.txt
BGjobs=$TMP/upc.jobs.$$
MD5compare=$TMP/$BASEname_$VERSion.md5
IOPSfile=/var/vcap/sys/tmp/iops.data
IOPSmin=800

CPUTEST=~/upc.cputest.$$
DSH=~/upc.dsh.$$
DSINFO=~/upc.dsinfo.$$
IDINFO=~/upc.idinfo.$$

OSfile=$TMP/upc.$$.os_release
DBfile=$TMP/upc.$$.db.host
UBAfile=$TMP/upc.$$.uba_version
CONTENTfile=$TMP/upc.$$.uba_content

MEMBERout=$TMP/upc.nodeout.$$_
MEMBERwait=$TMP/upc.wait.$$

trap "rm -f $TMP1file $TMP2file $TMP3file  $TMP4file ${MEMBERout}* $MEMBERwait $DBfile $SIZINGfile $EXCfile $EXLfile $OBLfile $OUTfile $MD5compare $IOPSfile $BGjobs $CPUTEST $DSH $DSINFO $IDINFO $OSfile $UBAfile $CONTENTfile; exit" 0 2 15		# clean up temporary files

echo 0 > $EXCfile		# store count of exceptions


base64 -d > $CPUTEST <<EndOfData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EndOfData
chmod 500 $CPUTEST

base64 -d > $DSH <<EndOfData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EndOfData
chmod 500 $DSH

cat > $DSINFO <<EndOfData
U2FsdGVkX1/lvctc46nFfSVtaTWN+APp2Wb3ucYPN0EacI0dGCTgfQxPIAf4wcyL
TcmPYGpgQafdO+D4PzF5ZMrO8erpJHgF1m6zGwhfUqriw9boY4noBtrxvP2SWCZl
+NNuVmRX9+5wtfzlY5j3b/Q2QDWH20Dh2KX/8+e4Thsr7td/AUwu4bV9PG37R0ax
5RFggZGfn1wQfGzjTX2YkHd+gprb8g1L43fVOKl/iXGaIVtWdz2kdaDHivWiwh+r
G4MVSW4R61QOz68UQVv66OtfkcCo5wgQX143nXRoaJLPQNCL5NiTil5s7hDeEFnm
GhovQmtTlepM5zScVZGtuteLnLNsc+/ggwl4bBiSh3TDKSFy5gHMI3EL7C8FnCmB
rqzXPOlJoUHzwlgUpS3BhNYYnWq/WJOUtWBA3TE4O95q20fQn9m6x3lCe/DpJf3F
hEE2zhKybzMye2etLHiTSbnIn+9J/QDQNsdU7/J/VgHQrqDoU/UJbnPaTg3tgrWt
vNWycJ932XOYRoOJrKOmHOh2pz8FbAb22QgHCJ1SyJEI0o4ElwVjiOGZyRrPifk+
vKy/qonBm/ojRMBpSxrpHoZzBm/z26LSX0P2551ltoZ3cUjoDWSv/ScsvvTID3di
cavoZ7LWbRfgdsKQJT+94g7tARx8iV0A1gxa3s6iPCePorT7Ng4pZgJ0mBxT5f46
ojNoO89YrxvUOFpSX+0xO5HwxAwUgaxtOagbTG6jx46PAjNno9W7YurikqSZXVAn
dOOXrD7KD5Q9Pc5hLlyxFgH7eN4kjdjJe7iCdnp/ZjBL/VuyaVhGdtXehk782tCw
z+uAstAb8JI5J2hSowvpVds6Mznn0P6afmym7z5KmvxFobSPiNQplRBsV+32lVLq
EFjWk3FKOeMR3ZlpujtOt1ErpFCPekleqqlujVQGr72YKxbDlw0Tt6MT7z5MuNw0
JnljySa2dfzPJ3zxIcTZb2/3BOq8xfNQqrqrPzvwF1xFvJf7Qas8db4TxTvwDJ4R
rvweBi/D+4aJVvk/1I8otvqKgN1q1NW5xd95aRVZ/m8J4JwJwOHKiHak+HKuX12K
DCEfsLJEJF9Tn4P+5WpO1pPiauNrFx+6OUqbSPYbSS9G638v4pjiasft11aZtDup
tcp7Iq8gNhRfk+izNwwzzF+U5Zhw1K1zPSY4aso7L1LAEt28Dggx/NjbMG3EMY9E
FZb1iKmS3u8DXPaJBekgUZShFfWZaEKl35JawZMmVAdBy7qCXOoIU8MpLL4Tyf38
SLOrjjYx3PBCG4Iamj3s1IDvRW5o5X551O1TgXEaH3lscvwRkydovATCi5Ws0yrS
Z5InVuOlWxs5BEZsg/ihsQfEbomscAzgIyHxJ4CnBmMsGLc3G2ml6qpask8MqubU
Bn83vEYL8J/UEVlDWNonCRTH0Mdgx10YklkwAbBPn5HNepvCKhoyMzLmah2muwgJ
4xa4noT0DPovJ6s+ewUZEpCtsGnlzsd9UaqM+rQEYisaXqCn2+l0xKtCOgB+IiVA
OTRAg0kRevzV/WFDhwy2D8bk6Yf4sSwzuIW/gx1ZC6SfPIpJyV0WQ4f7ND2APMZK
CkwCXAKrHukkZerKq5kQHXGn/o3KSkzQT5CO/TK5yMdhGPk1Yl9spDc4jdUvFjpD
PdxYb1jN8j+/299UvcVooF7ZLqPf4TWnsPSGjvLUs+PPSXQfDkdmSqv5yKxut/M3
U/AkN4ZddBx1+dcg+RCkdSto+2cuHmnZr1hgfiZLAW0kyoc/UuS0iwHVlHIf0F7c
+tUkNXhugE+DHFisWrWRUORDkpm2O+8L0wpohMEK4Ywi4TmtNR19mHLhyPPB+vha
fkQ5p7FtncmcI5vVEFeq9rtGj+pK8GFqn8D7LZ/RXb+cWhMUNdi8oZV62OBpTdPf
MnplQ3HhxZyKv7yKrp9y9zCw5/poKZz9bw16bTdM7N8QfcPxWAyVPtECfSdPNupE
HPMEd+6B8vYtvILSEiC69gZSrLj//kqiGLSesCttyMpH8ckmobsuvYYHDc9VRjGw
5NQq0jVDEzWHEt12SzHmZtxKO08riaVkxeSz2DCDkNfqjfaspk/GWHLZFOjLH4pX
IXlrqNbc0ZjCEqM7FbaiTQx2erIovAhoHXbL0jRdD2qPi3GtZSlZsFOWvbjUSm6k
vVh4AfMgG2raeGU2+rj/5fOzIQweJw82nyYznEbj5ZDPH4H3ynbJduWYsBlDjaYY
uxTJmibw4q6rQrP6WLKM26oeAyetTzVI7t47n98r5+vbhywGDlTB4BVlimVNMHH2
bd2xPLPmMIcCbCfzpB+N6RSQljPiM85YAe08CBop/FdXVlbSbHIVRluhbTufTRGL
+ULU2NTVlWWzAsRFdwv7NTrv0zamM4Jv99cCxR1KQl+rFUxywQ7NXy03ibTYMm9k
EMyw/1rwne+6wYtrUcMaYkfZE5rb3Y+R5CuVvQKm+NuYfExPIWkMUryd7mQqSrK2
m2xX7zLjqRcfH0ecJnxP4T01znTSQdNcbxeHJWS+nLBy9t4cQi1/Y7q2nw9xXJh7
QqVFlQFmPIjt0jYhRjTiD10CBIB2uYWCH0PpZCtIAegsQw7M4/FtMRXrLm9UCXdo
G7xFsQKyz42oTlHodKf5cwlk1HGcCSCGV34aNEhAu2ZIz12ydphqv5Dzjzz//iZF
cGPzBtRyL4K3EpSCJq8cUww/LsmEdpnOw1j8tTZ4SrGiLOeCUmekyxxzSYlh4mzG
/ZbC4WaCMMNDzFpRHPS1AeK5R0+i8rBdInikS7Q9e6kn63iv6xxbnKLHiWhPiuIT
EndOfData
chmod 400 $DSINFO

cat > $IDINFO 2>/dev/null  <<EndOfData
U2FsdGVkX19EsvueY7hJzRexw3G5gd8AbKWzuUyUPlpkb8TzZoLrJxOyx02jSh7e
e8BopP5sFL+pOREuiA2e76JvZJSzzWTzyGJzFrWRWlMr3SngLJfvY/qbseUxETcp
/X4I8bc9NEVoKlE3RmDZcQ+nzHgq2oIXMrpJiwa8KzIoMZYuSDwVX3YvITWzPUSh
WXvVmWra7sFtmMd4JHUfQbng5VZRZ+KlkLbdmFiPk2hsZyYWBicpwY56YWOn98Sw
SyVEK2a5DGMe2BEBtnWwsg0os6RHCLYx/+QncCzbpM7hmY2xqeIFxHVAi1pA4KU/
vLVjf0JTf5viU8SibX8t4U64XQOi61bcTWipQmPpT3XbnWkbCNNjL38yl/eBz34f
BC3/lGFyZ/cIcxiVqUmoE9JbS31JrtiMYN4z3yhS4IknAiNPTFWe/WnrFE/kThbI
alU7NvHCTzgLPNUQEd8RD5Zci1Awd54eOQZGB0dqmd627UJkyPBaIIaYYFP4djEn
b03WW+u/LbaGK3HzOiOePHml2nHozwoe8siPyzk+//XQRg8AQz7s6WKkWwiZZN4d
lCYo8iEnNy7Mr7FeYpWviDG6MGcZTRTjOh1aEzdNzmwNenb7diDgWHWOnAvxsD8f
VNCMhxe4O6U8UlqPoewGCuFinuXSehH1MYlpzYCchDqVslDJEpZkFB/KcOTGRIcZ
jejWkOUQFTfqqhb5NO8j7IH+PrpPeLUW3jCNjNHtJ4czcP7sQ/WgdQLojBkyFAvK
kpODWlkZS0/qKVn9RnEG5a1f6506ko5zJ3fbzAVM4gZCvCKm2AQAiyagR8EQ856/
fZLOrjDQAYe2Qdz6x7dXQmkRAtjU2xUrNApcIzRzS0DWrp84hL4lewWJc1Y98T0n
lVY8suAmOOY7eNTPwpw8KI6OGgTyagN+m8GgsQlKgaKycxiEdOl0TpGmivdplsyt
XDQo+jwvaJudLAXAb2dndZjGSSFHrVJXkKym9InGA2hiXj8m8zgm9Xon/gbJIsaG
zV3Oixy0ZrR9FME1EYTKgnQFVmlfnZIWANa/oFYgjkHgQnRibiHyrrCrylEbCue3
qhIfhH0+hf6hdg4XeGESIGndpqGvEq9n7o7B4bPuusjwnBCehv0q8cAlXOlaf9/u
wR7BPBNDZTPF0RwlZER0fgeYprOgPbgIzcTyFRrIARpKwU6ne2QWoUR4ldyxwiMg
/8DqcZPTMIHip84e3V+EDYxB0ppKjA26OZdMURmMGzcWqNa5MblShA8VIc0dRteQ
k4aFVQbhBqHdiHuLO7Ky8Z2ZN7STVEqMDGOXy19P6PQR05T++zjz24dSfbgpSVfx
uVk7MwN4Q1iqaMvWt5PAc3b5Bd291/Ef3sc/xXG4WM0bkorR6CFn2j0VR7pej2Lh
7TNHRB0E1b6aeZHBOUtfzXfLDYuzToFArosrEetyfDYcX4cZpk/ztodPGziyR+C9
p81hrQwWuK8u9mscHWFbVPbGzUrpI1iVMYxozb0d1hGZ5Y/slYASwFHldN/ZJvm1
X3mGwd6gxfLl+aq5MTHXX+hPLTdCSNguMShnt+YPMH7F88pSUBnuNHt5QotEMw+H
EndOfData
chmod 400 $IDINFO 2>/dev/null

#
#	get UBA version info
#

UBA_VERSION () {

	UBA_DEFAULT_PROPERTIES=$CASPIDAdir/conf/caspida-default.properties	# defaults
	UBA_ENV_PROPERTIES=""			# blueprint deployment in 2.4
	UBA_SITE_PROPERTIES=$CASPIDAdir/conf/caspida-site.properties		# customization

	> $UBAfile						# start with empty file

        if test -f $CASPIDAdir/conf/version.properties
        then
                echo
                UBAversion=`grep release-version $CASPIDAdir/conf/version.properties | cut -d "=" -f 2`
		echo "$UBAversion" > $UBAfile
                reqDB="`echo $UBAversion | sed -e 's/^\([0-9\.]*\)-.*$/\1/'`"
                reqDB="`echo $reqDB | sed -e 's/\.0$//'`"
        
		UBAvers=`echo $UBAversion | sed -e 's/\-.*$//' -e 's/\.0$//'`		# remove for version checking
		NECHO "UBAvers:" "$UBAvers"; echo

		if test -n "$UBAvers"
		then
			case $UBAvers in
				$uba24version)
					uba24feature=true;;
				$uba30version)
					uba24feature=true;
					uba30feature=true;;
				$uba301version)
					uba24feature=true;
					uba30feature=true;
					uba301feature=true;;
				$uba32version)
					uba24feature=true;
					uba30feature=true;
					uba301feature=true;
					uba32feature=true;;
				$uba321version)
					reqDB="3.2";			# exception to minor upgrade logic
					uba24feature=true;
					uba30feature=true;
					uba301feature=true;
					uba32feature=true;
					uba321feature=true;;
				$uba33version)
					uba24feature=true;
					uba30feature=true;
					uba301feature=true;
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;;
				$uba34version)
					uba24feature=true;
					uba30feature=true;
					uba301feature=true;
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;;
				$uba40version)
					uba24feature=true;
					uba30feature=true;
					uba301feature=true;
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;;
				$uba4001version)
					reqDB="4.0";			# exception to minor upgrade logic
					uba24feature=true;
					uba30feature=true;
					uba301feature=true;
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;
					uba4001feature=true;;
				$uba4002version)
					reqDB="4.0";			# exception to minor upgrade logic
					uba24feature=true;
					uba30feature=true;
					uba301feature=true;
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;
					uba4001feature=true;
					uba4002feature=true;;
 				$uba41version)
 					reqKAFKA="2.11-0.11.0.1";
 					uba24feature=true;
 					uba30feature=true;
 					uba301feature=true;
 					uba32feature=true;
 					uba321feature=true;
 					uba33feature=true;
 					uba34feature=true;
 					uba40feature=true;
 					uba4001feature=true;
 					uba4002feature=true;
 					uba41feature=true;;
 				$uba411version)
 					reqKAFKA="2.11-0.11.0.1";
					reqDB="4.1";			# exception to minor upgrade logic
 					uba24feature=true;
 					uba30feature=true;
 					uba301feature=true;
 					uba32feature=true;
 					uba321feature=true;
 					uba33feature=true;
 					uba34feature=true;
 					uba40feature=true;
 					uba4001feature=true;
 					uba4002feature=true;
 					uba41feature=true;
 					uba411feature=true;;
 				$uba412version)
 					reqKAFKA="2.11-0.11.0.1";
					reqDB="4.1";			# exception to minor upgrade logic
 					uba24feature=true;
 					uba30feature=true;
 					uba301feature=true;
 					uba32feature=true;
 					uba321feature=true;
 					uba33feature=true;
 					uba34feature=true;
 					uba40feature=true;
 					uba4001feature=true;
 					uba4002feature=true;
 					uba41feature=true;
 					uba411feature=true;
 					uba412feature=true;;
 				$uba413version)
 					reqKAFKA="2.11-0.11.0.1";
					reqDB="4.1";			# exception to minor upgrade logic
 					uba24feature=true;
 					uba30feature=true;
 					uba301feature=true;
 					uba32feature=true;
 					uba321feature=true;
 					uba33feature=true;
 					uba34feature=true;
 					uba40feature=true;
 					uba4001feature=true;
 					uba4002feature=true;
 					uba41feature=true;
 					uba411feature=true;
 					uba412feature=true;
 					uba413feature=true;;
 				$uba42version)
 					reqKAFKA="2.11-0.11.0.1";
 					uba24feature=true;
 					uba30feature=true;
 					uba301feature=true;
 					uba32feature=true;
 					uba321feature=true;
 					uba33feature=true;
 					uba34feature=true;
 					uba40feature=true;
 					uba4001feature=true;
 					uba4002feature=true;
 					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
 					uba42feature=true;;
 				$uba421version)
 					reqKAFKA="2.11-0.11.0.1";
 					uba24feature=true;
 					uba30feature=true;
 					uba301feature=true;
 					uba32feature=true;
 					uba321feature=true;
 					uba33feature=true;
 					uba34feature=true;
 					uba40feature=true;
 					uba4001feature=true;
 					uba4002feature=true;
 					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
					uba42feature=true;
 					uba421feature=true;;
 				$uba422version)
 					reqKAFKA="2.11-0.11.0.1";
 					uba24feature=true;
 					uba30feature=true;
 					uba301feature=true;
 					uba32feature=true;
 					uba321feature=true;
 					uba33feature=true;
 					uba34feature=true;
 					uba40feature=true;
 					uba4001feature=true;
 					uba4002feature=true;
 					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
					uba42feature=true;
					uba421feature=true;
 					uba422feature=true;;
 				$uba423version)
 					reqKAFKA="2.11-0.11.0.1";
 					uba24feature=true;
 					uba30feature=true;
 					uba301feature=true;
 					uba32feature=true;
 					uba321feature=true;
 					uba33feature=true;
 					uba34feature=true;
 					uba40feature=true;
 					uba4001feature=true;
 					uba4002feature=true;
 					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
					uba42feature=true;
					uba421feature=true;
					uba422feature=true;
 					uba423feature=true;;
 				$uba43version)
 					reqKAFKA="2.11-0.11.0.1";
 					uba24feature=true;
 					uba30feature=true;
 					uba301feature=true;
 					uba32feature=true;
 					uba321feature=true;
 					uba33feature=true;
 					uba34feature=true;
 					uba40feature=true;
 					uba4001feature=true;
 					uba4002feature=true;
 					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
 					uba42feature=true;
 					uba421feature=true;
 					uba422feature=true;
 					uba423feature=true;
 					uba43feature=true;;
 				$uba431version)
 					reqKAFKA="2.11-0.11.0.1";
 					uba24feature=true;
 					uba30feature=true;
 					uba301feature=true;
 					uba32feature=true;
 					uba321feature=true;
 					uba33feature=true;
 					uba34feature=true;
 					uba40feature=true;
 					uba4001feature=true;
 					uba4002feature=true;
 					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
 					uba42feature=true;
 					uba421feature=true;
 					uba422feature=true;
 					uba423feature=true;
 					uba43feature=true;
 					uba431feature=true;;
 				$uba4311version)
 					reqKAFKA="2.11-0.11.0.1";
 					uba24feature=true;
 					uba30feature=true;
 					uba301feature=true;
 					uba32feature=true;
 					uba321feature=true;
 					uba33feature=true;
 					uba34feature=true;
 					uba40feature=true;
 					uba4001feature=true;
 					uba4002feature=true;
 					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
 					uba42feature=true;
 					uba421feature=true;
 					uba422feature=true;
 					uba423feature=true;
 					uba43feature=true;
 					uba431feature=true;
 					uba4311feature=true;;
 				$uba432version)
 					reqKAFKA="2.11-0.11.0.1";
 					uba24feature=true;
 					uba30feature=true;
 					uba301feature=true;
 					uba32feature=true;
 					uba321feature=true;
 					uba33feature=true;
 					uba34feature=true;
 					uba40feature=true;
 					uba4001feature=true;
 					uba4002feature=true;
 					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
 					uba42feature=true;
 					uba421feature=true;
 					uba422feature=true;
 					uba423feature=true;
 					uba43feature=true;
 					uba431feature=true;
 					uba4311feature=true;
 					uba432feature=true;;
 				$uba433version)
 					reqKAFKA="2.11-0.11.0.1";
 					uba24feature=true;
 					uba30feature=true;
 					uba301feature=true;
 					uba32feature=true;
 					uba321feature=true;
 					uba33feature=true;
 					uba34feature=true;
 					uba40feature=true;
 					uba4001feature=true;
 					uba4002feature=true;
 					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
 					uba42feature=true;
 					uba421feature=true;
 					uba422feature=true;
 					uba423feature=true;
 					uba43feature=true;
 					uba431feature=true;
 					uba4311feature=true;
 					uba432feature=true;
 					uba433feature=true;;
 				$uba434version)
 					reqKAFKA="2.11-0.11.0.1";
 					uba24feature=true;
 					uba30feature=true;
 					uba301feature=true;
 					uba32feature=true;
 					uba321feature=true;
 					uba33feature=true;
 					uba34feature=true;
 					uba40feature=true;
 					uba4001feature=true;
 					uba4002feature=true;
 					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
 					uba42feature=true;
 					uba421feature=true;
 					uba422feature=true;
 					uba423feature=true;
 					uba43feature=true;
 					uba431feature=true;
 					uba4311feature=true;
 					uba432feature=true;
 					uba433feature=true;
 					uba434feature=true;;
 				$uba501version)
 					reqKAFKA="2.11-0.11.0.1";
 					uba24feature=true;
 					uba30feature=true;
 					uba301feature=true;
 					uba32feature=true;
 					uba321feature=true;
 					uba33feature=true;
 					uba34feature=true;
 					uba40feature=true;
 					uba4001feature=true;
 					uba4002feature=true;
 					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
 					uba42feature=true;
 					uba43feature=true;
 					uba431feature=true;
 					uba4311feature=true;
 					uba432feature=true;
 					uba433feature=true;
 					uba434feature=true;
                                        uba50feature=true;
                                        uba501feature=true;;
 				$uba502version)
 					reqKAFKA="2.11-0.11.0.1";
 					uba24feature=true;
 					uba30feature=true;
 					uba301feature=true;
 					uba32feature=true;
 					uba321feature=true;
 					uba33feature=true;
 					uba34feature=true;
 					uba40feature=true;
 					uba4001feature=true;
 					uba4002feature=true;
 					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
 					uba42feature=true;
 					uba43feature=true;
 					uba431feature=true;
 					uba4311feature=true;
 					uba432feature=true;
 					uba433feature=true;
 					uba434feature=true;
                                        uba50feature=true;
                                        uba501feature=true;
                                        uba502feature=true;;
				$uba503version)
 					reqKAFKA="2.11-0.11.0.1";
 					uba24feature=true;
 					uba30feature=true;
 					uba301feature=true;
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;
					uba4001feature=true;
					uba4002feature=true;
					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
					uba42feature=true;
					uba421feature=true;
					uba42f2eature=true;
					uba42f3eature=true;
					uba43feature=true;
					uba431feature=true;
					uba432feature=true;
					uba433feature=true;
					uba434feature=true;
					uba50feature=true;
					uba501feature=true;
					uba502feature=true;
					uba503feature=true;;
				$uba504version)
 					reqKAFKA="2.11-0.11.0.1";
 					uba24feature=true;
 					uba30feature=true;
 					uba301feature=true;
					uba32feature=true;
					uba321feature=true;
					uba33feature=true;
					uba34feature=true;
					uba40feature=true;
					uba4001feature=true;
					uba4002feature=true;
					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
					uba42feature=true;
					uba421feature=true;
					uba42f2eature=true;
					uba42f3eature=true;
					uba43feature=true;
					uba431feature=true;
					uba432feature=true;
					uba433feature=true;
					uba434feature=true;
					uba50feature=true;
					uba501feature=true;
					uba502feature=true;
					uba503feature=true;
					uba504feature=true;;
                                $uba505version)
 					reqKAFKA="2.11-0.11.0.1";
 					uba24feature=true;
 					uba30feature=true;
 					uba301feature=true;
                                        uba32feature=true;
                                        uba321feature=true;
                                        uba33feature=true;
                                        uba34feature=true;
                                        uba40feature=true;
                                        uba4001feature=true;
                                        uba4002feature=true;
                                        uba41feature=true;
                                        uba411feature=true;
                                        uba412feature=true;
                                        uba413feature=true;
                                        uba42feature=true;
                                        uba421feature=true;
                                        uba42f2eature=true;
                                        uba42f3eature=true;
                                        uba43feature=true;
                                        uba431feature=true;
                                        uba432feature=true;
                                        uba433feature=true;
                                        uba434feature=true;
                                        uba50feature=true;
                                        uba501feature=true;
                                        uba502feature=true;
                                        uba503feature=true;
                                        uba504feature=true;
                                        uba505feature=true;;
                                $uba506version)
 					reqKAFKA="2.11-0.11.0.1";
 					uba24feature=true;
 					uba30feature=true;
 					uba301feature=true;
                                        uba32feature=true;
                                        uba321feature=true;
                                        uba33feature=true;
                                        uba34feature=true;
                                        uba40feature=true;
                                        uba4001feature=true;
                                        uba4002feature=true;
                                        uba41feature=true;
                                        uba411feature=true;
                                        uba412feature=true;
                                        uba413feature=true;
                                        uba42feature=true;
                                        uba421feature=true;
                                        uba42f2eature=true;
                                        uba42f3eature=true;
                                        uba43feature=true;
                                        uba431feature=true;
                                        uba432feature=true;
                                        uba433feature=true;
                                        uba434feature=true;
                                        uba50feature=true;
                                        uba501feature=true;
                                        uba502feature=true;
                                        uba503feature=true;
                                        uba504feature=true;
                                        uba505feature=true;
                                        uba506feature=true;;
                                $uba51version)
 					reqKAFKA="2.11-0.11.0.1";
 					uba24feature=true;
 					uba30feature=true;
 					uba301feature=true;
                                        uba32feature=true;
                                        uba321feature=true;
                                        uba33feature=true;
                                        uba34feature=true;
                                        uba40feature=true;
                                        uba4001feature=true;
                                        uba4002feature=true;
                                        uba41feature=true;
                                        uba411feature=true;
                                        uba412feature=true;
                                        uba413feature=true;
                                        uba42feature=true;
                                        uba421feature=true;
                                        uba42f2eature=true;
                                        uba42f3eature=true;
                                        uba43feature=true;
                                        uba431feature=true;
                                        uba432feature=true;
                                        uba433feature=true;
                                        uba434feature=true;
                                        uba50feature=true;
                                        uba501feature=true;
                                        uba502feature=true;
                                        uba503feature=true;
                                        uba504feature=true;
                                        uba505feature=true;
                                        uba506feature=true;
                                        uba51feature=true;;
 				$uba50version|*)
 					reqKAFKA="2.11-0.11.0.1";
 					uba24feature=true;
 					uba30feature=true;
 					uba301feature=true;
 					uba32feature=true;
 					uba321feature=true;
 					uba33feature=true;
 					uba34feature=true;
 					uba40feature=true;
 					uba4001feature=true;
 					uba4002feature=true;
 					uba41feature=true;
					uba411feature=true;
					uba412feature=true;
					uba413feature=true;
 					uba42feature=true;
 					uba43feature=true;
 					uba431feature=true;
 					uba4311feature=true;
 					uba432feature=true;
 					uba433feature=true;
 					uba434feature=true;
                                        uba50feature=true;;
			esac	
		fi

		if $uba32feature
		then
			UBA_DEFAULT_PROPERTIES=$CASPIDAdir/conf/uba-default.properties	# defaults
			UBA_ENV_PROPERTIES=$CASPIDAdir/conf/uba-env.properties			# blueprint deployment in 2.4
			UBA_SITE_PROPERTIES=/etc/caspida/local/conf/uba-site.properties		# customization
		fi
                if $uba41feature
                then
                        UBA_TUNING_PROPERTIES=/etc/caspida/local/conf/deployment/uba-tuning.properties
                else
                        UBA_TUNING_PROPERTIES=""
                fi
		SITE_PROPERTIES=`basename $UBA_SITE_PROPERTIES`
		UBA_PROPERTIES="$UBA_DEFAULT_PROPERTIES $UBA_ENV_PROPERTIES $UBA_TUNING_PROPERTIES $UBA_SITE_PROPERTIES"
		CHECKlist="$CHECKlist $UBA_PROPERTIES"
		ETLdir=`dirname $UBA_SITE_PROPERTIES`/etl/
		CHECKlist="$CHECKlist $ETLdir/configuration/EntityValidations.json $ETLdir/morphlines/include/global.conf $ETLdir/morphlines/static/* $ETLdir/setup/type_definitions.json"
		if $uba42feature
		then
			CHECKlist="$CHECKlist $CASPIDAdir/conf/kafka/kafka.properties"
		else
			CHECKlist="$CHECKlist $CASPIDAdir/conf/kafka.properties"
		fi
		if $uba32feature
		then
			CHECKlist="`find /etc/caspida/local $CASPIDAdir -type f -print | egrep -ve '/rules/|/content/|/analytics.stage/|/content.history/|.pdf\$|.pyc\$|nohup.out\$'`"				# much easier because of sync-cluster
		fi
	fi

}


#
#       display datasource configuration
#

DISPLAY_DATASOURCE () {
        DS_ID=$1

        export IOd="datasources"

        DS_NAME=`psql -d caspidadb -t -c "select name from datasources where id='$DS_ID';" 2>/dev/null | sed -e 's/^\s\s*//' -e 's/\s\s*$//' -e '/^$/d'`


        if which openssl >/dev/null 2>&1
        then
                ( exec $DSH $IDINFO $DS_ID ) > $TMP3file 2>&1   # get datasource query

                DSusername=`grep '"userName"' $TMP3file |  cut -d '"' -f 4`
                DSusername=`TRANSLATE "$DSusername"`

                DSrecipients=`grep '"recipients"' $TMP3file |  cut -d '"' -f 4`
                DSrecipients=`TRANSLATE "$DSrecipients"`

                sed -e '/"userName"/ s/": "[^"][^"]*/": "notDisplayable/' -e '/"recipients"/ s/": "[^"][^"]*/": "notDisplayable/' \
                        -e '/"query":/ s/y": ".*",$/y": "seeFormattedBelow",/' \
                        -e '/"query":/ s/y": ".*"$/y": "seeFormattedBelow"/' \
                        -e "/userName/ s/notDisplayable/$DSusername/" -e "/recipients/ s/notDisplayable/$DSrecipients/" < $TMP3file > $TMP4file
        else
                echo "   <= openssl not installed" > $TMP4file
        fi

        if ! test -s $TMP4file
        then
                echo "   <= datasource was not retrieved; could be blocked" > $TMP4file
        fi
        echo
        NECHO "${DS_NAME} conf:" "`head -1 $TMP4file`"; echo
        ALL_BUT_FIRST $TMP4file

        grep '"query":' < $TMP3file  |\
	sed -e 's/\\"/"/g' -e 's/\\n/\n/g' -e 's/\\\\/\\/g' -e 's/^\s\s*"query": "//' -e 's/"$//' -e 's/,\([^\s]\)/,        \1/g' |\
	fold -s -w 120 |\
	sed -e 's/,        /,/g' -e 's/^\s\s*//' > $TMP4file
        if test -s $TMP4file
        then
                echo
                NECHO "${DS_NAME} query:" "`head -1 $TMP4file`"; echo
                ALL_BUT_FIRST $TMP4file
        fi
        > $TMP3file
}

#
#	display datasource query	
#

DISPLAY_DATASOURCE_QUERY () {

	DS_LABEL=$1
	DS_NAME=$2

	if which openssl >/dev/null 2>&1
	then
		export uba42feature
		( exec $DSH $DSINFO $DS_NAME ) > $TMP3file 2>/dev/null
		grep query $TMP3file | sed -e 's/^[^:]*://' -e "s/\([^|]\)|/\1\n|/g" | fmt -s -w 120  
	else
		echo "   <= openssl not installed"
	fi >$TMP4file
	if test -s $TMP4file
	then
		echo
		NECHO "${DS_LABEL}:" "`head -1 $TMP4file`"; echo
		ALL_BUT_FIRST $TMP4file
	fi
	> $TMP3file
}

#
#	display datasource endpoint	
#

DISPLAY_DATASOURCE_ENDPOINT () {

	DS_NAME=$1

        $DSH $DSINFO $DS_NAME > $TMP3file 2>&1    # get datasource info

        export uba42feature

	grep 'endpoint:' $TMP3file | sed -e 's/^[^:]*://' | cut -d ' ' -f 1 
	> $TMP3file
}

#
# sub-routine to ouput a label and value without a new line
#

NECHO() {
	NEtag="$1"
	NEval="$2"

	echo "$NEtag" | awk '{ printf "    %-24.24s ", $0}'	# note: no newline
	echo -n "$NEval"		# note: no newline
}

#
#	sub-routine to verfify name resolution of a node, forward and reverse
#

LOOKUP() {
	HOST=$1
	IPhosts=""
	IPdns=""

	egrep -ve "^127" /etc/hosts > $TMP3file		# ignore local network references
	if  egrep -i "\b$HOST\b" $TMP3file > $TMP1file 2>/dev/null
	then
		grep -v "^#" $TMP1file | tail -1 > $TMP2file	# have to assume that the last one is correct
		IPhosts=`sed 's/[ \t].*$//' < $TMP2file`	# will use $TMP2file later
	else
		> $TMP2file
	fi

	if which timeout >/dev/null 2>&1
	then
		TIMEOUT="timeout 15"	# ideally, want to timeout DNS queries
	else
		TIMEOUT=""
	fi

	if ! $USEip
	then
		STARTsecs=`date "+%s"`
		if $NSLOOKUP
		then
			LOOKprog="nslookup"
			$TIMEOUT nslookup $HOST > $TMP1file 2>/dev/null

			FAIL=`tail -3 $TMP1file | grep "^** server" 2>/dev/null | egrep "SERVFAIL|NXDOMAIN"`
		else
			> $TMP1file
			if which getent >/dev/null 2>&1
			then
				LOOKprog="getent"
				IPdns=`$TIMEOUT getent --service=dns ahostsv4 $HOST 2>/dev/null | cut -d " " -f 1 | sort -u | head -1`
				if ! test -z "$IPdns"
				then
					echo "Address $IPdns" > $TMP1file		# expected from nslookup
				else
					FAIL="no value from getent"
				fi
			else
				LOOKprog="no_lookup"
				FAIL="no nslookup/getent"
			fi
		fi
		STOPsecs=`date "+%s"`
        	ELAPSEDsecs=`expr $STOPsecs - $STARTsecs`
		NECHO "${LOOKprog}:" "    ($ELAPSEDsecs seconds)"; echo

		if ! test -z "$FAIL" 
		then
			if test -z "$IPhosts"
			then
				NECHO "/etc/hosts:" "'$HOST'   <== not found in /etc/hosts"; echo
				INCREMENT_EXCEPTIONS
				NECHO "dns lookup:" "$FAIL   <== dns required without hosts entry"; echo	# name not found in /etc/hosts or DNS
				INCREMENT_EXCEPTIONS
			else
				if ! egrep -iq "\b$HOST\." $TMP2file 
				then
					NECHO "dns lookup:" "$HOST"
					if echo "$HOST" | grep -q '.'
					then
						echo "   ($FAIL)"
					else
						echo "   (FQDN may be needed for DNS ($FAIL))"
					fi
				fi
			fi
		else
			NECHO "hosts:" "`cat $TMP2file`"; echo "   ($HOST)"		
			IPdns=`grep "^Address" $TMP1file | grep -v "#" | tail -1 | cut -d ' ' -f 2`
			NECHO "DNS:" "'$IPdns'"
			#		cannot do reverse lookup without nslookup
			if $NSLOOKUP
			then
				STARTsecs=`date "+%s"`
				if ! $TIMEOUT nslookup "$IPdns" > /dev/null 2>&1
				then
					echo "   (reverse lookup failed)"
				else
					echo
				fi
				STOPsecs=`date "+%s"`
                		ELAPSEDsecs=`expr $STOPsecs - $STARTsecs`
                		NECHO "nslookup:" "    ($ELAPSEDsecs seconds)"; echo
				if ! test -z "$IPhosts"
				then
					if ! test "$IPhosts" = "$IPdns"
					then
						NECHO "" "   <== files '$IPhosts' and dns '$IPdns' IP addresses don't match"
						INCREMENT_EXCEPTIONS
					fi
				fi
				echo
			else
				echo "   (without reverse lookup)"
			fi
		fi

		if test -z "$IPhosts" && test -z "$IPdns"
		then
			NECHO "/etc/hosts:" "'$HOST'   <== not found in /etc/hosts and DNS cannot be confirmed"; echo
                	INCREMENT_EXCEPTIONS
		fi
	fi
}

#
#	sub-routine to obtain TLS protocol from data sources
#
#	TLS version should be consistent
#

PROTO_CHECK () {
	SPLUNKserver="$1"

        echo
        SPLUNKserverNoPort=`echo "$SPLUNKserver" | sed -e 's/:..*$//'`
        LOOKUP $SPLUNKserverNoPort
        echo

	if which timeout >/dev/null 2>&1
	then
		TIMEOUT="timeout 15"
	else
		TIMEOUT=""
	fi

	TLScheck=`echo "$PROTOCOLuba" | tr '[:upper:]' '[:lower:]' | sed -e 's/\./_/' -e 's/v//'`
	if ! test -z "$TLScheck"
	then
		TLScheck="-$TLScheck"
	fi

	SSL="-ssl3"
	if $uba33feature
	then
		SSL=""
	fi

	if which openssl >/dev/null 2>&1
	then
		echo "openssl s_client -connect $SPLUNKserver $SSL $TLScheck" | sed -e "s/^/$INDENT/"
	fi

	NECHO "" "$SPLUNKserver"
	if which openssl >/dev/null 2>&1
	then
		PROTOCOLsplunk=`$TIMEOUT openssl s_client -connect $SPLUNKserver $SSL $TLScheck </dev/null 2>/dev/null | tee $TMP4file | grep "Protocol" | tr -d ' ' | cut -d ':' -f 2` 
		CIPHERsplunk="`grep "Cipher    :" $TMP4file | tr -d ' ' | cut -d ':' -f 2`"

		if test -z "$PROTOCOLsplunk"
		then
			PROTOCOLsplunk="'Protocol' not returned for verification"
		fi

		echo -n "   '$PROTOCOLsplunk'"
		if (echo "$PROTOCOLsplunk" | grep -q "$PROTOCOLuba")
		then
			echo " '$CIPHERsplunk'"
		else
			echo "   <==  TLS version on Splunk server does not match '$PROTOCOLuba'; $UBA_SITE_PROPERTIES needs proper value and needs to be verified by other means"
			INCREMENT_EXCEPTIONS
		fi
	else
		echo "   <== openssl not installed"
		INCREMENT_EXCEPTIONS
	fi
}

GET_CASPIDA_PROPERTY () {
		PROPERTYarg=${1-missing}
		PROPERTYvalue=`egrep -he "^$PROPERTYarg\s*=" $UBA_PROPERTIES 2>/dev/null |\
		tail -1 | sed -e 's/\s*=\s*/=/'| cut -d '=' -f 2`
}


CHECK_DEPLOYMENT () {
	SERVICE=${1-missing}
	NODE=${2-$THISnode}
	IP=${3-$THISip}

	if test -f $CASPIDAdir/conf/deployment/caspida-deployment.conf
	then 
        	egrep "^$SERVICE\b" $CASPIDAdir/conf/deployment/caspida-deployment.conf > $TMP3file 2>/dev/null
        	if egrep -iq "\b$NODE\b" $TMP3file
        	then
                	FOUND=true
		else
			FOUND=false
			if egrep -iq "\b$IP\b" $TMP3file
			then
				FOUND=true
			fi
        	fi	
		if test -s $TMP3file
		then
			DEPLOYMENTlist=`cut -d '=' -f 2 < $TMP3file`
		else
			DEPLOYMENTlist="localhost"		# use 'localhost' as a default
		fi
	else
		FOUND=false	# without deployment, everything assumed to be false 
		DEPLOYMENTlist=""
	fi

	if ! $CASPIDA_PROPERTIES_EXIST
	then
		# all bets off until 'setup' has been run

		FOUND=false
		DEPLOYMENTlist=""	
	fi
}

#
#	new 3.0 feature is deployment sizing 'tunables' file
#

CHECK_TUNABLES() {
	TUNABLEsection=${1-missing}
	TUNABLEarg=${2-missing}
	CLUSTERnodes=${3-$NODEcount}

	if test "$TUNABLEarg" = "workersHint"
	then
		DELIM=":"
		DELchar='" ,'
	else
		DELIM="="
		DELchar='"'
	fi

	TUNABLEvalue=""

#	if test "$TUNABLEsection" = "uba-tuning.properties"
#	then
#		TUNABLEfile=/etc/caspida/local/conf/deployment/uba-tuning.properties 
#		if test -r $TUNABLEfile
#		then
#			sed -ne "/^#BEGIN Tunables/,/^#END Tunables/p" < $TUNABLEfile |\
#			egrep "$TUNABLEarg[^\.]" | tr -d "$DELchar" > $TMP3file
#			TUNABLEvalue=`sed -e "s/^[^$DELIM]*$DELIM//" < $TMP3file`
#		fi
#	fi

	if test "$TUNABLEvalue" = ""
	then
		TUNABLEfile=$CASPIDAdir/conf/deployment/recipes/caspida/caspidatunables-${CLUSTERnodes}_node.conf
		if test -r $TUNABLEfile
		then
			sed -ne "/^#BEGIN $TUNABLEsection/,/^#END $TUNABLEsection/p" < $TUNABLEfile |\
			egrep "$TUNABLEarg[^\.]" | tr -d "$DELchar" > $TMP3file
			TUNABLEvalue=`sed -e "s/^[^$DELIM]*$DELIM//" < $TMP3file`
		else
			TUNABLEvalue=""
		fi
	fi

# 	echo "section: $TUNABLEsection; arg: $TUNABLEarg; nodes: ${CLUSTERnodes}; value: $TUNABLEvalue"
}

#
#	sub-routine to maintain count of exceptions
#
#	stored in a file because sub-shell can not update global variable
#

INCREMENT_EXCEPTIONS () {
	INCREMENT=${1-1}	# by default, increment by one
	EXCEPTIONS=`cat $EXCfile`
	EXCEPTIONS=`expr $EXCEPTIONS + $INCREMENT`
	echo $EXCEPTIONS > $EXCfile
}

#
#	sub-routine to indent remaining lines of a file
#

ALL_BUT_FIRST () {
	FILEname=$1

	LINEcount=`wc -l < $FILEname`	
	if test $LINEcount -gt 1
	then
		LINEcount=`expr $LINEcount - 1`
		tail -$LINEcount $FILEname | sed -e "s/^/$INDENT/"
	fi
}

#
#       obscure PII
#

SET_SEED() {
        CHARset='#1Aa2Bb3Cc4Dd5Ee6Ff7Gg8Hh9Ii0JjKkLlMmNnOoPpQqRrSsTtUuVvWwXxYyZz'

        if which shuf >/dev/null 2>&1
        then
                SEED=`shuf -e a b c d e f g h i j k l m n o p q r s t u v w x y z A B C D E F G H I J K L M N O P Q R S T U V W X Y Z 0 1 2 3 4 5 6 7 8 9 '#'  |
 tr -d '\n'`
        else
                SEED='YDPOy5IuKVWRa76h8CFXrq1w4GibkdEfS#cgLQJUxjnNZv30oAt9Mmspelz2TBH'
        fi

        RANDOM=`echo 'echo $RANDOM' | bash`
        ROTATE=`expr $RANDOM \% 23 + 1`
}

SHUFFLE() {
        direction=$1

        case $direction in
                in) echo "$SEED" | sed -e "s/^\(.\{$ROTATE\}\)\(.*\)/\2\1/g";;
                 *) echo "$SEED";;
        esac
}

TRANSLATE() {
        FROM=$1

        TO=`echo "$FROM" | tr "$CHARset" "$INPUTstring" | tr '[:upper:]' '[:lower:]' | tr "$CHARset" "$OUTPUTstring" | tr '[:upper:]' '[:lower:]' | tr "$CHARset" "$INPUTstring" | tr '[:upper:]' '[:lower:]' `
        echo "$TO"
}



#
#	compute sequential IOPS 
#

IOPS() {
        OPERATION=${1}
        BLOCKsize=${2}
        REPORTfile=${3}
	PART=${4}

        grep copied $REPORTfile |\
        sed -e 's/ bytes.* s, / /' |\
        while read BYTES RATE UNIT
        do
#               echo "bytes: $BYTES; rate: $RATE; unit: $UNIT"
                case "$UNIT" in
                        [mM]*) CONVERSION=1024; UNIT="KB/s";;
                        [gG]*) CONVERSION=`expr 1024 \* 1024`; UNIT="KB/s";;
			[kK]*) CONVERSION=1;;
                esac
                RATE=`echo $RATE $CONVERSION | awk '{print ($1 * $2)}'`
#               echo "bytes: $BYTES; rate: $RATE; unit: $UNIT"

                IOPS=`echo $RATE $BLOCKsize | awk '{printf "%d", ($1 / $2)}'`
                NECHO "iops $OPERATION:" "sequential (${BLOCKsize}k): $IOPS"
		if test "$IOPS" -gt  $IOPSmin
		then
			echo
		else
			echo "  (zookeeper latency on disk $PART may need to be monitored)"
			sed -e "s/^/$INDENT/" < $REPORTfile
		fi
        done
}


CHECK_FOR_UPDATE() {

#
#	Before doing anything more, check to see if there is an updated version of script on Box
#

if ! test "$TTY" = "not a tty"
then
	echo "checking for latest version" > /dev/tty
        NECHO "version check:" ""
	if sudo which wget >/dev/null 2>&1
	then
		SCRIPTname=$PROGname
	
		case "$SCRIPTname" in
        		uba_pre_check.sh)       FILEurl="https://splunk.box.com/shared/static/pzgpthu7zhucyeb3tcqwmatibrj8te0d.sh"
                       		         	MD5url="https://splunk.box.com/shared/static/myext7xe8qlkzaif90n95pl98sjx0qyf.md5";;
        		uba_health_check.sh)    FILEurl="https://splunk.box.com/shared/static/fkbqty0zkhsfnodf6ucsdjhj6m86fqvs.sh"
                       		         	MD5url="https://splunk.box.com/shared/static/870akdfxy0j3vj0mfgrulcv0tc44s8g6.md5";;
		esac

		mkdir -p ~/bin >/dev/null 2>&1		# probably exists but make sure

		GETmd5=false
		MD5file=$TMP/${BASEname}.md5
		if test -f $MD5file
		then
			INTERVALsecs=600
			MD5time=`stat -c %Y $MD5file`
			NOW=`date '+%s'`
       			THENsecs=`expr $NOW - $INTERVALsecs`
        		if test "$THENsecs" -gt "$MD5time"
        		then
                		GETmd5=true
        		fi
		else
			GETmd5=true
		fi

		GETfile=false			# assume that file is up to date

		if $GETmd5
		then
			if which timeout >/dev/null 2>&1
			then
				TIMEOUT="timeout 15"	# ideally, want to timeout DNS queries
			else
				TIMEOUT=""
			fi

			if $TIMEOUT wget --no-check-certificate -O $MD5file $MD5url >/dev/null 2>&1
                	then
                        	if which md5sum >/dev/null 2>&1
                        	then
					SAVEwd="`pwd`"
					for BINdir in "$PROGdir"
					do
						cd $BINdir
                                		if md5sum -c $MD5file >/dev/null 2>&1
                                		then
                                        		echo "current $BINdir; matches Box"    # nothing to change     
                                		else
                                        		echo -n "update available"
							if test "$BINdir" = "$CASPIDAhome/bin"
							then
								echo "; will attempt download to '$BINdir'"
                                        			GETfile=true
							else
								echo "   <= download latest version of $SCRIPTname from Box to $BINdir"
								 echo "$FILEurl" | sed "s/^/$INDENT    /"
							fi
						fi
					done
					cd "$SAVEwd"
                        	else
                               		echo "failed   <= 'md5sum' not available"
				fi
                	else
                        	echo "failed   ('wget' of $MD5file failed; check Box for new version of $SCRIPTname)"
 				echo "$FILEurl" | sed "s/^/$INDENT    /"
                	fi
		else
			echo "current at this time"
		fi
	else
		echo "failed   ('wget' not available to check for update)"
 		echo "$FILEurl" | sed "s/^/$INDENT    /"
	fi	
	echo
fi 

}

DOWNLOAD_SCRIPT() {

	GOTfile=false

	cd "$BINdir"
        NECHO "downloading script:" ""
        if wget --no-check-certificate -O ${SCRIPTname}.new $FILEurl >/dev/null 2>&1
        then
                GOTfile=true
                NEWmd5=`md5sum ${SCRIPTname}.new | cut -d ' ' -f 1`
                EXPECTEDmd5=`cut -d ' ' -f 1 < $MD5file`
                if test "$NEWmd5" = "$EXPECTEDmd5"
                then
                        echo "successful"
                else
                        echo "md5sum mismatch   <= download $SCRIPTname from Box manually"
                        GOTfile=false
                fi
        else
                echo "wget failed	<= download $SCRIPTname from Box manually $FILEurl"
        fi

        if ! $GOTfile
        then
                rm -f ${SCRIPTname}.new
        fi

	cd "$SAVEwd"
}


#
#	checking to see if UBA has already been configured
#

UBA_VERSION				# get UBA version

TTY=`tty 2>/dev/null`

( NECHO "$BASEname:" "version $VERSion"; echo ) >> $OUTfile
( NECHO "" "exec'd from $PROGdir"; echo ) >> $OUTfile
( NECHO "" "exec'd by $WHOIAM"; echo ) >> $OUTfile
( NECHO "" "          "; id ) >> $OUTfile
(
        chage -l $WHOIAM 2>/dev/null | grep -i '^Password expires'> $TMP1file
        if test -s $TMP1file
        then
                NECHO "" "          "; head -1 $TMP1file | sed -e 's/\s\s*:/ :/'
        fi
) >> $OUTfile
(
        if test "$SUDO_USER" != ""
        then
                NECHO ""  "SUDO_USER: $SUDO_USER; SUDO_UID: $SUDO_UID; SUDO_COMMAND: $SUDO_COMMAND"; echo
        fi
) >> $OUTfile
( NECHO "" "HOME: $HOME"; echo ) >> $OUTfile
( NECHO "" "PATH: $PATH"; echo ) >> $OUTfile
( NECHO "" "TTY: $TTY; SSH_TTY: $SSH_TTY"; echo ) >> $OUTfile
( NECHO "" "TEMPdir: $TMP"; echo ) >> $OUTfile

if :
then
	echo
	NECHO  "proxy settings:" ""
        env | grep -i "_proxy=" > $TMP1file
        if test -s $TMP1file
        then
                head -1 $TMP1file
                ALL_BUT_FIRST $TMP1file
                if test -z "$no_proxy"
                then
			if $uba41release
			then
				IPcontainers=",10.96.0.0/12,10.244.0.0/16"
			else
				IPcontainers=""
			fi
                        NECHO "no_proxy:" "'$no_proxy'   <== not set;  no_proxy value must include 'localhost,127.0.0.1${IPcontainers}' and all deployed nodes"; echo
			INCREMENT_EXCEPTIONS
                fi
	else
		echo "(no proxy)"
        fi
	echo
fi >> $OUTfile


if test "$TTY" != "not a tty" -a "$TTY" != "$SSH_TTY"
then
	if $CASPIDAuser -a $PROGdir = "$CASPIDAhome/bin" 
	then
 		# BOX has been deprecated
 		#
 		# CHECK_FOR_UPDATE	# see if Box has different version
 		#
 		# public Google Drive being investigated
		:
	else
		GETfile=false
		GOTfile=false
	fi

	if $GETfile
	then
		DOWNLOAD_SCRIPT	# updated script should be downloaded
	fi

	if $GOTfile
	then
		OLDstamp=`date '+%Y_%m_%d_%H:%M:%S' -r  $PROGdir/${PROGname}`
		if cp -p $PROGdir/${PROGname} $PROGdir/${PROGname}.$OLDstamp >/dev/null 2>&1
		then
			if cp $PROGdir/${PROGname}.new $PROGdir/${PROGname} >/dev/null 2>&1
			then
				echo 
				NECHO "saved script:"	"prior version saved as $PROGdir/${PROGname}.$OLDstamp"; echo
				echo
				NECHO "updated script:" "re-run new version of $PROGdir/$PROGname"; echo
				echo; echo
				cat $OUTfile > /dev/tty
				exit 0
			else
				NECHO "update script:" "failed   <= failed to replace script with $PROGdir/${PROGname}.new"; echo
			fi
		else
			NECHO "save script:" "failed   <= could not save to $PROGdir/${PROGname}.$OLDstamp"; echo
		fi
	fi
fi >> $OUTfile

if test -f $UBA_DEFAULT_PROPERTIES
then
	if test -f $UBA_SITE_PROPERTIES
	then
		if grep -q '<%=' $UBA_SITE_PROPERTIES
		then
			CASPIDA_PROPERTIES_EXIST=false			# 'setup' has not been run
		else
			CASPIDA_PROPERTIES_EXIST=true			# reduce need for frequent 'test -f'
		fi
	else
		CASPIDA_PROPERTIES_EXIST=false				# capida-site.properties does not exist
	fi
else
	CASPIDA_PROPERTIES_EXIST=false					# capida-default.properties does not exist
fi

CHECK_DEPLOYMENT caspida.cluster.nodes $DEPLOYnode
CLUSTER_NODES_list="$DEPLOYMENTlist"
CLUSTER_DEPLOYED=$FOUND

#
#	Script is run from the 'leader', the first node in a cluster. 
#
#	When multi-node, script is automatically copied to other nodes
#

if test $# -eq 0
then
	set "as_deployed"
fi

if test $# -eq 1
then
	if test "$1" = "-h" -o "$1" = "--help"
	then
		echo "usage: $PROGname -h | --help                                    # show this message		       version $VERSion"
		echo "       $PROGname node1 [node2 node3 [node4 ... node##]]         # list of node names prior to 'Setup'"
		echo "       $PROGname node1[,node2,node3[,node4, ... ,node##]]       # list of node names prior to 'Setup'"
		echo "       $PROGname [as_deployed]                                  # use configuration after 'Setup'"
		echo
		exit 1
	fi
	if test "$1" = "as_deployed"
	then
		if $CASPIDA_PROPERTIES_EXIST
		then
			if ! test -z "$CLUSTER_NODES_list"
			then
				set $CLUSTER_NODES_list
			else 
				echo "$PROGname $1   <== UBA cluster nodes have not been configured"
				exit 1
			fi
		else
			echo "$PROGname $1   <== UBA cluster has not been 'setup'"
			exit 1
		fi
	fi
fi

if test $# -eq 1
then
	IFSsav="$IFS"
	IFS=",$IFS"		# comma can be used as delimiter
	set $* 
	IFS="$IFSsav"
fi

case $# in
	0)	echo "usage: $PROGname [node1 [node2 node3 [node4 ... node10]]] | as_deployed           # version $VERSion"
		exit 1;;
	1)	MULTInode=false;;
	3|5|7|10)	MULTInode=true;;
	20)	MULTInode=true
		if ! $uba41feature
		then
			NECHO "multinode:" "$#   <= 20 nodes not supported prior to UBA 4.1"; echo
                	INCREMENT_EXCEPTIONS
		fi;;
	25)	MULTInode=true
		if ! $uba42feature
		then
			NECHO "multinode:" "$#   <= 25 nodes not supported prior to UBA 4.2"; echo
                	INCREMENT_EXCEPTIONS
		fi;;
	45)	MULTInode=true
		if ! $uba42feature
		then
			NECHO "multinode:" "$#   <= 45 nodes not supported prior to UBA 4.2"; echo
                	INCREMENT_EXCEPTIONS
		fi;;
	*)	MULTInode=true
		NECHO "multinode:" "$#   <== only 1|3|5|7|10|20|25|45 nodes supported in cluster"; echo
		INCREMENT_EXCEPTIONS
esac

NODElist="`echo $* | tr -d ','`"
set $*

NODEcount=$#

NUMnodes_unique=`echo "$NODElist" | tr ',' ' ' | sed -e 's/\s\s/ /g' | tr ' ' '\n' | sort -fu | wc -l`
if test "$NODEcount" -ne "$NUMnodes_unique"
then
        NECHO "node names:" "'$NODElist'   <== node names are not unique; re-run 'Setup' with unique names"; echo
        NECHO "" " ... cannot continue as cluster..."; echo
        INCREMENT_EXCEPTIONS
	MULTInode=false	
	NODElist="$1"
	NODEcount=1
fi


if ! $CASPIDAuser
then
	MULTInode=false			# can't do anything fancy without 'caspida' user
	NODElist="$1"
	NODEcount=1
fi

USEip=false
USEfqdn=false
for NODE in $NODElist
do
	case $NODE in
		[0-9]*\.[0-9]*\.[0-9]*\.[0-9]*)USEip=true;;
		*\.*)	USEfqdn=true;;
	esac	
done > $TMP1file
if test -s "$TMP1file"
then
	NECHO "invalid nodename:" "`head -1 $TMP1file`"; echo
	ALL_BUT_FIRST $TMP1file
	exit 2
fi

if $USEfqdn
then
	THISnode=`hostname -f 2>/dev/null | tr '[:upper:]' '[:lower:]'`
else
	THISnode=`hostname 2>/dev/null | tr '[:upper:]' '[:lower:]' | cut -d '.' -f 1` 
fi
THISip=`hostname -i 2>/dev/null`
if $USEip
then
	THISnode=$THISip
fi
	
if test "$1" = "localhost"
then
	echo
	NECHO "node1:"	"deployed as 'localhost', using '$THISnode' "; echo
	NODElist=$THISnode
fi >> $OUTfile


NOWsecs=`date "+%s"`
NOW=`date "--date=@$NOWsecs" "+%F_%R:%S"`
OUTnow=`date "--date=@$NOWsecs" '+%y%m%d_%H%M'`

#
#	sizing values are based on the number of nodes and will assume that > 5 will have same values
#

EXPECTED_SPARK_offline_workers_hosts=`grep '^spark.worker' $CASPIDAdir/conf/deployment/caspida-deployment.conf 2>/dev/null | tr ',' ' ' | wc -w`	# deployment.conf contains list of spark workers

if test "$EXPECTED_SPARK_offline_workers_hosts" -eq "0"
then
	EXPECTED_SPARK_offline_workers_hosts=1		# prevent divide by zero
fi
EXPECTED_SPARK_offline_workers="1"

if ! $uba41feature
then
	case $NODEcount in
		1)	EXPECTED_IMPALA_MemLimit="20%"
			EXPECTED_KAFKA_log_retention_hours="5"
			EXPECTED_KAFKA_system_messaging_fetchsizeinmb="5"
			EXPECTED_KAFKA_system_messaging_partitions="2"
			EXPECTED_KAFKA_system_realtime_workers="4"
			EXPECTED_KAFKA_system_messaging_flowcontrol_maxtopicsize="100G"
			EXPECTED_KAFKA_Heap="-Xmx2G"
			EXPECTED_MODEL_jobmanager_master_allow_live="true"
			EXPECTED_ANALYTICS_analytics_store_data_retention="1m"
			EXPECTED_ANALYTICS_analytics_messaging_read_interval="1800"
			EXPECTED_ANALYTICS_analytics_messaging_read_maxmsgs="5000"
			EXPECTED_ANALYTICS_MaxMemory="Xmx4G"
			EXPECTED_ANALYTICS_Threads="XX:ParallelGCThreads=2"
			EXPECTED_POSTGRESQL_shared_buffers="512MB"
			EXPECTED_POSTGRESQL_max_connections="200"
			EXPECTED_MODELMEM_system_realtime_workers_modelcache_memory="128"
			if $uba41feature
			then
				EXPECTED_MODELMEM_system_realtime_workers_modelcache_memory="256"
			fi
			if ! $uba32feature
			then
				EXPECTED_MODELMEM_system_realtime_workers_maxmodelcache="10000"
				EXPECTED_ANALYTICS_identity_session_cache_limit="200000"
			else
				EXPECTED_MODELMEM_system_realtime_workers_maxmodelcache="20000"
				EXPECTED_ANALYTICS_identity_session_cache_limit="100000"
			fi
			EXPECTED_STORM_supervisor_slots_ports="6700 6701 6702 6703"
			EXPECTED_STORM_MaxMem="Xmx6144m"
			EXPECTED_STORM_User_workersHint="1"
			EXPECTED_STORM_Partition_workersHint="1"
			EXPECTED_STORM_Domain_workersHint="1"
			EXPECTED_STORM_Device_workersHint="1"
			EXPECTED_STORM_AnomalyEvent_workersHint="1"
			EXPECTED_SPARK_offline_workflow_cores="10"
			EXPECTED_SPARK_offline_worker_cores=`expr $EXPECTED_SPARK_offline_workflow_cores / $EXPECTED_SPARK_offline_workers / $EXPECTED_SPARK_offline_workers_hosts 2>/dev/null`		# math done for clarity
			;;
		3)	EXPECTED_IMPALA_MemLimit="32%"
			EXPECTED_KAFKA_log_retention_hours="5"
			if ! $uba41feature
			then
				EXPECTED_KAFKA_system_messaging_fetchsizeinmb="8"
				EXPECTED_KAFKA_system_messaging_flowcontrol_maxtopicsize="200G"
				EXPECTED_KAFKA_system_realtime_workers="9"
			else
				EXPECTED_KAFKA_system_messaging_fetchsizeinmb="5"
				EXPECTED_KAFKA_system_messaging_flowcontrol_maxtopicsize="100G"
				EXPECTED_KAFKA_system_realtime_workers="4"
			fi
			EXPECTED_KAFKA_system_messaging_partitions="4"
			EXPECTED_KAFKA_Heap="-Xmx4G"
			EXPECTED_MODEL_jobmanager_master_allow_live="false"
			EXPECTED_ANALYTICS_analytics_store_data_retention="1m"
			EXPECTED_ANALYTICS_analytics_messaging_read_interval="1800"
			EXPECTED_ANALYTICS_analytics_messaging_read_maxmsgs="5000"
			EXPECTED_ANALYTICS_identity_session_cache_limit="200000"
			EXPECTED_ANALYTICS_MaxMemory="Xmx4G"
			EXPECTED_ANALYTICS_Threads="XX:ParallelGCThreads=2"
			EXPECTED_POSTGRESQL_shared_buffers="512MB"
			EXPECTED_POSTGRESQL_max_connections="500"
			EXPECTED_MODELMEM_system_realtime_workers_modelcache_memory="128"
			if $uba41feature
			then
				EXPECTED_MODELMEM_system_realtime_workers_modelcache_memory="256"
			fi
			EXPECTED_MODELMEM_system_realtime_workers_maxmodelcache="20000"
			EXPECTED_STORM_supervisor_slots_ports="6700 6701 6702 6703 6704 6705 6706 6707 6708"
			EXPECTED_STORM_MaxMem="Xmx4G"
			EXPECTED_STORM_User_workersHint="2"
			EXPECTED_STORM_Partition_workersHint="2"
			EXPECTED_STORM_Domain_workersHint="2"
			EXPECTED_STORM_Device_workersHint="2"
			EXPECTED_STORM_AnomalyEvent_workersHint="1"
			EXPECTED_SPARK_offline_workflow_cores="10"
			EXPECTED_SPARK_offline_worker_cores=`expr $EXPECTED_SPARK_offline_workflow_cores / $EXPECTED_SPARK_offline_workers / $EXPECTED_SPARK_offline_workers_hosts 2>/dev/null`		# math done for clarity
			;;
		5)	EXPECTED_IMPALA_MemLimit="32%"
			EXPECTED_KAFKA_log_retention_hours="3"
			EXPECTED_KAFKA_system_messaging_fetchsizeinmb="10"
			EXPECTED_KAFKA_system_messaging_partitions="8"
			EXPECTED_KAFKA_system_realtime_workers="10"
			EXPECTED_KAFKA_system_messaging_flowcontrol_maxtopicsize="300G"
			EXPECTED_KAFKA_Heap="-Xmx4G"
			EXPECTED_MODEL_jobmanager_master_allow_live="false"
			EXPECTED_ANALYTICS_analytics_store_data_retention="1m"
			EXPECTED_ANALYTICS_analytics_messaging_read_interval="1800"
			EXPECTED_ANALYTICS_analytics_messaging_read_maxmsgs="5000"
			EXPECTED_ANALYTICS_identity_session_cache_limit="5000"
			EXPECTED_ANALYTICS_MaxMemory="Xmx4G"
			EXPECTED_ANALYTICS_Threads="XX:ParallelGCThreads=2"
			EXPECTED_POSTGRESQL_shared_buffers="512MB"
			EXPECTED_POSTGRESQL_max_connections="500"
			EXPECTED_MODELMEM_system_realtime_workers_modelcache_memory="256"
			EXPECTED_MODELMEM_system_realtime_workers_maxmodelcache="20000"
			EXPECTED_STORM_supervisor_slots_ports="6700 6701 6702 6703 6704 6705"
			EXPECTED_STORM_MaxMem="Xmx6G"
			EXPECTED_STORM_User_workersHint="4"
			EXPECTED_STORM_Partition_workersHint="2"
			EXPECTED_STORM_Domain_workersHint="3"
			EXPECTED_STORM_Device_workersHint="2"
			EXPECTED_STORM_AnomalyEvent_workersHint="2"
			EXPECTED_SPARK_offline_workflow_cores="20"
			EXPECTED_SPARK_offline_worker_cores=`expr $EXPECTED_SPARK_offline_workflow_cores / $EXPECTED_SPARK_offline_workers / $EXPECTED_SPARK_offline_workers_hosts 2>/dev/null`		# math done for clarity
			;;
		7)	EXPECTED_IMPALA_MemLimit="38%"
			EXPECTED_KAFKA_log_retention_hours="3"
			EXPECTED_KAFKA_system_messaging_fetchsizeinmb="10"
			EXPECTED_KAFKA_system_messaging_partitions="12"
			EXPECTED_KAFKA_system_realtime_workers="18"
			EXPECTED_KAFKA_system_messaging_flowcontrol_maxtopicsize="300G"
			EXPECTED_KAFKA_Heap="-Xmx4G"
			EXPECTED_MODEL_jobmanager_master_allow_live="false"
			EXPECTED_ANALYTICS_analytics_store_data_retention="20d"
			EXPECTED_ANALYTICS_analytics_messaging_read_interval="1800"
			EXPECTED_ANALYTICS_analytics_messaging_read_maxmsgs="5000"
			EXPECTED_ANALYTICS_identity_session_cache_limit="500000"
			EXPECTED_ANALYTICS_MaxMemory="Xmx10G"
			EXPECTED_ANALYTICS_Threads="XX:ParallelGCThreads=4"
			EXPECTED_POSTGRESQL_shared_buffers="512MB"
			EXPECTED_POSTGRESQL_max_connections="500"
			EXPECTED_MODELMEM_system_realtime_workers_modelcache_memory="256"
			EXPECTED_MODELMEM_system_realtime_workers_maxmodelcache="20000"
			EXPECTED_STORM_supervisor_slots_ports="6700 6701 6702 6703 6704 6705 6706"
			EXPECTED_STORM_MaxMem="Xmx6G"
			EXPECTED_STORM_User_workersHint="4"
			EXPECTED_STORM_Partition_workersHint="5"
			EXPECTED_STORM_Domain_workersHint="4"
			EXPECTED_STORM_Device_workersHint="4"
			EXPECTED_STORM_AnomalyEvent_workersHint="1"
			EXPECTED_SPARK_offline_workflow_cores="30"
			EXPECTED_SPARK_offline_workflow_cores="32"
			EXPECTED_SPARK_offline_worker_cores=`expr $EXPECTED_SPARK_offline_workflow_cores / $EXPECTED_SPARK_offline_workers / $EXPECTED_SPARK_offline_workers_hosts 2>/dev/null`		# math done for clarity
			;;
		10)	EXPECTED_IMPALA_MemLimit="38%"
			EXPECTED_KAFKA_log_retention_hours="3"
			EXPECTED_KAFKA_system_messaging_fetchsizeinmb="10"
			EXPECTED_KAFKA_system_messaging_partitions="16"
			EXPECTED_KAFKA_system_realtime_workers="32"
			EXPECTED_KAFKA_system_messaging_flowcontrol_maxtopicsize="800G"
			EXPECTED_KAFKA_Heap="-Xmx4G"
			EXPECTED_MODEL_jobmanager_master_allow_live="false"
			EXPECTED_ANALYTICS_analytics_store_data_retention="20d"
			EXPECTED_ANALYTICS_analytics_messaging_read_interval="1800"
			EXPECTED_ANALYTICS_analytics_messaging_read_maxmsgs="5000"
			EXPECTED_ANALYTICS_identity_session_cache_limit="500000"
			EXPECTED_ANALYTICS_MaxMemory="Xmx10G"
			EXPECTED_ANALYTICS_Threads="XX:ParallelGCThreads=4"
			EXPECTED_POSTGRESQL_shared_buffers="512MB"
			EXPECTED_POSTGRESQL_max_connections="500"
			EXPECTED_MODELMEM_system_realtime_workers_modelcache_memory="256"
			EXPECTED_MODELMEM_system_realtime_workers_maxmodelcache="50000"
			EXPECTED_STORM_supervisor_slots_ports="6700 6701 6702 6703 6704 6705 6706"
			EXPECTED_STORM_MaxMem="Xmx6G"
			EXPECTED_STORM_User_workersHint="4"
			EXPECTED_STORM_Partition_workersHint="5"
			EXPECTED_STORM_Domain_workersHint="4"
			EXPECTED_STORM_Device_workersHint="4"
			EXPECTED_STORM_AnomalyEvent_workersHint="1"
			EXPECTED_SPARK_offline_workflow_cores="64"
			EXPECTED_SPARK_offline_worker_cores=`expr $EXPECTED_SPARK_offline_workflow_cores / $EXPECTED_SPARK_offline_workers / $EXPECTED_SPARK_offline_workers_hosts / 2 2>/dev/null`		# math done for clarity
			;;
		*)	EXPECTED_IMPALA_MemLimit="38%"
                	EXPECTED_KAFKA_log_retention_hours="3"
                	EXPECTED_KAFKA_system_messaging_fetchsizeinmb="10"
                	EXPECTED_KAFKA_system_messaging_partitions="16"
                	EXPECTED_KAFKA_system_realtime_workers="32"
                	EXPECTED_KAFKA_system_messaging_flowcontrol_maxtopicsize="800G"
                	EXPECTED_KAFKA_Heap="-Xmx4G"
                	EXPECTED_MODEL_jobmanager_master_allow_live="false"
                	EXPECTED_ANALYTICS_analytics_store_data_retention="20d"
                	EXPECTED_ANALYTICS_analytics_messaging_read_interval="1800"
                	EXPECTED_ANALYTICS_analytics_messaging_read_maxmsgs="5000"
                	EXPECTED_ANALYTICS_identity_session_cache_limit="500000"
                	EXPECTED_ANALYTICS_MaxMemory="Xmx10G"
                	EXPECTED_ANALYTICS_Threads="XX:ParallelGCThreads=4"
                	EXPECTED_POSTGRESQL_shared_buffers="512MB"
                	EXPECTED_POSTGRESQL_max_connections="500"
                	EXPECTED_MODELMEM_system_realtime_workers_modelcache_memory="256"
                	EXPECTED_MODELMEM_system_realtime_workers_maxmodelcache="20000"
                	EXPECTED_STORM_supervisor_slots_ports="6700 6701 6702 6703 6704 6705 6706"
                	EXPECTED_STORM_MaxMem="Xmx6G"
                	EXPECTED_STORM_User_workersHint="4"
                	EXPECTED_STORM_Partition_workersHint="5"
                	EXPECTED_STORM_Domain_workersHint="4"
                	EXPECTED_STORM_Device_workersHint="4"
                	EXPECTED_STORM_AnomalyEvent_workersHint="1"
                	EXPECTED_SPARK_offline_workflow_cores="48"
                	# EXPECTED_SPARK_offline_worker_cores=`expr $EXPECTED_SPARK_offline_workflow_cores / $EXPECTED_SPARK_offline_workers / $EXPECTED_SPARK_offline_workers_hosts / 2 2>/dev/null`             # math done for clarity
                	EXPECTED_SPARK_offline_worker_cores=6
                	;;
		esac
	else
		EXPECTED_IMPALA_MemLimit="tunable"
                EXPECTED_KAFKA_log_retention_hours="tunable"
                EXPECTED_KAFKA_system_messaging_fetchsizeinmb="tunable"
                EXPECTED_KAFKA_system_messaging_partitions="tunable"
                EXPECTED_KAFKA_system_realtime_workers="tunable"
                EXPECTED_KAFKA_system_messaging_flowcontrol_maxtopicsize="tunable"
                EXPECTED_KAFKA_Heap="tunable"
                EXPECTED_MODEL_jobmanager_master_allow_live="tunable"
                EXPECTED_ANALYTICS_analytics_store_data_retention="tunable"
                EXPECTED_ANALYTICS_analytics_messaging_read_interval="tunable"
                EXPECTED_ANALYTICS_analytics_messaging_read_maxmsgs="tunable"
                EXPECTED_ANALYTICS_identity_session_cache_limit="tunable"
                EXPECTED_ANALYTICS_MaxMemory="tunable"
                EXPECTED_ANALYTICS_Threads="tunable"
                EXPECTED_POSTGRESQL_shared_buffers="tunable"
                EXPECTED_POSTGRESQL_max_connections="tunable"
                EXPECTED_MODELMEM_system_realtime_workers_modelcache_memory="tunable"
                EXPECTED_MODELMEM_system_realtime_workers_maxmodelcache="tunable"
                EXPECTED_STORM_supervisor_slots_ports="tunable"
                EXPECTED_STORM_MaxMem="tunable"
                EXPECTED_STORM_User_workersHint="tunable"
                EXPECTED_STORM_Partition_workersHint="tunable"
                EXPECTED_STORM_Domain_workersHint="tunable"
                EXPECTED_STORM_Device_workersHint="tunable"
                EXPECTED_STORM_AnomalyEvent_workersHint="tunable"
                EXPECTED_SPARK_offline_workflow_cores="tunable"
                EXPECTED_SPARK_offline_worker_cores="tunable"
		EXPECTED_ANALYTICS_analytics_store_max_connections_active="tunable"
	fi
	
EXPECTED_THREAT_models_threats_highconfidence="true" 
EXPECTED_THREAT_models_threats_goBackDays="15"

set `echo $* | tr -d ','`	# strip out commas

FIRSTnode=`echo "$1" | tr '[:upper:]' '[:lower:]'`	# this is the 'leader' of the cluster

if $MULTInode
then
	if test $THISnode = $FIRSTnode
	then
		LEADERnode=true
		MEMBERnode=false
		shift
		MEMBERlist="$*"

		echo
		echo "multi-node preparation takes more than a few seconds ..." > /dev/tty
		echo > /dev/tty
		echo "parallelization ..." 
		# FIRSTnode is responsible for checking other nodes
                for NODE in $MEMBERlist
                do
                        echo
                        NECHO "verifying version:"  "$NODE ... "        # check the member node

                        SCRIPTbin="${CASPIDAdir}/bin"
			if $uba32feature
			then
				SCRIPTbin="$SCRIPTbin/utils"
			fi

                        if ssh -o ConnectTimeout=15 $NODE grep "^VERSion" "${SCRIPTbin}/$PROGname" > $TMP1file 2>/dev/null
                        then
                                REMOTEversion=`sed -e 's/^VERSion="//' -e 's/"//' < $TMP1file`
                                echo -n "'$REMOTEversion'"
                                if test "$VERSion"  != "$REMOTEversion"
                                then
                                        REPLICATE=true          # use the newer version
                                else
                                        REPLICATE=false         # use the distributed version
                                fi
                        else
                                echo -n "'check failed'"
                                REPLICATE=true                  # distribution file not found
                        fi

                        if $REPLICATE
                        then
                                echo "   (replication needed)"
                                if ssh -o ConnectTimeout=15 $NODE mkdir -p ~/bin 2>/dev/null
                                then
                                        :               # ~/bin/ directory created
                                else
                                        NECHO "replicate:" "   <== mkdir on '$NODE' failed; check will not run on this node"; echo
					INCREMENT_EXCEPTIONS
                                        continue
                                fi

                                if scp "$PROGdir/$PROGname" ${NODE}:~/bin >/dev/null 2>&1
                                then
                                        SCRIPTbin="~/bin"       # script was copied
                                else
                                        NECHO "replicate:" "   <== scp to '$NODE' failed; check will not run on this node"; echo
					INCREMENT_EXCEPTIONS
                                        continue
                                fi
                        else
                                echo
                        fi

			NECHO "building checklist ..." ""; echo
			FILElist=`eval echo $CHECKlist`                 # expand *

			if ! test -f $MD5compare
			then
                        	for FILE in $FILElist
                        	do
                                	case "$FILE" in
                                        	/opt/caspida/bin/utils/*)continue;;
                                        	/etc/caspida/local/bin/utils/*)continue;;
                                        	/opt/caspida/bin/free-impala-connxs.sh)continue;;
						/opt/caspida/conf/analytics.*/*)continue;;
						/opt/caspida/sql/*)continue;;
						/opt/caspida/web/*)continue;;
                                	esac
                                	if test -f $FILE
                                	then
                                        	md5sum $FILE
                                	fi
                        	done > $MD5compare                              # create md5sum file for comparison
			fi

                        if test -f $MD5compare
                        then
				NECHO "copying checklist ..." ""; echo
                                scp $MD5compare ${NODE}:$MD5compare >/dev/null 2>&1     # copy the md5file to member node
                        fi

                        MEMBERoutfile="${MEMBERout}${NODE}.txt"

                        echo "$MEMBERoutfile" >> $MEMBERwait

                        NECHO "dispatching script:" "initiated on ${NODE}:${SCRIPTbin} ..."; echo
			ssh -o ServerAliveInterval=55 -o ServerAliveCountMax=3 $NODE "sh ${SCRIPTbin}/$PROGname $NODElist" > $MEMBERoutfile 2>&1 &      # run in background for parallelism
			echo $! >> $BGjobs
                done

		echo
		echo "Checking $FIRSTnode first ..."
	else
		LEADERnode=false
		MEMBERnode=true
	fi

	DEPLOYnode=$THISnode
else
	LEADERnode=true
	MEMBERnode=false

	if $uba41feature
	then
		DEPLOYnode=$THISnode
	else
		DEPLOYnode=localhost
	fi
	
	echo
	echo "Checking $FIRSTnode only ..."
	if test $THISnode != $FIRSTnode && test $FIRSTnode != localhost
        then
		thisnode=`echo "$THISnode" | tr '[:upper:]' '[:lower:]'`
		firstnode=`echo "$FIRSTnode" | tr '[:upper:]' '[:lower:]'`
		if test $thisnode != $firstnode
		then
			ERROR="<== deployment does not match hostname"
			INCREMENT_EXCEPTIONS	
		else
			ERROR="<= deployment does not precisely match hostname"
		fi
		NECHO "node1:" "'$FIRSTnode'   $ERROR  '$THISnode'"; echo
	fi
fi >> $OUTfile 	# output file starts now	

	cat $OUTfile

(
        CASPIDAhome=`( cd ~caspida/bin; pwd )`
        
	for U in caspida sudo
	do
		if test $U = "caspida"
		then
			Upath=`printenv PATH 2>/dev/null`
		else
			Upath=`sudo printenv PATH 2>/dev/null`
		fi

		echo
		NECHO "path check (${U}):" "$Upath"

        	ERROR=""
        	ERRdelim=""
		reqSECUREPATH="/sbin:/bin:/usr/sbin:/usr/bin"
        	REQpath=`echo "$reqSECUREPATH" | tr ":" " "` 
        	for REQdir in $REQpath
        	do
                	case ":$Upath:/home" in
                        	*:$REQdir:*);;
                        	*) ERROR="${ERROR}${ERRdelim}$REQdir not in PATH"; ERRdelim=", ";
                	esac
        	done
        	if test "$ERROR" != ""
        	then
                	echo "    <= $ERROR; PATH should be corrected"
        	else
                	echo
        	fi
	 
        	RMcmd="curl svc nproc mpstat dmidecode uptime nslookup openssl shuf wget timeout dig ifconfig ip top vmstat pidstat netstat mailx sendmail lsof impala-shell w3m java ntpdate hostnamectl timedatectl zip md5sum ss nc netcat killall "

        	echo
        	NECHO "cmd check (${U})..." ""; echo
        	for CMD in $RMcmd
        	do
                	PATH=$Upath which $CMD > $TMP1file 2>/dev/null
                	if grep -q '/' $TMP1file 
                	then
				:
			else
                        	echo "(not in PATH)" > $TMP1file
                	fi

                	NECHO "  ${CMD}:" "`head -1 $TMP1file`"; echo
        	done | sort
        	echo
	
		for PYcmd in python python2 python3
		do 
			PYpath=`PATH=$Upath which $PYcmd 2>/dev/null`
			if test "$PYpath" = ""
			then
				echo "$PYcmd: not in PATH"
			else
				PYsym=`stat -c "%N" $PYpath`
				echo -n "$PYcmd: $PYsym;  "
				$PYpath -c 'import pexpect' >/dev/null 2>&1
				if test $? -eq 0
				then
					echo -n "pexpect: module found;  PS1: "
					echo "$PS1"
				else
					echo "pexpect: module not found"
				fi
			fi
		done > $TMP1file

		NECHO "python check (${U}):" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
		echo
	done

	echo
	NECHO "hostname:" "$THISnode"
	if test -z "$THISnode"
	then
		echo -n "   <== hostname failed"	# hostname must be correct
	        INCREMENT_EXCEPTIONS
	else
		if test "$THISnode" = "Caspida" || test "$THISnode" = "caspida"
		then
			echo -n "   <== please change to meaningful hostname"
			INCREMENT_EXCEPTIONS
		else
			case "$NODElist" in
				*$THISnode*)	;;
				*)		echo -n "   <= does not appear in argument list '$NODElist'";;
			esac
		fi
	fi

	if $CLUSTER_DEPLOYED
	then
		LOWlist=`echo "$DEPLOYMENTlist" | tr '[:upper:]' '[:lower:]'`
		LOWnode=`echo "$DEPLOYnode" | tr '[:upper:]' '[:lower:]'`
		case "$LOWlist" in
			*$LOWnode*) echo;;
                       *)              echo  "   <= does not appear in deployed list '$DEPLOYMENTlist'";;
                esac
	else
		echo
	fi

	if ! touch $TMP1file >/dev/null 2>&1
	then
		NECHO "$TMP/:" "failed to write   <== verify $TMP/ permissions"
		echo "... must exit"
		exit 5
	fi	

	hostnamectl status > $TMP1file 2>/dev/null
	if test -s $TMP1file
	then
		HOSTNAMEstatic=`grep "Static hostname:" $TMP1file | sed -e 's/^.*: //' | tr '[:upper:]' '[:lower:]'`
		NODENAMElower=`echo $THISnode | tr '[:upper:]' '[:lower:]'`

		if test "$HOSTNAMEstatic" != "$NODENAMElower"
		then
			sed -ie "/Static hostname:/ s/\$/   <== static hostname '$HOSTNAMEstatic' is not '$NODENAMElower'/" $TMP1file
			INCREMENT_EXCEPTIONS
		fi
		
		NECHO "hostnamectl status:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
		echo
	fi
	HOSTip=`hostname -i 2>/dev/null | cut -d ' ' -f 1` 	# only the first
	NECHO "hostip:" "$HOSTip"; echo
	
	egrep -e '^[0-9]+\.' /etc/hosts > $TMP1file
	NECHO "/etc/hosts:" "`head -1 $TMP1file`"; echo
	ALL_BUT_FIRST $TMP1file

	if egrep -qe '^hosts:.*\bdns\b' /etc/nsswitch.conf
	then
		DNScheck=true
	else
		DNScheck=false
	fi
	
	grep -v "^#" /etc/resolv.conf 2>/dev/null > $TMP1file	# DNS should be used for connecting to external data source servers
	NECHO "resolv.conf:" "`head -1 $TMP1file`"
	if ! egrep -qe '^search |^domain' $TMP1file
	then
		echo "   <= without domain or search, nslookup shortname is expected to fail"
	else
		echo 
	fi
	ALL_BUT_FIRST $TMP1file
	
	if $DNScheck
	then
		if ! which nslookup >/dev/null 2>&1
		then
			NECHO "nslookup:" "   <= desired program is not available"; echo
			NSLOOKUP=false
		fi
	
		echo
		for NODE in $NODElist
		do
			LOOKUP $NODE	# verify that all nodes are addressable from this node
			echo
		done
	fi

	echo 
	NECHO "public ip:" ""
	if which dig >/dev/null 2>&1
	then
		if dig +short myip.opendns.com @resolver1.opendns.com 2>/dev/null
		then
			:
		else
			echo '""   (dig failed)'
		fi	
	else
		echo '""   (dig not available)'
	fi
	
	LOCALHOSTip=""		# start as empty string
	if $NSLOOKUP
	then
		LOCALHOSTname=`nslookup localhost 2>/dev/null | egrep "^Name" | cut -d ':' -f 2 | sed -e "s/\s\s*//g"`
		if test "$LOCALHOSTname" = ""
		then
			LOCALHOSTname="localhost"
		fi
		LOCALHOSTip=`nslookup $LOCALHOSTname 2>/dev/null | egrep -e "^Address" | grep -v '#' |cut -d ':' -f 2 | sed -e "s/\s\s*//g"`
		LOOKprog="nslookup"
	else
		if which dig >/dev/null 2>$1
		then
			DIGanswer=`dig -4 +search localhost | sed -n -e '/^;; ANSWER SECTION/,+1 p' | tail -1 | sed -e 's/\s\s*/ /g' -e 's/\. / /' | cut -d ' ' -f 1,5`
			LOCALHOSTname=`echo $DIGanswer | cut -d ' ' -f 1`
			LOCALHOSTip=`echo $DIGanswer | cut -d ' ' -f 2`
			LOOKprog="dig"
		fi
	fi
	if test "$LOCALHOSTip" = ""
	then
		LOCALHOSTname="localhost"
		LOCALHOSTip=`getent ahosts localhost | egrep '\.' | grep "RAW" | sed -e 's/\s\s*/ /g' -e 's/ RAW\s*//'`
		LOOKprog="getent"
	fi
	
	echo
	NECHO "localhost ($LOOKprog):" "name='$LOCALHOSTname', ip='$LOCALHOSTip'"
	if test "$LOCALHOSTip" != "127.0.0.1"
	then
		if $uba43feature
		then
			FLAG="<="
		else
			FLAG="<=="
			INCREMENT_EXCEPTIONS
		fi
		echo "   $FLAG localhost does not resolve to '127.0.0.1'"
	else
		echo
	fi
	echo
	
	if netstat -rn 2>/dev/null
	then
		:
	else
		ip route 2>/dev/null
	fi > $TMP1file
	if test -s $TMP1file
	then
		NECHO "routing:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	fi

	echo
	for BANNER in /etc/issue /etc/motd
	do
		if test -f $BANNER
		then
			if test -s $BANNER
			then
				cp $BANNER $TMP1file
			else
				echo '(empty)' > $TMP1file
			fi
		else
			echo '(does not exist)' > $TMP1file
		fi
		NECHO "${BANNER}:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	done

        echo
        ls -l ~/.profile ~/.bash_login ~/.bash_profile ~/.bashrc 2>/dev/null > $TMP1file
        if ! test -s $TMP1file
        then
                echo "(no profile)" > $TMP1file
        fi
        NECHO "profile:" "`head -1 $TMP1file`"; echo
        ALL_BUT_FIRST $TMP1file

	echo
	ls -laR ~/.ssh > $TMP1file 2>/dev/null
	NECHO "~/.ssh:" "`head -1 $TMP1file`"; echo
	ALL_BUT_FIRST $TMP1file

        sudo /usr/sbin/sshd -T 2>&1 |\
	tee $TMP2file |\
        egrep -i 'maxsessions|permittty|usedns|strictmodes|usepam|hostbasedauthentication|ignoreuserknownhosts|passwordauthentication|authorizedkeysfile|permituserrc|setenv|pubkeyauthentication' |\
        sort > $TMP1file
        if ! test -s $TMP1file
        then
                echo "(no matches)" > $TMP1file
		cat $TMP2file >> $TMP1file

		echo >> $TMP1file
		ls -l /etc/ssh/sshd_config 2>/dev/null >> $TMP1file
		sed -e '/^#/d' -e '/^\s*$/d' < /etc/ssh/sshd_config 2>&1 >> $TMP1file
        fi
        echo
        NECHO "sshd -T:" "`head -1 $TMP1file`" ; echo
        ALL_BUT_FIRST $TMP1file

        SSHconnections=`ps -elf | grep '[s]shd:' | wc -l | sed -e 's/\s\s*//g'`
        echo
        NECHO "sshd connections:" "'$SSHconnections'" ; echo

	echo
	for SSHconf in ~/.ssh/config /etc/ssh/ssh_config
	do 
		if test -r $SSHconf
		then 
			NECHO "ssh config:" ""
			grep -H StrictHostKeyChecking $SSHconf
		fi
	done
	echo

	AUTHreq=".ssh/authorized_keys"
	sudo egrep -e '^AuthorizedKeysFile\s\s*' /etc/ssh/sshd_config > $TMP1file 2>/dev/null
	AUTHfile=`sed -e 's/^AuthorizedKeysFile\s\s*//' < $TMP1file`
	NECHO "AuthorizedKeyFile:" "$AUTHfile"
	case "$AUTHfile" in
		"")		echo "   (default)";;
		"$AUTHreq")	echo;;
		*) 		echo "   <== must be '$AUTHreq'"
				INCREMENT_EXCEPTIONS;;
	esac

	#
	#	check to certain that ssh is successful between all nodes in the cluster
	#	without out any prompts
	#
	
	echo
	if $CASPIDAuser
	then
		NODEnum=1
		if $MEMBERnode
		then
			for NODEitem in $NODElist
			do
				if test $NODEitem = $THISnode
				then
					break
				else
					NODEnum=`expr $NODEnum + 1`
				fi
			done
			SSHdelay=`expr $NODEnum \* 27  2>/dev/null`
		else
			SSHdelay=0
		fi

		sleep $SSHdelay # try to avoid too many concurrent SSH connections

		DATEstamp="`date`"
		LOCALtz=`date -d "$DATEstamp" "+%Z"`
		PREVsecs=`date -d "$DATEstamp" "+%s"`			# watching for clock skew
		for NODE in $NODElist
		do
			for MODEval in yes no
			do
				NECHO "ssh connectivity:" "$THISnode to $NODE"
				if ssh -o BatchMode=$MODEval -o StrictHostKeyChecking=no -o PasswordAuthentication=no -o ConnectTimeout=10 $NODE date > $TMP1file 2>$TMP2file
				then
					DATEstamp="`egrep -e '\s[0-9][0-9]:[0-9][0-9]:[0-9][0-9]' $TMP1file`"
					echo -n "   (ssh okay - $DATEstamp)"
					CURRsecs=`date -d "$DATEstamp" "+%s" 2>/dev/null`
					if test "$CURRsecs" -lt "$PREVsecs"
					then
						echo -n "   <== clock is skewed"
						INCREMENT_EXCEPTIONS
					fi
					PREVsecs=$CURRsecs

					if $LEADERnode
					then
						REMOTEtz=`date -d "$DATEstamp" "+%Z"` 
						if test "$LOCALtz" != "$REMOTEtz"
						then 
							echo -n "   <== timezone mismatch "
							INCREMENT_EXCEPTIONS
						fi
					fi
					if ssh -o BatchMode=yes -o StrictHostKeyChecking=no $NODE getent ahostsv4 $THISnode > $TMP1file 2>/dev/null
					then
						REMOTEip=`grep STREAM $TMP1file | tee $TMP2file | cut -d ' ' -f 1 | sort -u`
						echo -n "   (`head -1 $TMP2file`)"
						if test "$HOSTip" = "$REMOTEip"
						then
							echo
						else
							echo "   <== $NODE references $THISnode by different IP '$REMOTEip' not '$HOSTip'"
							INCREMENT_EXCEPTIONS	
						fi
					else
						echo "   <== getent failed on $NODE"
						INCREMENT_EXCEPTIONS
					fi
					break
				else
					if test $MODEval == "yes"
					then
						echo -n "   <= "
					else
						echo -n "   <== "
						INCREMENT_EXCEPTIONS 
					fi
					echo "failed - BatchMode=$MODEval; `head -1 $TMP2file`"
				fi
			done
		done
	fi
	
	echo
	CPUs=`grep "^processor" /proc/cpuinfo 2>/dev/null | wc -l`
	NECHO "CPUs:" "$CPUs"
	if test "$CPUs" -lt "$minCPUs"
	then
		echo "    <== less than $minCPUs"	# at least 16 CPUs/cores is a requirement
		INCREMENT_EXCEPTIONS
	else
		echo
	fi
	
	sudo -n dmidecode -t system 2>/dev/null | grep 'Manufacturer\|Product' > $TMP1file
	NECHO "Manufacturer:" "`grep "Manufacturer" $TMP1file | sed -e 's/^.*: //'`"; echo
	NECHO "Product:" "`grep "Product" $TMP1file | sed -e 's/^.*: //'`"; echo
	BogoMIPS=`grep -m 1 "^bogomips" /proc/cpuinfo | tr -d ' ' | cut -d ':' -f 2`  
	NECHO "bogoMIPS:" "$BogoMIPS"; echo		# may be meaningless but a data point nonetheless
        NECHO "cpu/vmem observation:" ""
        ( exec $CPUTEST ) 
		
	MEM=`free 2>/dev/null | grep "^Mem" | sed -e 's/^Mem:[ \t]*//' -e 's/\W\b.*$//'`
	MEM=`expr $MEM / 1024 / 1024`	# reduce to GB
	NECHO "Mem:" "${MEM} GB"
	if test $MEM -lt $minMEM
	then
		echo "   <== less than ${minMEM} GB"	# 64 GB
		INCREMENT_EXCEPTIONS
	else
		echo
	fi

	free -m > $TMP1file 2>/dev/null		# show free memory
	if test -s $TMP1file
	then
		NECHO "mem usage:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	fi

	echo
	IPforward=`cat /proc/sys/net/ipv4/ip_forward 2>/dev/null`
	NECHO "ip_forward:" "/proc/sys/net/ipv4/ip_forward=$IPforward"
	if test "$IPforward" = "1"
	then
		echo
	else
		if $CLUSTER_DEPLOYED
		then
                	if $CONTAINER_MASTER || $CONTAINER_WORKER
                	then
                        	echo "   <= might need to be 1 (enabled) for containerization"
                	else
				echo
			fi
		else
                        echo "   <= might need to be 1 (enabled) for containerization"
                fi
	fi
	echo

	if egrep -q "bond0:" /proc/net/dev 2>/dev/null
	then
		INTERFACE="bond0"
	else 
		INTERFACE=`cat /proc/net/dev 2>/dev/null | grep ":" | egrep -ve "lo:|docker0:|flannel|veth|cni" | head -1 | sed -e 's/^\s\s*//' -e 's/:.*$//'`
	fi

	if $uba41feature 
	then
		# NETWORK_INTERFACE=`egrep "^system.network.interface\b" $CASPIDAdir/conf/deployment/caspida-deployment.conf 2>/dev/null | cut -d '=' -f 2`
		GET_CASPIDA_PROPERTY system.network.interface
                NETWORK_INTERFACE="$PROPERTYvalue"
		NECHO "properties:" "system.network.interface=$NETWORK_INTERFACE"; echo
		if test "$NETWORK_INTERFACE" != ""
		then
			INTERFACE=$NETWORK_INTERFACE
		fi
	fi
		
	if test -z "$INTERFACE"
	then
	       	if which ifconfig >/dev/null 2>&1
	       	then
	                INTERFACE=`sudo -n ifconfig -s  2>/dev/null | grep BMRU |  cut -d ' ' -f 1`
		fi
	fi

	if test -z "$INTERFACE"
	then
		if which ip >/dev/null 2>&1
		then
	       		INTERFACE=`ip addr | grep -v SLAVE | grep UP |  grep -m 1 BROADCAST | tr -d ' ' | cut -d ':' -f 2`
	        fi
	fi
	
	
	NECHO "interface:" "'$INTERFACE'"
	if test -z "$INTERFACE"
	then
		INTERFACE="eth0"	# best guess
		echo "   <= assuming 'eth0'"
	else
		echo
	fi

	SPEED=`cat /sys/class/net/$INTERFACE/speed 2>/dev/null`
	if test $? -eq 0
	then
		NECHO "$INTERFACE:" "$SPEED Mb/s"
		if test "$SPEED" -lt "$minSPEED"
		then
			echo "   <== less than $minSPEED Mb/s"
			INCREMENT_EXCEPTIONS
		else
			echo
		fi
	else
		NECHO "${INTERFACE}:" "indeterminate Mb/s"; echo	# some virtual machines do not report speed
	fi

	if $MEMBERnode
	then
		ssh -q $FIRSTnode 'dd if=/dev/zero bs=64MB count=16 2>/dev/null' | dd of=/dev/null 2>$TMP1file
		if test -s $TMP1file
		then
			NECHO "transfer speed:" "`tail -1 $TMP1file`"; echo
			echo
		fi
	fi

        ifconfig $INTERFACE > $TMP1file 2>/dev/null
        if test -s $TMP1file
        then
                echo
                NECHO "ifconfig $INTERFACE:" "`head -1 $TMP1file`"; echo
                ALL_BUT_FIRST $TMP1file
        else
                ip -s link show dev $INTERFACE > $TMP1file 2>/dev/null
                if test -s $TMP1file
                then
                        echo
                        NECHO "ip show $INTERFACE:" "`head -1 $TMP1file`"; echo
                        ALL_BUT_FIRST $TMP1file
                fi
        fi

	
	if test -f /etc/sysconfig/network-scripts/ifcfg-${INTERFACE}
	then
		NECHO  "ifcfg-${INTERFACE}:" "`grep '^BOOTPROTO' /etc/sysconfig/network-scripts/ifcfg-${INTERFACE} 2>/dev/null | tee $TMP1file`"
		if ! test -s $TMP1file
		then
			echo -n "BOOTPROTO ''"
		fi
	else
		NECHO "interfaces:" ""
		if test -f /etc/network/interfaces
		then
			grep "^iface" /etc/network/interfaces 2>/dev/null | grep "$INTERFACE" > $TMP1file 
			if ! test -s $TMP1file
			then
				grep "^iface" /etc/network/interfaces.d/${INTERFACE}.cfg 2>/dev/null > $TMP1file # source'd file
			fi

			if ! test -s $TMP1file
			then
				echo -n "iface 'not found'"
			else
				echo -n `cat $TMP1file`
			fi
		else
			echo -n "iface 'file missing'"
		fi
	fi
	
	if grep -qi "dhcp" $TMP1file
	then
		echo "   (ideally, IP should be 'static')"	# if may be a reserved IP ...
	else
		echo
	fi 

        GET_CASPIDA_PROPERTY replication.enabled                   # is this a DR cluster?
        REPLICATIONstate=`echo "$PROPERTYvalue" | tr -d ' ' | tr '[:upper:]' '[:lower:]'` 
        echo
        NECHO "replication.enabled:" "'$REPLICATIONstate'"; echo
        if test "$REPLICATIONstate" = 'true'
        then
		CHECK_DEPLOYMENT caspida.cluster.replication.nodes $DEPLOYnode
        	REPLICATION_NODE_list=$DEPLOYMENTlist
		NECHO "replication nodes:" "$REPLICATION_NODE_list"; echo

		REPLICATION_NODE_list=`echo "$REPLICATION_NODE_list" | tr "," " "`
		for REPnode in $REPLICATION_NODE_list
		do
			LOOKUP  $REPnode
		done 
	fi
	 
	#
	#	Check number and size of disks
	#

	if $uba42feature
	then
		minDISK2r=`expr 2 \* $minDISK2r`
		minDISK2f=`expr 2 \* $minDISK2f`
		minAVAILdisk2=`expr 2 \*  $minAVAILdisk2`
	fi

	NUMdisks=`lsblk -ibnd --output NAME,RM,SIZE,TYPE -P 2>/dev/null | grep 'RM="0"' | tee $TMP1file | wc -l`

	if test -z "$NUMdisks"
	then
		NUMdisks=0
	fi

	echo
	NECHO "disks:" "$NUMdisks"
	if test "$NUMdisks" -lt "$reqDISKs"
	then
		echo "   <== less than $reqDISKs disk requirement"
		INCREMENT_EXCEPTIONS		
	else
		echo
	fi

	if test "$NUMdisks" -eq "$reqDISKs"
	then
		NECHO "lsbk info:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
		SIZE="`head -1 $TMP1file 2>/dev/null | sed -e 's/^.*SIZE="//' -e 's/".*$//'`"
		if test -z "$SIZE"
		then
			SIZE="0"
		else
			SIZE=`expr $SIZE / 1024 / 1024 / 1024`
		fi
		NECHO "disk1 size:" "$SIZE GB"
		if test "$SIZE" -lt "$minDISK1r"
		then
			echo "   <== smaller than required $minDISK1r GB"
			INCREMENT_EXCEPTIONS		
		else
			echo
		fi
		SIZE="`tail -1 $TMP1file 2>/dev/null | sed -e 's/^.*SIZE="//' -e 's/".*$//'`"
		if test -z "$SIZE"
		then
			SIZE="0"
		else
			SIZE=`expr $SIZE / 1024 / 1024 / 1024`
		fi
		NECHO "disk2 size:" "$SIZE GB"
		if test "$SIZE" -lt "$minDISK2r"
		then
			echo "   <== smaller than required $minDISK2r GB"
			INCREMENT_EXCEPTIONS		
		else
			echo
		fi
	else
		lsblk -d --output NAME,RM,SIZE,TYPE 2>/dev/null |\
		grep -v ' 1 ' |\
		sed "s/^/$INDENT    /" 	# simply list the disks 
	fi

        echo
	df -h -x tmpfs -x devtmpfs -P > $TMP1file 2>&1
	NECHO "partitions:" "`head -1 $TMP1file`"; echo
	ALL_BUT_FIRST $TMP1file

      	df -x tmpfs -x devtmpfs -i > $TMP1file 2>&1

	df --output=source,ipcent /home/caspida /etc/caspida /var/vcap /var/vcap2 2>/dev/null |\
        egrep -e '^/dev/' | sed -e 's/\s\s*/ /g' -e 's/%//' -e 's+/+\.+g' |\
        while read DEV AVAIL
        do
                if test $AVAIL -ge 75
                then
                        sed -ie "/^$DEV/ s/\$/   <== usage 75% or greater; inodes must be recovered/" $TMP1file
                        INCREMENT_EXCEPTIONS
                else
                        if test $AVAIL -ge 20
                        then
                                sed -ie "/^$DEV/ s/\$/   <= usage 20%; monitor closely/" $TMP1file
                        fi
                fi
        done

        if test -s $TMP1file
        then
                echo
                NECHO "inodes:" "`head -1 $TMP1file`"; echo
                ALL_BUT_FIRST $TMP1file
        fi

	echo
        mount -t ext4 > $TMP1file 2>&1
        NECHO "disk mounts:" "`head -1 $TMP1file`"; echo
        ALL_BUT_FIRST $TMP1file
	
	egrep "\sro\b" /proc/mounts > $TMP1file 2>&1
	if test -s $TMP1file
	then
		if grep -q '/var/vcap2 ' $TMP1file
		then
			sed -i '/\/var\/vcap2 / s/$/   <== \/var\/vcap2 must not be mounted read-only/' $TMP1file
			INCREMENT_EXCEPTIONS
		fi
		if grep -q '/var/vcap ' $TMP1file
		then
			sed -i '/\/var\/vcap / s/$/   <== \/var\/vcap must not be mounted read-only/' $TMP1file
			INCREMENT_EXCEPTIONS
		fi
		echo
		NECHO "read-only fs:" "`head -1 $TMP1file`"; echo
        	ALL_BUT_FIRST $TMP1file
		
		echo
	fi

	PWD=`pwd`
	cd /var/lib 
        AVAIL=`df . | egrep -ve '^Filesystem' | sed -r 's/^(\S)/\n\1/' | sed -n '/^\S/,/^$/p' | sed -e :a -e '$!N; s/\n//; ta' | sed -e 's/  */ /g' | cut -d ' ' -f 4`
        if ! test -z "$AVAIL"
        then
        	AVAIL=`expr $AVAIL / 1024 / 1024` # reduce to GB

                NECHO "/var/lib:" "available: $AVAIL GB"
                if test $AVAIL -lt $minAVAILdisk1
                then
			HALFoMIN=`expr $minAVAILdisk1 / 2`
			if test $AVAIL -lt $HALFoMIN
			then 
                		echo "   <== less than ${minAVAILdisk1} GB available in /var/lib"
                        	INCREMENT_EXCEPTIONS
			else
                		echo "   <= less than ${minAVAILdisk1} GB available in /var/lib"
			fi
			df -h . | sed "s/^/$INDENT/" 
                else
                	echo
                fi
	fi
	cd $PWD
	echo

	NECHO "directory info:" ""; echo
	DIRlist="/ /etc/caspida /opt/caspida $CASPIDAhome /var/vcap /var/log/caspida"
	if $uba42feature 
	then
		if test -d /var/vcap2
		then
			DIRlist="$DIRlist /var/vcap2"
		fi
	fi
	for DIR in $DIRlist
	do 
		NECHO "  $DIR" "`df -h $DIR 2>/dev/null | tail -1`"; echo
	done
	echo

	OSrelease="`lsb_release -ds 2>/dev/null | tr -d '\"'| sed -e 's/ *$//'`"
	if test "$OSrelease" = ""
	then
		OSrelease=`hostnamectl 2>/dev/null | grep "Operating System:" | sed -e 's/^.*: //'`
		if test "$OSrelease" = "Red Hat Enterprise Linux"
		then
			OSrelease=`cat /etc/redhat-release`
		fi
	fi
	
	echo "$OSrelease" > $OSfile	# value for later
	NECHO "current os:" "'$OSrelease'"

	RHELrelease=true		# limits.conf is RHEL/CentOS requirement
	RHELrelease72=false;
	CERTIFIED=false

	case "$OSrelease" in
		"Ubuntu 14.04.2 LTS")						RHELrelease=false; CERTIFIED=true;;
		"Ubuntu 14.04.3 LTS")						RHELrelease=false; CERTIFIED=true;;
		"Ubuntu 14.04.5 LTS")						RHELrelease=false; CERTIFIED=true;;
		"Ubuntu 16.04.3 LTS")                                   	RHELrelease=false; CERTIFIED=true;;
		"Red Hat Enterprise Linux Server release 6.7 (Santiago)")	RHELrelease=true; CERTIFIED=true;;
		"Red Hat Enterprise Linux Server release 7.2 (Maipo)")		RHELrelease72=true; CERTIFIED=true;;
		"CentOS Linux release 7.2.1511 (Core)") 			RHELrelease72=true; CERTIFIED=true;;
                "Red Hat Enterprise Linux Server release 6.9 (Santiago)")       RHELrelease=true; CERTIFIED=true;;
                "Red Hat Enterprise Linux Server release 7.3 (Maipo)")          RHELrelease72=true; CERTIFIED=true;;
                "CentOS Linux release 7.3.1611 (Core)")                         RHELrelease72=true; CERTIFIED=true;;
		"Red Hat Enterprise Linux Server release 7.4 (Maipo)")		RHELrelease72=true; CERTIFIED=true;;
		"Red Hat Enterprise Linux Server release 7.5 (Maipo)")		RHELrelease72=true; CERTIFIED=true;;
		"CentOS Linux release 7.4.1708 (Core)") 			RHELrelease72=true; CERTIFIED=true;;
		"CentOS Linux release 7.5.1804 (Core)") 			RHELrelease72=true; CERTIFIED=true;;
		Oracle*7.4*)	RHELrelease72=true;;
		Oracle*7.5*)	RHELrelease72=true;;
	esac

	if $uba33feature && ! $uba40feature
	then
		CERTIFIED=false
        	case "$OSrelease" in
			"Ubuntu 14.04.5 LTS")						RHELrelease=false; CERTIFIED=true;;
                	"Red Hat Enterprise Linux Server release 6.7 (Santiago)")       CERTIFIED=true;;
                	"Red Hat Enterprise Linux Server release 7.2 (Maipo)")          RHELrelease72=true; CERTIFIED=true;;
                	"CentOS Linux release 7.3.1611 (Core)")                         RHELrelease72=true; CERTIFIED=true;;
        	esac
	fi

	if $uba40feature && ! $uba41feature
	then
		CERTIFIED=false
        	case "$OSrelease" in
			"Ubuntu 16.04.3 LTS")                                           RHELrelease=false; CERTIFIED=true;;
			"Ubuntu 16.04.4 LTS")                                           RHELrelease=false; CERTIFIED=true;;
			"Ubuntu 16.04.5 LTS")                                           RHELrelease=false; CERTIFIED=true;;
			"Ubuntu 16.04.6 LTS")                                           RHELrelease=false; CERTIFIED=true;;
                	"Red Hat Enterprise Linux Server release 6.9 (Santiago)")       CERTIFIED=true;;
                	"Red Hat Enterprise Linux Server release 7.3 (Maipo)")          RHELrelease72=true; CERTIFIED=true;;
		esac
	fi

	if $uba41feature
	then
		CERTIFIED=false
        	case "$OSrelease" in
			"Red Hat Enterprise Linux Server release 7.4 (Maipo)")	RHELrelease72=true; CERTIFIED=true;;
			"Red Hat Enterprise Linux Server release 7.5 (Maipo)")	RHELrelease72=true; CERTIFIED=true;;
			"CentOS Linux release 7.4.1708 (Core)") 		RHELrelease72=true; CERTIFIED=true;;
			"CentOS Linux release 7.5.1804 (Core)") 		RHELrelease72=true; CERTIFIED=true;;
			"Ubuntu 16.04.3 LTS")                                   RHELrelease=false; CERTIFIED=true;;
			"Ubuntu 16.04.4 LTS")                                   RHELrelease=false; CERTIFIED=true;;
			"Ubuntu 16.04.5 LTS")                                   RHELrelease=false; CERTIFIED=true;;
			"Ubuntu 16.04.6 LTS")                                   RHELrelease=false; CERTIFIED=true;;
			Oracle*7.4*)	RHELrelease72=true;;
			Oracle*7.5*)	RHELrelease72=true;;
		esac
	fi

	if $uba42feature
	then
		CERTIFIED=false
        	case "$OSrelease" in
			"Red Hat Enterprise Linux Server release 7.5 (Maipo)")	RHELrelease72=true; CERTIFIED=true;;
			"Red Hat Enterprise Linux Server release 7.6 (Maipo)")  RHELrelease=true; RHELrelease72=true;;
			"CentOS Linux release 7.5.1804 (Core)") 		RHELrelease72=true; CERTIFIED=true;;
			"CentOS Linux release 7.6.1810 (Core)")                 RHELrelease=true; RHELrelease72=true;;
			"Ubuntu 16.04.3 LTS")                                   RHELrelease=false; CERTIFIED=true;;
			"Ubuntu 16.04.4 LTS")                                   RHELrelease=false; CERTIFIED=true;;
			"Ubuntu 16.04.5 LTS")                                   RHELrelease=false; CERTIFIED=true;;
			"Ubuntu 16.04.6 LTS")                                   RHELrelease=false; CERTIFIED=true;;
			Oracle*7.5*)						RHELrelease72=true; CERTIFIED=true;;
			Oracle*7.6*)						RHELrelease72=true; CERTIFIED=true;;
		esac
	fi

	if $uba43feature
	then
		CERTIFIED=false
        	case "$OSrelease" in
			"Red Hat Enterprise Linux Server release 7.6 (Maipo)")  RHELrelease=true; RHELrelease72=true; CERTIFIED=true;;
			"CentOS Linux release 7.6.1810 (Core)")                 RHELrelease=true; RHELrelease72=true; CERTIFIED=true;;
			"Ubuntu 16.04.3 LTS")                                   RHELrelease=false; CERTIFIED=true;;
			"Ubuntu 16.04.4 LTS")                                   RHELrelease=false; CERTIFIED=true;;
			"Ubuntu 16.04.5 LTS")                                   RHELrelease=false; CERTIFIED=true;;
			"Ubuntu 16.04.6 LTS")                                   RHELrelease=false; CERTIFIED=true;;
			Oracle*7.6*)						RHELrelease=true; RHELrelease72=true; CERTIFIED=true;;
		esac
	fi

        if $uba50feature
        then
                CERTIFIED=false
                case "$OSrelease" in
                        "CentOS Linux release 7.7.1908 (Core)") RHELrelease=true; RHELrelease72=true; CERTIFIED=true;;
                        "Red Hat Enterprise Linux Server release 7.7 (Maipo)")    RHELrelease=true; RHELrelease72=true; CERTIFIED=true;;
                        Oracle*7.7*)                                              RHELrelease72=true; RHELrelease=true; CERTIFIED=true;;
                        "Ubuntu 16.04.3 LTS")                                     RHELrelease=false; CERTIFIED=true;;
                        "Ubuntu 16.04.4 LTS")                                     RHELrelease=false; CERTIFIED=true;;
                        "Ubuntu 16.04.5 LTS")                                     RHELrelease=false; CERTIFIED=true;;
                        "Ubuntu 16.04.6 LTS")                                     RHELrelease=false; CERTIFIED=true;;
                esac
        fi

 	if $uba503feature
 	then
 		CERTIFIED=false
 		case "$OSrelease" in
 			"CentOS Linux release 7.7.1908 (Core)") RHELrelease=true; RHELrelease72=true; CERTIFIED=true;;
 			"CentOS Linux release 7.8."*) RHELrelease=true; RHELrelease72=true; CERTIFIED=true;;
 			"Red Hat Enterprise Linux Server release 7.7 (Maipo)")	  RHELrelease=true; RHELrelease72=true; CERTIFIED=true;;
 			"Red Hat Enterprise Linux Server release 7.8 (Maipo)")	  RHELrelease=true; RHELrelease72=true; CERTIFIED=true;;
 			Oracle*7.7*)						  RHELrelease72=true; RHELrelease=true; CERTIFIED=true;;
                         "Ubuntu 16.04.3 LTS")                                     RHELrelease=false; CERTIFIED=true;;
                         "Ubuntu 16.04.4 LTS")                                     RHELrelease=false; CERTIFIED=true;;
                         "Ubuntu 16.04.5 LTS")                                     RHELrelease=false; CERTIFIED=true;;
                         "Ubuntu 16.04.6 LTS")                                     RHELrelease=false; CERTIFIED=true;;
 		esac
 	fi

	if $RHELrelease72
	then
		RHELrelease=true
	fi

	if $CERTIFIED
	then
		echo
	else
		if $CASPIDAuser
		then
			UBArelease="`cat $UBAfile`"
			echo "   <== not certified for UBA '$UBArelease'"
			INCREMENT_EXCEPTIONS
		else
			echo
		fi
		case "$OSrelease" in
			Red*|CentOS*|Oracle*)	RHELrelease=true;;
			*)		RHELrelease=false;;
		esac
	fi

        if test -r /proc/sys/crypto/fips_enabled
        then
                echo
                FIPSval=`cat /proc/sys/crypto/fips_enabled`
                NECHO "fips enabled:" "$FIPSval"; echo
                if test "$FIPSval" = "1"
                then
                        export OPENSSL_FIPS=1           # let openssl know
                fi
        fi

	if which java >/dev/null 2>&1
	then
        	java -version > $TMP1file 2>&1
		ERROR=""
		if $uba32feature
		then
			if ! egrep -q -e 'version.*1.8' $TMP1file
			then
				ERROR="<== version 1.8 required"
				INCREMENT_EXCEPTIONS
			fi
		fi
	
        	NECHO "java version:" "`head -1 $TMP1file`   $ERROR";echo
        	ALL_BUT_FIRST $TMP1file
	else
		NECHO "java version:" "''   <= java not installed"; echo
	fi
		
		
	SELINUXfile="/etc/selinux/config"

	if test -f /etc/sysconfig/selinux
	then
		echo
		SELINUX=`egrep "^SELINUX=" $SELINUXfile | cut -d "=" -f 2`
		NECHO "/etc/selinux/config:" "SELINUX=${SELINUX}"
		case "$SELINUX" in
			disabled)	echo;;
			permissive)	echo;;
			*)		echo "   <== 'disabled' or 'permissive' expected"
					INCREMENT_EXCEPTIONS;;
		esac

		NECHO "getenforce:" "`getenforce 2>&1`"; echo

		sestatus > $TMP1file 2>&1
		STATUSconfig=`egrep "^Mode from config file:" $TMP1file | sed -e 's/^..*:\s\s*//'`
		STATUScurrent=`egrep "^Current mode:" $TMP1file | sed -e 's/^..*:\s\s*//'`
		if test "$STATUSconfig" != "$STATUScurrent"
		then
			sed -i -e '/^Current mode:/ s/$/   <== does not match config/' $TMP1file
			INCREMENT_EXCEPTIONS
		fi
		NECHO "sestatus:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file

		if selinuxenabled >/dev/null 2>&1
		then
			NECHO "selinuxenabled:" "'true'"
			
			BOOTtimeReadable=`who -b 2>/dev/null| sed -e 's/\s\s*/ /g' -e 's/ system boot //'`

			ERROR=""
			if expr 0 + "$BOOTtimeReadable" >/dev/null 2>&1
			then
				BOOTtimeEpoch=`date --date "$BOOTtimeReadable" +%s 2>/dev/null`
				if expr 0 + "$BOOTtimeEpoch"  >/dev/null 2>&1
				then
					SELINUXtimeMod=`stat -c '%Z' $SELINUXfile 2>/dev/null`
					if expr 0 + "$SELINUXtimeMod" >/dev/null 2>&1
					then
						if test "$SELINUXtimeMod" -gt "$BOOTtimeEpoch"
						then
							ERROR="   <= $SELINUXfile modifed since re-boot"
						fi
					
					fi
				fi
			fi
			echo "$ERROR"
		fi
	fi

	if $RHELrelease && $uba41feature
	then
		BRIDGEfile="/proc/sys/net/bridge/bridge-nf-call-iptables"
		if test -r $BRIDGEfile
		then
			BRIDGE=`cat $BRIDGEfile 2>/dev/null`
			echo
			NECHO "bridge-nf-call-iptables:" "'$BRIDGE'"
			if test "$BRIDGE" = "1"
			then
				echo
			else
				echo "   <==  $BRIDGEfile must be set to '1' for kubernetes"
				INCREMENT_EXCEPTIONS
			fi
		fi
	fi

	IPV6disabled=`cat /proc/sys/net/ipv6/conf/all/disable_ipv6 2>/dev/null`
	NECHO "disable_ipv6:" "'$IPV6disabled'"; echo
	echo
	# if $RHELrelease && $uba41feature
	# then
		# if test "$IPV6disabled" = "0"
		# then
			# echo
		# else
			# echo "   <= expecting /proc/sys/net/ipv6/conf/all/disable_ipv6 to be '0'"
		# fi
	# else
		# echo
	# fi
	# echo

	#
	#	the hope is that no one has implemented firewall rules that prevent Splunk>UBA from communicating
	#


	if which timedatectl >/dev/null 2>/dev/null
	then
		if timedatectl status > $TMP1file 2>/dev/null 
		then
			TIMEzone="`egrep 'Time\s*zone:' $TMP1file | sed -e 's/^.*Time\s*zone: //'`"
			NECHO "timedatectl status:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
		else
			> $TMP1file
			TIMEzone="`cat /etc/timezone 2>/dev/null`"
		fi

	        if test -s $TMP1file
        	then
                	UTCtimestamp=`grep Universal $TMP1file | sed -e 's/^[^:]*: //'`
                	RTCtimestamp=`grep 'RTC time:' $TMP1file | sed -e 's/^[^:]*: //'`

	                LOCALtz="`egrep 'Time\s*zone:' $TMP1file | sed -e 's/^.*Time\s*zone: //' -e 's/\s.*$//'`"

                	RTClocal=`egrep 'RTC in local TZ' $TMP1file | sed -e 's/\s//g' -e 's/^.*://'`
                	if test "$RTClocal" = "yes"
                	then
                        	RTCtz="$LOCALtz"
                	else
                        	RTCtz="UTC"
                	fi

                	UTCsecs=`TZ=UTC date --date="$UTCtimestamp" "+%s"`
                	RTCsecs=`TZ="$RTCtz" date --date="$RTCtimestamp" "+%s"`
                	DIFFsecs=`expr $UTCsecs - $RTCsecs`
                	echo
                	NECHO "clocks:" "UTC: $UTCsecs; RTC: $RTCsecs; diff: $DIFFsecs"
			ABSdiff=`expr $DIFFsecs \* $DIFFsecs`
                	if test $ABSdiff -lt 26
                	then
                        	echo    # +/- 5 seconds acceptable
                	else
				if test $DIFFsecs -gt 0
				then
                                	if test $DIFFsecs -ge 300
                                	then
                                        	echo "   <== future skew from RTC"
                                        	INCREMENT_EXCEPTIONS
                                	else
						if test  $DIFFsecs -ge 60
                                        	then
                                               		echo "   <= future skew from RTC"
                                        	else
                                               		echo "   (future skew from RTC)"
                                        	fi
                                	fi
                        	else
                                	echo "   <= past skew from RTC"
				fi
                	fi
        	fi

		echo
		NECHO "timezone:" "$TIMEzone"; echo
	
		#
		#	The concern is that clocks may skew on virtual machines
		#
	
		timedatectl | egrep -e 'NTP|Network' | tr -d ' ' > $TMP1file
		NTPenabled=`egrep -e 'enabled:|timeon:' $TMP1file | cut -d ":" -f 2`
		NECHO "ntp enabled:" "$NTPenabled";echo
	
		NTPsynced=`grep synchronized $TMP1file | cut -d ":" -f 2`
		NECHO "ntp synced:" "$NTPsynced"
		if test "$NTPsynced" = "yes"
		then
			echo
		else
			if $MULTInode
			then
				echo "   <== NTP synchronization required for multi-node clusters"
		        	INCREMENT_EXCEPTIONS
			else
				echo "   <= NTP synchronization recommended for single-node clusters"
			fi
		fi
	else
		if which service >/dev/null 2>/dev/null
		then
			sudo service ntp status | cat -v  > $TMP1file 2>/dev/null
			NECHO "ntpd:" "`head -1 $TMP1file`"; echo
		fi

		if grep -q "stopped" $TMP1file
		then
			if which chkconfig >/dev/null 2>&1
			then
				chkconfig --list ntpdate  2>/dev/null | sed -e 's/[ \t][ \t]*/ /g' > $TMP1file
				NECHO "ntpdate synced:" "`head -1 $TMP1file`"
				if grep -q "on" $TMP1file
				then
					echo
				else
					echo "   <== NTP synchronization recommended for multi-node clusters"
                                	INCREMENT_EXCEPTIONS	
				fi 
			fi
		fi
	fi

	if test -r /etc/ntp.conf
	then
		egrep -e "^server " /etc/ntp.conf > $TMP1file
		if test -s $TMP1file
		then
			NECHO "ntp.conf:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
			echo
		fi
	fi

	if ntpq -p > $TMP1file 2>/dev/null
	then
		if test -s $TMP1file
		then
			NECHO "ntpq -p:" "`head -1 $TMP1file`"; echo
			ALL_BUT_FIRST $TMP1file
			echo
		fi
	fi

	if sudo which ntpdate >/dev/null 2>&1
	then	
		NTPserver=`grep -m 1 "^server" /etc/ntp.conf 2>/dev/null | cut -d " " -f 2`
		if test -z "$NTPserver"
		then
			NTPserver=0.pool.ntp.org
		fi
		echo "${INDENT}... date query of $NTPserver will take a few seconds ..."
		NECHO "ntpdate:" "`sudo ntpdate -q $NTPserver 2>&1 | tail -1` ($NTPserver)"; echo
	fi

	NECHO "system date:" "`date '+%e %b %T %Z %Y'`"; echo

	if ! $RHELrelease
	then
		( echo -n "ufw, "; sudo -n ufw status) 2>&1
        else
		if systemctl status iptables >/dev/null 2>&1
		then
			(echo -n "iptables, "; sudo -n service iptables status | cat -v)  2>&1	
		else
                        (echo -n "firewalld, "; sudo -n firewall-cmd --info-zone=trusted )  2>&1
                fi
	fi > $TMP1file
	
	NECHO "firewall:" "`head -1 $TMP1file`"; echo
	ALL_BUT_FIRST $TMP1file

	echo


#
#	A small bit of logic to test that nodes can communicate on a 'registered' port not used in UBA environemt
#
#	using netcat to start listener on the leader node
#

	NECHO "upc listener node:" "'$FIRSTnode'"; echo

        if $LEADERnode
        then
                if which killall >/dev/null 2>&1
                then
                        if which nc > $TMP1file 2>/dev/null
                        then
                                rm -f ~/bin/$LISTENprog >/dev/null 2>&1
                                ln -s `cat $TMP1file` ~/bin/$LISTENprog  >/dev/null 2>&1   # symbolic name is easier to killall
			else
				LISTENport=""
				ERROR="   (nc not in PATH)"
                        fi


                        if test -h ~/bin/$LISTENprog >/dev/null 2>&1
                        then
                        	killall $LISTENprog >/dev/null 2>&1                     # kill any lingering listeners

                                for I in 1 2
                                do
                                        ss -al > $TMP1file 2>/dev/null; netstat -Wave >> $TMP1file 2>/dev/null
                                        if egrep -qe ":${LISTENport}\b" $TMP1file 2>/dev/null
                                        then
                                                ERROR="   (already in use)"
						LISTENmode=false
                                        else
                                                ERROR=""
                                                LISTENmode=true
                                                break
                                        fi
                                        sleep 35
                                done

                                if $LISTENmode
                                then
                                        for I in $NODElist
                                        do
                                                cd ~/bin
                                                (echo -e 'HTTP/1.1 200 OK\r\n'; echo "$LISTENresp" )  | ./$LISTENprog -l $LISTENport
                                        done >/dev/null 2>&1 &
                                        LISTENpid=$!
				else
					LISTENport=""
					ERROR="   (not listening)"
                                fi
			else
				LISTENport=""
				ERROR="   ( symlink ~/bin/$LISTENprog doesn't exist )"	
                        fi
		else
			LISTENport=""
                        ERROR="   (killall not in PATH )"
                fi
	else
		ERROR=""
        fi

	NECHO "upc listener port:" "'$LISTENport'"
	echo "$ERROR"

	if $LEADERnode
	then
        	if test "$LISTENpid" != ""
        	then
        		if ps $LISTENpid >/dev/null 2>&1
        		then 
				ERROR="   (running)"	
			else
        			LISTENpid=""           # already finished
				ERROR="   (finished)"
        		fi
		else
			ERROR="   (not started)"
        	fi
		NECHO "upc listener pid:" "'$LISTENpid'"; echo "$ERROR"
	fi

	> $TMP1file
	> $TMP2file

        URL="http://${FIRSTnode}:$LISTENport"

        NECHO "upc listener response:" ""

        for I in 1 LAST 
	do
		if eval curl --noproxy --fail --silent --show-error  --output $TMP1file --connect-timeout 30 $URL 2>$TMP2file >/dev/null
        	then
                	if grep -q "$LISTENresp" $TMP1file 2>/dev/null
                	then
                        	echo "$LISTENresp   (success)"
                	else
                        	echo "`head -1  $TMP1file`   (unexpected response)"
                       		ALL_BUT_FIRST $TMP1file
                	fi
			break			# realized!
        	else
			if test $I != "LAST" 
			then
				sleep 35
				continue	# try again
			fi
                	echo    "`head -1 $TMP2file`   (verify all ports are open between nodes)"
                	ALL_BUT_FIRST $TMP2file
        	fi
	done

	echo

#	if $MULTInode
#	then
#		for NODE in $NODElist
#		do
#			NECHO "testing firewall:" "$THISnode to $NODE using ntpdate with UDP"
#			if sudo ntpdate -qu $NODE > $TMP1file 2>&1
#			then
#				echo -n "   (unprivileged ok)"
#			else
#				echo -n "   <== verify that service is running and necessary unprivileged ports are open"
#				INCREMENT_EXCEPTIONS
#			fi
#			if sudo ntpdate -q $NODE > $TMP1file 2>&1
#			then
#				echo "   (privileged ok)"
#			else
#				echo "   <== verify that service is running and necessary privileged ports are open"
#				INCREMENT_EXCEPTIONS
#			fi
#		done
#	fi
	
	if ! $CASPIDAuser
	then
		echo	
		echo	"   <= Nothing more can be done without using 'caspida' user =>"
		INCREMENT_EXCEPTIONS
		echo
		echo
		exit
	fi

	echo
	CASPIDAuid=`id -u`
	NECHO "uid:"	"$CASPIDAuid"

	CASPIDAuserName=`getent passwd "$CASPIDAuid" | cut -d ':' -f 1`
	if test "$CASPIDAuserName" != 'caspida'
	then
		echo -n "   <== is '$CASPIDAuserName' and not 'caspida'"
		INCREMENT_EXCEPTIONS
	fi

	if $MULTInode
	then
		if $LEADERnode
		then
			echo
		else
			LEADERuid=`ssh $FIRSTnode id -u 2>/dev/null`
			if test "$CASPIDAuid" = "$LEADERuid"
			then
				echo
			else
				echo "   <== does not match uid '$LEADERuid' on first node"
				INCREMENT_EXCEPTIONS
			fi
		fi
	else
		echo
	fi
	
	CASPIDAgid=`id -g`
	NECHO "gid:"	"$CASPIDAgid"

	CASPIDAgroupName=`getent group "$CASPIDAgid" | cut -d ':' -f 1`
	if test "$CASPIDAgroupName" != 'caspida'
	then
		echo -n "   <== is '$CASPIDAgroupName' and not 'caspida'"
		INCREMENT_EXCEPTIONS
	fi

	if $MULTInode
	then
		if $LEADERnode
		then
			echo
		else
			LEADERgid=`ssh $FIRSTnode id -g 2>/dev/null`
			if test "$CASPIDAgid" = "$LEADERgid"
			then
				echo
			else
				echo "   <== does not match gid '$LEADERgid' on first node"
				INCREMENT_EXCEPTIONS
			fi
		fi
	else
		echo
	fi
	
	UMASK=`umask`
	NECHO "umask:" "$UMASK"
	if test "$UMASK" = "0002" -o "$UMASK" = "0022"
	then
		echo
	else
		echo "   <== 'caspida' umask is incorrect; must be at minimum '0022'"
		INCREMENT_EXCEPTIONS
	fi

#	if $RHELrelease
#	then
#		# make sure that soft/hard limits are set for caspida user
#		egrep -e '^caspida\b' /etc/security/limits.conf 2>/dev/null > $TMP1file
#		COUNT=`egrep -e '\bhard\b|\bsoft\b' < $TMP1file | egrep -e '\bunlimited$|\b32768$'| wc -l`
#		if test $COUNT -ne 7
#		then
#			NECHO "limits.conf:" "   <= 'caspida' entries missing or incorrect in /etc/security/limits.conf"; echo
#		else
#			NECHO "limits.conf:" "`head -1 $TMP1file`"; echo
#			ALL_BUT_FIRST $TMP1file
#		fi
#	fi
	
	(echo "ulimit -a" | bash) 2>/dev/null > $TMP1file

	VAL=`echo "ulimit -c" | bash`
	if test "$VAL" != "unlimited"
	then
		sed -i -e "/^core file/ s/\$/   \('unlimited' expected for 'core file size'; correct ulimit for 'caspida' user\)/" $TMP1file
	fi
	
	VAL=`echo "ulimit -l" | bash`
	if test "$VAL" != "unlimited"
	then
		sed -i -e "/^max locked/ s/\$/   \('unlimited' expected for 'max locked memory'; correct ulimit for 'caspida' user\)/" $TMP1file
	fi
	
	VAL=`echo "ulimit -s" | bash`
	if test "$VAL" != "unlimited"
	then
		sed -i -e "/^stack size/ s/\$/   \('unlimited' expected for 'stack size'; correct ulimit for 'caspida' user\)/" $TMP1file
	fi
	
	VAL=`echo "ulimit -u" | bash`
	if test "$VAL" = "unlimited"
	then
		:
	else
		if test "$VAL" -lt "128000"
		then
			sed -i -e "/^max user/ s/\$/   <= 'unlimited' expected for 'max user processes'; correct ulimit for 'caspida' user/" $TMP1file
		else
			sed -i -e "/^max user/ s/\$/   \('unlimited' expected for 'max user processes')/" $TMP1file
		fi
	fi
	
	VAL=`echo "ulimit -n" | bash`
	if test "$VAL" -lt  "32768" 
	then
		sed -i -e "/^open files/ s/\$/   <== at least '32768' expected for 'open files'; correct ulimit for 'caspida' user/" $TMP1file
	fi
	
	NUMexceptions=`grep " <== " $TMP1file | wc -l`
	if test "$NUMexceptions" != "0"
	then
		INCREMENT_EXCEPTIONS $NUMexceptions
	fi
	NECHO "ulimit:" "`head -1 $TMP1file`"; echo 
	ALL_BUT_FIRST $TMP1file
	
	echo
	egrep -e "^passwd:|^group:|^shadow:|^hosts:|^networks:" /etc/nsswitch.conf > $TMP1file
        NECHO "/etc/nsswitch.conf:" "`head -1 $TMP1file`"; echo
        ALL_BUT_FIRST $TMP1file

	echo
	USERlist="caspida redis solr hbase sentry yarn mapred flume hdfs hive impala redis zookeeper postgres influxdb"
	if ! $uba41feature
	then
		USERlist="$USERlist neo4j"
	fi
	for U in $USERlist
	do 
		if egrep "^$U:" /etc/passwd
		then :
		else 
			echo "$U   <== username '$U' not found in /etc/passwd" 
			INCREMENT_EXCEPTIONS
			if getent passwd $U > $TMP2file
			then
				NECHO "" ""; head -1 $TMP2file
				ALL_BUT_FIRST $TMP2file
				while read LINE
				do
					GID=`echo "$LINE" | cut -d ':' -f 4`
					if test "$GID" != ""
					then
						if grep ":${GID}:" /etc/group > $TMP3file
						then
							NECHO "" "/etc/group: "; head -1 $TMP3file
						else
							NECHO "" "'$GID' not in /etc/group"
							getent group $GID > $TMP3file
							NECHO "" ""; head -1 $TMP3file
							ALL_BUT_FIRST $TMP3file
						fi 
					fi
				done < $TMP2file
			fi
		fi
	done > $TMP1file
	NECHO "/etc/passwd:" "`head -1 $TMP1file`"; echo
	ALL_BUT_FIRST $TMP1file
	echo

	for G in caspida redis postgres zookeeper hadoop hdfs yarn mapred hbase solr sentry hive impala flume influxdb
	do 
		if ! egrep "^$G:" /etc/group
		then 
			if test "$G" != "neo4j"
			then
				echo "$G   <= groupname '$G' not found in /etc/group" 
				# INCREMENT_EXCEPTIONS
			else
				echo "$G   <= groupname '$G' not found in /etc/group" 
			fi
			echo -n "    "
			getent group $G
		fi
	done > $TMP1file
	NECHO "/etc/group:" "`head -1 $TMP1file`"; echo
	ALL_BUT_FIRST $TMP1file

	echo
	for U in caspida impala hdfs yarn mapred
	do
		NECHO "user '$U' groups:" ""; id -G $U
		id -Gn $U > $TMP1file
		G=$U
		if ! grep -q $G $TMP1file
		then
			ERROR="<= user '$U' not in group '$G' list"
		else
			ERROR=""
		fi
		if test $U = "caspida"
		then
			G="redis"
			if ! grep -q "$G" $TMP1file
			then
				ERROR="<= user '$U' not in group '$G' list   $ERROR"
			fi
		else
			if test $U = "impala"
			then
				for G in hive # hdfs
				do
					if ! grep -q $G $TMP1file
					then
						ERROR="<= user '$U' not in group '$G' list   $ERROR"
					fi
				done
			else
				G="hadoop"
				if ! grep -q $G $TMP1file
				then
					ERROR="<= user '$U' not in group '$G' list    $ERROR"
				fi
			fi
		fi

		if test "$ERROR" != ""
		then
			# INCREMENT_EXCEPTIONS
			:
		fi
			
		NECHO "" "`head -1 $TMP1file`"; echo "   $ERROR"
	done
	ERROR=""

	echo

	if sudo -n printenv PATH  > $TMP1file 2> $TMP2file		# does sudo work?
	then
	# 	echo	
	# 	if sudo ls -l /etc/sudoers.d/caspida > $TMP1file 2>/dev/null
	# 	then
	# 		:
	# 	else
	# 		NECHO "sudoers.d:" "'/etc/sudoers.d/caspida'"
	# 		echo "   <= does not exist and would be ideal"
	# 	fi

		SECUREPATH=`cat $TMP1file`
	
		NECHO "secure_path:" "$SECUREPATH"
	
		for DIR in `echo "$reqSECUREPATH" | tr ':' ' '`
		do
			if grep -q "$DIR" $TMP1file 2>/dev/null
			then
				continue
			else
				echo "   <== sudoers secure_path required for 'caspida' user"
				INCREMENT_EXCEPTIONS
				break
			fi
		done | tee $TMP2file

		if test ! -s $TMP2file
		then
			echo	# no errors found
		fi

		if ! ssh $THISnode sudo -n printenv PATH > $TMP1file 2>$TMP2file
		then
			FIRSTline="`head -1 $TMP2file | tr -d '\r'`"
			NECHO "tty required?" ""
			echo "$FIRSTline   <== validate 'sudoers' in your deployment; !requiretty for 'caspida' user may be needed"
			ALL_BUT_FIRST $TMP2file
			INCREMENT_EXCEPTIONS
		fi
	else
		echo
		NECHO "sudoers:" "sudo failed for 'caspida' user - `head -1 $TMP2file`   <== validate 'sudoers' before UBA setup|start"; echo
		echo
		INCREMENT_EXCEPTIONS
	fi

	#
        #       check to certain that ssh sudo is successful between all nodes in the cluster
        #       without out any prompts
        #

        if $CASPIDAuser
	then 
		
		RANDOMdelay=`echo 'echo $RANDOM' | bash`
        	RANDOMdelay=`expr $RANDOMdelay \% 10`
		sleep $RANDOMdelay			# sleep 0 to 9 seconds to reduce collisions

		for NODE in $NODElist
        	do
                	NECHO "sudo connectivity:" "$THISnode to $NODE"
                	if ssh  $NODE "sudo service sshd status 2>&1" >$TMP1file 2>&1
                	then
                        	echo "   ('ssh $NODE sudo service sshd status' okay)"
                	else
                        	echo "   <== 'ssh $NODE sudo service sshd status' from $THISnode failed; validate 'sudoers' on $NODE before UBA setup|start"
                        	INCREMENT_EXCEPTIONS
				sed -ie 's/^/    /' $TMP1file
				NECHO "" "`head -1 $TMP1file`"; echo
				ALL_BUT_FIRST $TMP1file
                	fi
        	done
	fi
	
#		if ! which zip >/dev/null 2>/dev/null
#		then
#			NECHO "zip:" "   <== required program is not available"; echo
#			INCREMENT_EXCEPTIONS
#		fi
	
	if crontab -l 2>/dev/null | egrep -v '^#|^ ' > $TMP1file		# show crontab -u caspida
 	then 
 		:		# it was successful
 	else
 		sudo crontab -u caspida -l 2> $TMP2file | egrep -v '^#|^ ' > $TMP1file
 	fi

	if test -s $TMP1file
	then
		echo
		NECHO "caspida crontab:" "`head -1 $TMP1file`"; echo
		ALL_BUT_FIRST $TMP1file
	fi

	for DIR in /home/caspida/.ssh /opt/caspida /etc/caspida/local
	do
		if test -d $DIR
		then 
			# stat -c '%F %a %U %G %n' $DIR
			ls -ld $DIR
			find $DIR -exec ls -ld {} \; 
		fi
	done |\
	grep -v " caspida caspida " > $TMP1file

	if test -s $TMP1file
	then
		echo
		NECHO "owner group:" "`head -1 $TMP1file`"
		echo "   <= caspida:caspida is expected owner:group"
		ALL_BUT_FIRST $TMP1file
	fi

	echo
	
	#
	#	Verify partition space
	#

	PWD="`pwd`"	# save current directory
	
	if cd $ZOOKEEPERdir >/dev/null 2>&1
	then
		ls -ld $ZOOKEEPERdir >$TMP1file 2>/dev/null
		NECHO "${ZOOKEEPERdir}:" "`cat $TMP1file`"
	
		# '.' is the current directory, will follow symbolic link
	
		ls -ld . >$TMP2file 2>/dev/null
		if grep -q " zookeeper zookeeper " $TMP2file >/dev/null 2>&1
		then
			echo
		else
			echo "   <== owner/group are not 'zookeeper'"
			INCREMENT_EXCEPTIONS
		fi
	
		#
		#	make sure that there's space on /var/lib for zookeeper
		#
	
		AVAIL=`df . | egrep -ve '^Filesystem' | sed -r 's/^(\S)/\n\1/' | sed -n '/^\S/,/^$/p' | sed -e :a -e '$!N; s/\n//; ta' | sed -e 's/  */ /g' | cut -d ' ' -f 4`
		if ! test -z "$AVAIL"
		then
			AVAIL=`expr $AVAIL / 1024 / 1024` # reduce to GB
	
			NECHO "${ZOOKEEPERdir}:" "available: $AVAIL GB"
			if test $AVAIL -lt $minAVAILdisk1
			then
                        	HALFoMIN=`expr $minAVAILdisk1 / 2`
                        	if test $AVAIL -lt $HALFoMIN
                        	then
                                	echo "   <== less than ${minAVAILdisk1} GB available in $ZOOKEEPERdir"
                                	INCREMENT_EXCEPTIONS
                        	else
                               	 	echo "   <= less than ${minAVAILdisk1} GB available in $ZOOKEEPERdir"
                       	 	fi
				df -h . | sed "s/^/$INDENT/" 
			else
				echo
			fi
		fi
	else
		NECHO "${ZOOKEEPERdir}:" "$ZOOKEEPERdir   <== does not exist"; echo
		INCREMENT_EXCEPTIONS
	fi
	
	if cd $CASPIDAdir >/dev/null 2>&1
	then
		ls -ld $CASPIDAdir >$TMP1file 2>/dev/null
		NECHO "$CASPIDAdir:" "`cat $TMP1file`"
	
		# '.' is the current directory, will follow symbolic link
	
		TIMESTAMP_OptCaspida=`stat --format='%Z' . 2>/dev/null`		# value used later 

		ls -ld . >$TMP2file 2>/dev/null
		if grep -q " caspida caspida " $TMP2file >/dev/null 2>&1
		then
			echo
		else
			echo "   <== owner/group are not 'caspida'"
			INCREMENT_EXCEPTIONS
		fi
	
		#
		#	make sure that there's space on /opt/caspida
		#
	
		AVAIL=`df . | egrep -ve '^Filesystem' | sed -r 's/^(\S)/\n\1/' | sed -n '/^\S/,/^$/p' | sed -e :a -e '$!N; s/\n//; ta' | sed -e 's/  */ /g' | cut -d ' ' -f 4`
		if ! test -z "$AVAIL"
		then
			AVAIL=`expr $AVAIL / 1024 / 1024` # reduce to GB
	
			NECHO "$CASPIDAdir:" "available: $AVAIL GB"
			if test $AVAIL -lt $minAVAILdisk1
			then
                        	HALFoMIN=`expr $minAVAILdisk1 / 2`
                        	if test $AVAIL -lt $HALFoMIN
                        	then
                                	echo "   <== less than ${minAVAILdisk1} GB available in $CASPIDAdir"
                                	INCREMENT_EXCEPTIONS
                        	else
                                	echo "   <= less than ${minAVAILdisk1} GB available in $CASPIDAdir"
                        	fi
				df -h . | sed "s/^/$INDENT/" 
			else
				echo
			fi
		fi
	else
		NECHO "$CASPIDAdir:" "   <== does not exist"; echo
		INCREMENT_EXCEPTIONS
	fi
	
	if test -d /var/vcap2
	then
		echo
		cd /var/vcap2		# will follow symbolic link

		df . | egrep -ve '^Filesystem' | sed -r 's/^(\S)/\n\1/' | sed -n '/^\S/,/^$/p' |\
		sed -e :a -e '$!N; s/\n//; ta' | sed -e 's/  */ /g' | cut -d ' ' -f 1,2,4,6 > $TMP1file 	# source size avail target

		PARTvcap=`cut -d ' ' -f 1 < $TMP1file`
		DEVvcap=`echo "$PARTvcap" | sed -e 's/[0-9]*$//'`

		DISK=`cut -d ' ' -f 2 < $TMP1file`	
		if test -z "$DISK"
		then
			DISK=0
		fi

		DISK=`expr $DISK / 1024 / 1024`	# reduce to GB
		NECHO "/var/vcap2:" "size: ${DISK} GB"
		if test "$DISK" -lt "$minDISK2f"
		then
			echo -n "   <== less than ${minDISK2f} GB"
			INCREMENT_EXCEPTIONS
		fi

		if ! grep -q "/var/vcap2" $TMP1file
		then
			echo "   <== not a separate mount"
			INCREMENT_EXCEPTIONS
		else
			echo
		fi

	        #
	        #       make sure that there's space on /var/vcap
	        #
	
		AVAIL=`cut -d ' ' -f 3 < $TMP1file`
		if test -z "$AVAIL"
                then
                        AVAIL=0
                fi

	        AVAIL=`expr $AVAIL / 1024 / 1024` # reduce to GB
	
	        NECHO "/var/vcap2:" "available: $AVAIL GB"
	        if test $AVAIL -lt $minAVAILdisk2
	        then
	                echo "   <== less that ${minAVAILdisk2} GB"
	                INCREMENT_EXCEPTIONS
			df -h . | sed "s/^/$INDENT/" 
	        else
	                echo
	        fi

		echo
	fi

	cd "$PWD"	# restore current directory
	
	if test -d /var/vcap
	then
		cd /var/vcap		# will follow symbolic link

		df . | egrep -ve '^Filesystem' | sed -r 's/^(\S)/\n\1/' | sed -n '/^\S/,/^$/p' |\
		sed -e :a -e '$!N; s/\n//; ta' | sed -e 's/  */ /g' | cut -d ' ' -f 1,2,4,6 > $TMP1file 	# source size avail target

		PARTvcap=`cut -d ' ' -f 1 < $TMP1file`
		DEVvcap=`echo "$PARTvcap" | sed -e 's/[0-9]*$//'`

		DISK=`cut -d ' ' -f 2 < $TMP1file`	
		if test -z "$DISK"
		then
			DISK=0
		fi

		DISK=`expr $DISK / 1024 / 1024`	# reduce to GB
		NECHO "/var/vcap:" "size: ${DISK} GB"
		if test "$DISK" -lt "$minDISK2f"
		then
			echo -n "   <== less than ${minDISK2f} GB"
			INCREMENT_EXCEPTIONS
		fi

		if ! grep -q "/var/vcap" $TMP1file
		then
			echo "   <== not a separate mount"
			INCREMENT_EXCEPTIONS
		else
			echo
		fi

	        #
	        #       make sure that there's space on /var/vcap
	        #
	
		AVAIL=`cut -d ' ' -f 3 < $TMP1file`
		if test -z "$AVAIL"
                then
                        AVAIL=0
                fi

	        AVAIL=`expr $AVAIL / 1024 / 1024` # reduce to GB
	
	        NECHO "/var/vcap:" "available: $AVAIL GB"
	        if test $AVAIL -lt $minAVAILdisk2
	        then
	                echo "   <== less that ${minAVAILdisk2} GB"
	                INCREMENT_EXCEPTIONS
			df -h . | sed "s/^/$INDENT/" 
	        else
	                echo
	        fi

		IOPSbs=4				# 4k blocks
		IOPScount=`expr 1024 \* 1`		# 1 MB file
		IOPScount=`expr $IOPScount / $IOPSbs` 
	
		IOPSdir=`dirname $IOPSfile`

		if test -d $IOPSdir
		then
			IOPSpart=`df $IOPSdir | egrep "^/" | cut -d ' ' -f 1`
			echo
			dd if=/dev/urandom of=$IOPSfile bs=${IOPSbs}k oflag=direct count=$IOPScount > $TMP2file 2>&1	# create the file
			IOPS write $IOPSbs $TMP2file $IOPSpart

			dd if=$IOPSfile of=/dev/null bs=${IOPSbs}k iflag=direct > $TMP2file 2>&1	# create the file
			IOPS read $IOPSbs $TMP2file $IOPSpart
		fi

		echo
	else
		NECHO "disk:" "   <== /var/vcap does not exist"; echo
		INCREMENT_EXCEPTIONS
	fi
	
	#
	#	check for symbolic links on bare metal
	#

	if $RHELrelease
	then
		cd /var/lib	
	
		df . | egrep -ve '^Filesystem' | sed -r 's/^(\S)/\n\1/' | sed -n '/^\S/,/^$/p' |\
		sed -e :a -e '$!N; s/\n//; ta' | sed -e 's/  */ /g' | cut -d ' ' -f 1,2,4,6 > $TMP1file 	# source size avail target
	
		PARTlib=`cut -d ' ' -f 1 < $TMP1file`
		DEVlib=`echo "$PARTlib" | sed -e 's/[0-9]*$//'`
	
		if $uba41feature
		then
			reqSYMlib=8
			reqSYMLINKlib="docker | hadoop-hdfs | hadoop-mapreduce | hadoop-yarn | kafka | kubelet | pgsql | redis "
		fi
	
		if $RHELrelease
		then
			reqSYMLINKlib=`echo "$reqSYMLINKlib" | sed -e 's/postgresql/pgsql/'`	# PostgreSQL name change
		fi
	
		ls -ltr | egrep "$reqSYMLINKlib" 2>/dev/null > $TMP1file
		SYMcount=`grep "\->" $TMP1file | egrep -e "/var/vcap/store/" | wc -l`
		if test -z "$SYMcount"
		then
			SYMcount=0
		fi
	
		LIBlist=`echo "$reqSYMLINKlib" | tr -d '\\\|'`
	
		NECHO "/var/lib symlinks:" "$SYMcount"
	
		if test "$SYMcount" -eq "$reqSYMlib"
		then
			echo
		else
			# if test "$SYMcount" -eq "0"
			# then
			# 	ARROW="<=="
			# 	INCREMENT_EXCEPTIONS
			# else
				ARROW="<="
			# fi
			echo "   $ARROW expecting ${reqSYMlib}; verify missing link"
			if test -s $TMP1file
			then
				for LIB in $LIBlist
				do
					if ! grep -q "$LIB ->" $TMP1file
					then
						echo "`ls -ld $LIB 2>/dev/null` ... '$LIB' symlink not found"
					fi
				done | sed -e "s/^/$INDENT/" 
			fi
		fi
	
		cd /var/log
	
		if ! $uba32feature
		then
			reqSYMlog=`expr $reqSYMlog + 2`
			reqSYMLINKlog="$reqSYMLINKlog| hbase| opentsdb"
		else
			reqSYMlog=`expr $reqSYMlog + 1`
			reqSYMLINKlog="$reqSYMLINKlog| influxdb"
			INFLUXDB="influxdb"
			
		fi
	
		if $uba41feature
		then
			reqSYMlog=14
			reqSYMLINKlog="caspida | containers | flume-ng | hadoop-hdfs | hadoop-mapreduce | hadoop-yarn | hive | hive-hcatalog | impala | influxdb | kafka | postgresql | redis | zookeeper"
		fi
	
		ls -ltr | egrep -e "$reqSYMLINKlog" 2>/dev/null > $TMP1file
		SYMcount=`egrep -e "^lrwx" $TMP1file | egrep -e ".var.vcap.sys|.var.vcap.store" |  wc -l`
	
		LOGlist=`echo "$reqSYMLINKlog" | tr -d '\\\|'`
	
		NECHO "/var/log symlinks:" "$SYMcount"
	
		if test "$SYMcount" -eq "$reqSYMlog"
		then
			echo
		else
	                # if test "$SYMcount" -eq "0"
	                # then
	                        # ARROW="<=="
	                        # INCREMENT_EXCEPTIONS
	                # else
	                        ARROW="<="
	                # fi
			echo "   $ARROW  expecting ${reqSYMlog}; verify missing link"
			if test -s $TMP1file
			then
				for LOG in $LOGlist
				do
					if ! grep -q "$LOG ->" $TMP1file
					then
						echo "`ls -ld $LOG 2>/dev/null` ... '$LOG' symlink not found"
					fi
				done | sed -e "s/^/$INDENT/" 
			fi
		fi
	
		for LOG in $LOGlist
		do
			stat --dereference --format='%a %U %G %n' /var/log/$LOG 2>/dev/null 
		done |\
		while read INFO
		do
			LOGname=`echo "$INFO" | cut -d " " -f 4-`
		
	
			for SAMEname in zookeeper redis impala hive hbase neo4j $INFLUXDB
			do
				if test "$LOGname" = "/var/log/$SAMEname"
				then
					if (echo "$INFO" | egrep -qe "^7.. $SAMEname ")
					then
						continue 2	# permissions are acceptable
					fi
				fi
			done
			
			for HADOOPsub in hdfs yarn mapreduce
			do
				if test "$LOGname" = "/var/log/hadoop-$HADOOPsub"
	                	then
					SUBowner=`echo $HADOOPsub | awk '{ printf "%.6s", $0}'`
	                		if (echo "$INFO" | egrep -qe "^77. $SUBowner hadoop ")
	                		then
	                			continue 2     # permissions are acceptable
	                		fi
	                	fi
			done
	
			if test "$LOGname" = "/var/log/kafka"
			then
				if (echo "$INFO" | egrep -qe "^7.. caspida ")
	                        then
	                        	continue      # permissions are acceptable
	                        fi
			fi
	
			if test "$LOGname" = "/var/log/hive-hcatalog"
			then
				if (echo "$INFO" | egrep -qe "^77. hive hive ")
	                        then
	                        	continue      # permissions are acceptable
	                        fi
			fi
	
			if test "$LOGname" = "/var/log/flume-ng"
			then
				if (echo "$INFO" | egrep -qe "^7.. flume ")
	                        then
	                        	continue      # permissions are acceptable
	                        fi
			fi
	
			if test "$LOGname" = "/var/log/postgresql"
			then
				if (echo "$INFO" | egrep -qe '^7.. postgres ')
	                        then
	                        	continue      # permissions are acceptable
	                        fi
			fi

			if $uba43feature && test "$LOGname" = "/var/log/postgresql"
			then
				if (echo "$INFO" | egrep -qe '^7.. root ')
	                       	then
	                        	continue      # permissions are acceptable
	                        fi
			fi

			if $uba43feature && test "$LOGname" = "/var/log/caspida"
			then
				if (echo "$INFO" | egrep -qe '^7.. caspida ')
	                       	then
	                        	continue      # permissions are acceptable
	                        fi
			fi

			if test "$LOGname" = "/var/log/containers"
			then
				continue	# only accessible by root
			fi
			LOGperms=`echo "$INFO" | cut -d " " -f 1`
			if test "$LOGperms" = "777"
			then
				continue	# regardless of any other test, this is acceptable (should be moved up, after testing)
			fi
	
			echo "\"$LOGname\""	
			if test -h "$LOGname"
			then
				echo "\"`readlink $LOGname`\""
			fi
		done > $TMP1file		# just output log sub-directories without proper permissions 
		if test -s $TMP1file
		then
			LOGnames="`cat $TMP1file`"
			eval ls -Uld $LOGnames > $TMP2file 2>&1
			NECHO "/var/log perm/owner:" "`head -1 $TMP2file`"; echo "   <= issue with one (or more) log sub-directories"
			ALL_BUT_FIRST $TMP2file
		fi
	fi
	
	#
	#	verify that impala can write to its log files
	#

	if $uba24feature
	then
		ls -l /var/log/caspida/analytics.log /var/log/caspida/eventstore.log 2>/dev/null | egrep -v '^-rw.* 1 impala ' > $TMP1file
		if test -s $TMP1file
		then
			NECHO "impala logs:" "`head -1 $TMP1file`   <== log permissions are incorrect"; echo
			ALL_BUT_FIRST $TMP1file
			INCREMENT_EXCEPTIONS
		fi
	fi
	
	cd $PWD			# back to current directory

	NECHO "mounts:" "/var/vcap on ${PARTvcap}; /var/lib on $PARTlib" 
	if test "$DEVvcap" = "$DEVlib"
	then
		echo "   <== should be on separate drives for optimal performance"
		INCREMENT_EXCEPTIONS 
	else
		echo
	fi

	if test -f /etc/opentsdb/opentsdb.conf
	then
		COMPACTION=`egrep '^tsd.storage.enable_compaction' /etc/opentsdb/opentsdb.conf 2>/dev/null | tee $TMP1file | tr -d ' ' | cut -d '=' -f 2`
		if test "$COMPACTION" != "false"
		then
			NECHO "opentsdb" "'`head -1 $TMP1file`'"
			echo "   <== expected value is 'false'; please correct /etc/opentsdb/opentsdb.conf"
			INCREMENT_EXCEPTIONS
		fi
	fi

	#
	#	deployment.conf is checked for multi-node clusters for purposes of relevancy
	#


	CHECK_DEPLOYMENT caspida.cluster.replication.nodes $DEPLOYnode
	REPLICATION_NODE=$FOUND
	REPLICATION_NODE_list=$DEPLOYMENTlist

	CHECK_DEPLOYMENT persistence.datastore $DEPLOYnode
	PERSISTENCE_DATASTORE=$FOUND

        CHECK_DEPLOYMENT persistence.graphdb.server $DEPLOYnode
	PERSISTENCE_GRAPHDB_SERVER=$FOUND

	CHECK_DEPLOYMENT persistence.irserver $DEPLOYnode
	PERSISTENCE_IRSERVER=$FOUND

        CHECK_DEPLOYMENT persistence.server $DEPLOYnode
	PERSISTENCE_SERVER=$FOUND

	CHECK_DEPLOYMENT persistence.datastore.tsdb $DEPLOYnode
	PERSISTENCE_DATASTORE_TSDB=$FOUND

	CHECK_DEPLOYMENT uiServer.host $DEPLOYnode
	UISERVER_HOST=$FOUND
	UISERVER_HOST_list=$DEPLOYMENTlist

	CHECK_DEPLOYMENT database.host $DEPLOYnode
	DATABASE_HOST=$FOUND
	DATABASE_HOST_list=$DEPLOYMENTlist
	echo "$DATABASE_HOST_list" > $DBfile

	CHECK_DEPLOYMENT kafka.brokers $DEPLOYnode
	KAFKA_BROKER=$FOUND
	
	CHECK_DEPLOYMENT spark.server $DEPLOYnode
	SPARK_SERVER=$FOUND
	SPARK_SERVER_list=$DEPLOYMENTlist
	
	CHECK_DEPLOYMENT spark.master $DEPLOYnode
	SPARK_MASTER=$FOUND
	SPARK_MASTER_list=$DEPLOYMENTlist
	
	CHECK_DEPLOYMENT spark.worker $DEPLOYnode
	SPARK_WORKER=$FOUND
	SPARK_WORKER_list=$DEPLOYMENTlist
	
	CHECK_DEPLOYMENT impala $DEPLOYnode
	IMPALA_HOST=$FOUND
	
	CHECK_DEPLOYMENT storm.supervisor $DEPLOYnode
	STORM_SUPERVISOR=$FOUND
	
	CHECK_DEPLOYMENT storm.nimbus.host $DEPLOYnode
	STORM_NIMBUS_HOST=$FOUND
	STORM_NIMBUS_HOST_list=$DEPLOYMENTlist
	
	CHECK_DEPLOYMENT jobmanager.restServer $DEPLOYnode
	JOB_MANAGER=$FOUND
	JOB_MANAGER_host=`echo $DEPLOYMENTlist | cut -d ':' -f 1`
	if test "$JOB_MANAGER_host" = ""
	then
        	JOB_MANAGER_host="localhost"
	fi
	export JOB_MANAGER_host

	CHECK_DEPLOYMENT jobmanager.agents $DEPLOYnode
	JOB_AGENT=$FOUND

	CHECK_DEPLOYMENT analytics.host $DEPLOYnode
	ANALYTICS_HOST=$FOUND
	
	CHECK_DEPLOYMENT zookeeper.servers $DEPLOYnode
	ZOOKEEPER_SERVER=$FOUND
	ZOOKEEPER_SERVER_list=$DEPLOYMENTlist

	CHECK_DEPLOYMENT hadoop.namenode.host $DEPLOYnode
	HADOOP_NAMENODE=$FOUND

	CHECK_DEPLOYMENT hadoop.snamenode.host $DEPLOYnode
	HADOOP_SNAMENODE=$FOUND

	CHECK_DEPLOYMENT hadoop.datanode.host $DEPLOYnode
	HADOOP_DATANODE=$FOUND
	HADOOP_DATANODE_list=$DEPLOYMENTlist

	if $uba32feature
	then
		HBASE_MASTER=false
		HBASE_REGIONSERVER=false
	else
		CHECK_DEPLOYMENT hbase.master.host $DEPLOYnode
		HBASE_MASTER=$FOUND
		HBASE_MASTER_list=$DEPLOYMENTlist
	
		CHECK_DEPLOYMENT hbase.regionserver.host $DEPLOYnode
		HBASE_REGIONSERVER=$FOUND
		HBASE_REGIONSERVER_list=$DEPLOYMENTlist
	fi

	if $uba40feature
	then
		CHECK_DEPLOYMENT container.worker.host $DEPLOYnode
		CONTAINER_WORKER=$FOUND
		CONTAINER_WORKER_list=$DEPLOYMENTlist

		CHECK_DEPLOYMENT container.master.host $DEPLOYnode
                CONTAINER_MASTER=$FOUND
                CONTAINER_MASTER_list=$DEPLOYMENTlist
	fi

	if $uba41feature
	then
                CHECK_DEPLOYMENT kubernetes.restServer $DEPLOYnode
                KUBERNETES_RESTSERVER=$FOUND
                KUBERNETES_RESTSERVER_list=$DEPLOYMENTlist
        fi

	if $DATBASE_HOST
	then
        	SET_SEED

        	INPUTstring=`SHUFFLE in`
        	OUTPUTstring=`SHUFFLE out`

        	OBSCUREfunction="create function pg_temp.obscure(text) returns text as \$\$ select lower(translate(lower(translate(lower(translate(\$1, '$CHARset', '$INPUTstring')), '$CHARset' , '$OUTPUTstring')), '$CHARset', '$INPUTstring'))  \$\$ language sql"
	fi


	if $CASPIDA_PROPERTIES_EXIST
	then
		#
		#	This info may be useful for debugging
		#

		if $MULTInode
		then
			echo
        		if $LEADERnode
        		then
                		echo leadernode
        		else
                		echo membernode
        		fi > $TMP1file

	       		REPLICATION_PRIMARY=false
        		REPLICATION_SECONDARY=false

        		GET_CASPIDA_PROPERTY replication.enabled                   # is this a DR cluster?
        		REPLICATIONstate=`echo "$PROPERTYvalue" | tr -d ' ' | tr '[:upper:]' '[:lower:]'`

        		if test "$REPLICATIONstate" = "true"
        		then
                		THIShost=`echo $THISnode | tr '[:upper:]' '[:lower:]'`

                		GET_CASPIDA_PROPERTY replication.primary.host
                		REPLICATIONprimary=`echo "$PROPERTYvalue" | tr -d ' ' | tr '[:upper:]' '[:lower:]'`
                		if test "$THIShost" = "$REPLICATIONprimary"
                		then
                        		REPLICATION_PRIMARY=true
                		fi

                		GET_CASPIDA_PROPERTY replication.secondary.host
                		REPLICATIONsecondary=`echo "$PROPERTYvalue" | tr -d ' ' | tr '[:upper:]' '[:lower:]'`
                		if test "$THIShost" = "$REPLICATIONsecondary"
                		then
                        		REPLICATION_SECONDARY=true
                		fi
        		fi


			for ROLE in UISERVER_HOST DATABASE_HOST KAFKA_BROKER SPARK_MASTER SPARK_SERVER \
        		SPARK_WORKER IMPALA_HOST STORM_SUPERVISOR STORM_NIMBUS_HOST STORM_UI JOB_MANAGER JOB_AGENT ANALYTICS_HOST \
        		ZOOKEEPER_SERVER HADOOP_NAMENODE HADOOP_SNAMENODE HADOOP_DATANODE HBASE_MASTER HBASE_REGIONSERVER \
        		PERSISTENCE_DATASTORE PERSISTENCE_DATASTORE_TSDB PERSISTENCE_IRSERVER PERSISTENCE_SERVER HIVE_HOST PERSISTENCE_GRAPHDB_SERVER \
        		RULE_REALTIME_HOST RULE_OFFLINE_HOST CONTAINER_MASTER CONTAINER_WORKER KUBERNETES_RESTSERVER REPLICATION_PRIMARY REPLICATION_SECONDARY
        		do
                		eval STATE=\$$ROLE
                		if $STATE
                		then
                       	 		echo "$ROLE " | tr '[:upper:]' '[:lower:]'
                		fi 2>/dev/null
        		done | sort >> $TMP1file

        		fmt -w 60 < $TMP1file > $TMP2file
        		echo
        		NECHO "UBA roles:" "`head -1 $TMP2file`"; echo
        		ALL_BUT_FIRST $TMP2file
		fi
	fi

	#
	# tests that follow may be deployment specific
	#


	if test -f $CASPIDAdir/conf/version.properties
	then
	        if $uba42feature
	        then
        	        ERROR=""
        	else
                	ERROR="   <= check Support EOL"
        	fi

		echo
		NECHO "current uba:" "'$UBAversion'"; echo $ERROR
		CONTENT=`egrep -e '"version"' /opt/caspida/content/*/content-descriptor.json 2>/dev/null | sort | tail -1 | cut -d '"' -f 4`
		echo "$CONTENT" > $CONTENTfile
		NECHO "last content:" "'$CONTENT'"; echo
	fi
	
	if test -f $CASPIDAdir/conf/deployment/caspida-deployment.conf
	then
		DEPLOYMENT=`grep "node deployment" /opt/caspida/conf/deployment/caspida-deployment.conf 2>/dev/null | cut -d ' ' -f 2`
		DEPLOYMENTcount=`echo $DEPLOYMENT | sed -e 's/[^0-9]//g'`
		NECHO "deployment:" "'$DEPLOYMENT'"
		if test "$NODEcount" = "$DEPLOYMENTcount"
		then
			echo
		else
			echo "   <== script using '$NODEcount': $NODElist"
			INCREMENT_EXCEPTIONS
		fi
		echo "$CLUSTER_NODES_list" | sed -e 's/,/, /g' | fmt -s -w 120 | sed -e 's/, /,/g' > $TMP1file  
		NECHO "cluster nodes:" "`head -1 $TMP1file`"; echo		# show modes in cluster 
		ALL_BUT_FIRST $TMP1file
		echo
	fi

	if $uba30feature
	then
		#
		#	sizing has changed with this release
		#

		if ! $uba41feature
		then
			EXPECTED_MODELMEM_system_realtime_workers_modelcache_memory=128
			EXPECTED_POSTGRESQL_shared_buffers="512MB"
			EXPECTED_POSTGRESQL_max_connections=500	
		#	else
	#			EXPECTED_POSTGRESQL_shared_buffers="1024MB"
			#	EXPECTED_POSTGRESQL_max_connections=200	
		fi
		if ! $uba41feature
		then
  			EXPECTED_ANALYTICS_analytics_messaging_read_maxmsgs="15000"
			EXPECTED_ANALYTICS_analytics_store_max_connections_active=20
			if ! ( $uba32feature || $uba33feature )
			then
				EXPECTED_KAFKA_log_retention_hours=5
				EXPECTED_KAFKA_system_messaging_fetchsizeinmb=5
				EXPECTED_KAFKA_system_messaging_partitions=2
				EXPECTED_KAFKA_system_realtime_workers=4
				EXPECTED_KAFKA_system_messaging_flowcontrol_maxtopicsize=100G
				EXPECTED_MODELMEM_system_realtime_workers_maxmodelcache=10000
                		EXPECTED_ANALYTICS_identity_session_cache_limit="100000"
				EXPECTED_KAFKA_Heap=""		# not set on single node
				EXPECTED_MODELMEM_system_realtime_workers_modelcache_memory=256
				EXPECTED_ANALYTICS_analytics_store_max_connections_active=32
			fi
		fi
		if ! $uba41feature
		then
			EXPECTED_ANALYTICS_MaxMemory="Xmx4G"
			EXPECTED_ANALYTICS_Threads="XX:ParallelGCThreads=2"
			EXPECTED_ANALYTICS_analytics_messaging_read_interval="1800"
			EXPECTED_STORM_AnomalyEvent_workersHint="1"
		fi

		IMPALAfile="$CASPIDAdir/conf/deployment/recipes/impala/etc/default/impala-${NODEcount}_node.conf"
		if test -f "$IMPALAfile"
		then
			EXPECTED_IMPALA_MemLimit=`egrep "be_port.*mem_limit" "$IMPALAfile" | sed -e 's/^.*=//' -e 's/"$//'`
		fi

		STORMfile="$CASPIDAdir/conf/deployment/recipes/storm/etc/storm/conf/storm-${NODEcount}_node.yaml"
		if test -f "$STORMfile"
		then
			EXPECTED_STORM_supervisor_slots_ports="`egrep -v '^#' "$STORMfile" |\
			sed -r 's/^(\w)/\n\1/' | sed -n '/^supervisor.slots.ports/,/^$/p' |\
			sed -e 's/^ *-//' | sed -e :a -e '$!N; s/\n//; ta'`"
			EXPECTED_STORM_MaxMem="`egrep -e '^worker.childopts' "$STORMfile" | sed -e 's/^worker.*-Xmx/Xmx/' -e 's/ -XX.*$//'`"
		fi

		# most values are found in 'tunables'

		if test $NODEcount -gt 1
		then
			if $uba32feature
			then
				SITEproperties=uba-env.properties
			else
				SITEproperties=caspida-site.properties
			fi

			if $uba41feature
			then
				SITEproperties="uba-tuning.properties"
			fi

			CHECK_TUNABLES uba-system-env.sh KAFKA_HEAP_OPTS
			if ! test -z "$TUNABLEvalue"
			then
				EXPECTED_KAFKA_Heap="$TUNABLEvalue"
			else
				EXPECTED_KAFKA_Heap="-Xmx2G"		# set a default
			fi

			CHECK_TUNABLES kafka.properties log.retention.hours
			if ! test -z "$TUNABLEvalue"
			then
				EXPECTED_KAFKA_log_retention_hours=$TUNABLEvalue
			fi

			CHECK_TUNABLES $SITEproperties system.realtime.workers
			if ! test -z "$TUNABLEvalue"
			then
				EXPECTED_KAFKA_system_realtime_workers=$TUNABLEvalue
			fi

			CHECK_TUNABLES $SITEproperties system.messaging.partitions
			if ! test -z "$TUNABLEvalue"
			then
				EXPECTED_KAFKA_system_messaging_partitions=$TUNABLEvalue
			fi

			CHECK_TUNABLES $SITEproperties system.messaging.fetchsizeinmb
			if ! test -z "$TUNABLEvalue"
			then
				EXPECTED_KAFKA_system_messaging_fetchsizeinmb=$TUNABLEvalue
			fi

			CHECK_TUNABLES $SITEproperties system.messaging.flowcontrol.maxtopicsize
			if ! test -z "$TUNABLEvalue"
			then
				EXPECTED_KAFKA_system_messaging_flowcontrol_maxtopicsize=$TUNABLEvalue
			fi

			CHECK_TUNABLES $SITEproperties system.realtime.workers.modelcache.memory
			if ! test -z "$TUNABLEvalue"
			then
				EXPECTED_MODELMEM_system_realtime_workers_modelcache_memory=$TUNABLEvalue
			fi

			CHECK_TUNABLES $SITEproperties system.realtime.workers.maxmodelcache
			if ! test -z "$TUNABLEvalue"
			then
				EXPECTED_MODELMEM_system_realtime_workers_maxmodelcache=$TUNABLEvalue
			fi

			CHECK_TUNABLES $SITEproperties identity.session.cache.limit
			if ! test -z "$TUNABLEvalue"
			then
				EXPECTED_ANALYTICS_identity_session_cache_limit=$TUNABLEvalue
			fi

			CHECK_TUNABLES $SITEproperties analytics.store.data.retention
			if ! test -z "$TUNABLEvalue"
			then
				EXPECTED_ANALYTICS_analytics_store_data_retention=$TUNABLEvalue
			fi

			if $uba32feature
			then
				CHECK_TUNABLES uba-system-env.sh ANALYTICS_JVM_OPTS
			else
				CHECK_TUNABLES CaspidaAnalyticsServer JVM_OPTS
			fi
			if ! test -z "$TUNABLEvalue"
			then
                                EXPECTED_ANALYTICS_MaxMemory=`echo "$TUNABLEvalue" | sed -e 's/^-//' -e 's/ -XX.*$//'`
                                EXPECTED_ANALYTICS_Threads=`echo "$TUNABLEvalue" | sed -e 's/^.* -//'`
			fi

			CHECK_TUNABLES $SITEproperties analytics.store.max.connections.active
			if ! test -z "$TUNABLEvalue"
			then
				EXPECTED_ANALYTICS_analytics_store_max_connections_active=$TUNABLEvalue
			fi

			CHECK_TUNABLES $SITEproperties offline.workflow.cores
			if ! test -z "$TUNABLEvalue"
			then
				EXPECTED_SPARK_offline_workflow_cores=$TUNABLEvalue
			fi

			CHECK_TUNABLES jobmanager.master.allow.live
			if ! test -z "$TUNABLEvalue"
			then
				EXPECTED_MODEL_jobmanager_master_allow_live=$TUNABLEvalue
			fi
		
			CHECK_TUNABLES spark-env.sh SPARK_WORKER_INSTANCES
			if ! test -z "$TUNABLEvalue"
                        then
				EXPECTED_SPARK_offline_workers=$TUNABLEvalue
			fi

			EXPECTED_SPARK_offline_worker_cores=`expr $EXPECTED_SPARK_offline_workflow_cores / $EXPECTED_SPARK_offline_workers / $EXPECTED_SPARK_offline_workers_hosts 2>/dev/null`		# math done for clarity

			CHECK_TUNABLES UserWorkflow.json workersHint
			if ! test -z "$TUNABLEvalue"
                        then
				EXPECTED_STORM_User_workersHint=$TUNABLEvalue
			fi
		
			CHECK_TUNABLES PartitionWorkflow.json workersHint
			if ! test -z "$TUNABLEvalue"
                        then
				EXPECTED_STORM_Partition_workersHint=$TUNABLEvalue
			fi
		
			CHECK_TUNABLES DomainWorkflow.json workersHint
			if ! test -z "$TUNABLEvalue"
                        then
				EXPECTED_STORM_Domain_workersHint=$TUNABLEvalue
			fi
		
			CHECK_TUNABLES DeviceWorkflow.json workersHint
			if ! test -z "$TUNABLEvalue"
                        then
				EXPECTED_STORM_Devic_workersHint=$TUNABLEvalue
			fi
		fi
		
	fi

	if $DATABASE_HOST && ! $uba24feature
	then
		if ! $RHELrelease
		then
			# check that ODBC driver is installed
			ODBCversion="`dpkg -l 2>/dev/null | grep impalaodbc | sed -e 's/[ \t][ \t]*/ /g' | cut -d ' ' -f 3`"
			NECHO "odbc:" "'$ODBCversion'"
			if test "$ODBCversion" \< "$reqODBC"
			then
				echo "   <== ODBC version mismatch, $reqODBC required"
				INCREMENT_EXCEPTIONS
			else
				echo
			fi
		fi
	
		JDBCversion="`ls -d /opt/cloudera/impalajdbc/*GA 2>/dev/null | sed -e 's+/.*/++' -e 's/ GA$//'`"
		NECHO "jdbc:" "'$JDBCversion'"
		if test "$JDBCversion" \< "$reqJDBC"
		then
			echo "   <== JDBC version mismatch, $reqJDBC required"
			INCREMENT_EXCEPTIONS
		else
			echo
		fi
	fi
	
	if $RHELrelease72 && ! $uba24feature
	then
		NECHO "symlink:" "$reqSYMLINK"
		if test -L $reqSYMLINK
		then
			SYMREF=`ls -ld /usr/lib/jvm/java-1.7.0-openjdk.x86_64 2>/dev/null | cut -d ' ' -f 11`
			if test "$SYMREF" = "$reqSYMREF"
			then
				echo	
			else
				echo "   <== does not reference $reqSYMREF"
				INCREMENT_EXCEPTIONS
			fi
		else
			echo "   <== does not exist"
			INCREMENT_EXCEPTIONS
		fi
	fi
	
	
	if ($SPARK_MASTER || $SPARK_WORKER)
	then
		if test -d /var/vcap/packages/spark/lib
		then
			SPARKversion=`ls /var/vcap/packages/spark/lib/spark-assembly*hadoop*.jar 2>/dev/null | tail -1 | sed -e 's+^.*/lib/spark-assembly-++' -e 's/-hadoop.*\.jar$//'`
			NECHO "spark:" "'$SPARKversion'"
			if test "$SPARKversion" != "$reqSPARK"
			then
				echo "   <== SPARK version mismatch, $reqSPARK required"
				INCREMENT_EXCEPTIONS
			else
				echo
			fi
		fi
	fi
	
	if $KAFKA_BROKER
	then
		KAFKAversion=`/bin/ls -1 /usr/share/kafka/libs/kafka_*.jar 2>/dev/null | egrep -v 'doc|sources|test' | sed -e 's+^.*libs/kafka_++' -e 's/\.jar//'`
		NECHO "kafka:" "'$KAFKAversion'"
		if test "$KAFKAversion" != "$reqKAFKA"
		then
			echo "   <== KAFKA version mismatch, $reqKAFKA required"
			INCREMENT_EXCEPTIONS
		else
			echo
		fi
	fi

	#
	#	database may not have been started and output incomplete
	#
	
	if $DATABASE_HOST
	then
		echo

		POSTGRESversion=`psql --version 2>/dev/null`
		if test $? = 0
		then 
			NECHO "postgresql:" "$POSTGRESversion"; echo

			PGvers=`echo "$POSTGRESversion" | sed -n 's/^.* \([0-9]*\.[0-9]*\)\b.*$/\1/p'`

			for DIR in /etc/postgresql /var/lib/pgsql /var/vcap/store/pqsql
			do
				if test -d $DIR
				then
					echo $DIR
					break
				fi
			done |\
			while read DIR
			do
				sudo find -L $DIR -name 'postgresql.conf' -print | grep -v "/conf"
			done > $TMP1file	

			PGconf=`cat $TMP1file`
			if test "$PGconf" = ""
			then
				PGconf="notFound"
			fi

			NECHO "postgresql.conf:" "$PGconf"
			if sudo test -r "$PGconf"
			then
				echo
			else
				echo "   <== unable to read $PGconf"
				INCREMENT_EXCEPTIONS
			fi
			NECHO "" ""; sudo ls -ld $PGconf 2>&1
			echo

			DBversion="`psql -d caspidadb -t -c 'select currentversion from dbinfo;' 2>/dev/null | tr -d ' '`"
			NECHO "current db:" "'$DBversion'"
			if test "$DBversion" != "$reqDB"
			then
				if test -z "$DBversion"
				then
					echo "   (could not verify; database is not running)"
				else
					echo "   <== database version should be '$reqDB'"
					INCREMENT_EXCEPTIONS
				fi
			else
				echo
			fi

			echo
			COLLATEvalue="`psql -d caspidadb -t -c 'show LC_COLLATE;' 2>/dev/null | tr -d ' '`"
			NECHO "lc_collate:" "'$COLLATEvalue'"
			if test "$COLLATEvalue" != "en_US.UTF-8"
			then
				if test -z "$COLLATEvalue"
				then
					echo "   (could not verify; database is not running)"
                        	else
                                	echo "   <= expecting 'en_US.UTF-8' not '$COLLATEvalue'"
				fi
			else
				echo
			fi
			

#			echo
#                	HELLO=`psql -d caspidadb -t -c "$OBSCUREfunction; select pg_temp.obscure('hello, world') as hello;" 2>/dev/null`
#                	NECHO "hello:" "$HELLO"; echo
			
			echo
			MAPPINGS=`psql -d caspidadb -c 'select count(*) from splunkSourceTypeMappings;' 2>/dev/null | sed -e '3!d' | tr -d ' '`
			NECHO "mappings:" "'$MAPPINGS'"
			if test -z "$MAPPINGS"
                        then
                        	echo "   <== could not verify; database is not running"
				INCREMENT_EXCEPTIONS
			else
				case "$MAPPINGS" in
					"0"|"")	echo "   <== need to apply SplunkMappings.sql";
						INCREMENT_EXCEPTIONS;;
					*)	echo
						psql -d caspidadb -c 'select sourcetype,caspidatype from splunksourcetypemappings order by sourcetype;'| sed -e "s/^/$INDENT/";;
				esac
	
				if $uba32feature
				then
					TYPEDEFfile="/etc/caspida/local/conf/etl/setup/type_definitions.json"
					TYPEDEFfiles="$TYPEDEFfile $CASPIDAdir/conf/etl/setup/type_definitions.json"
				else
					TYPEDEFfile="$CASPIDAdir/conf/etl/setup/type_definitions.json"
					TYPEDEFfiles="$TYPEDEFfile"
				fi
				( psql -d caspidadb -t -c "select format from datasources where NOT (format='System' or format='CSV' or format='SPLUNK/DIRECT' or format='Splunk');"
				psql -d caspidadb -t -c 'select caspidatype from splunksourcetypemappings;') | sort -u | tail -n +2 |\
				while read MAPPING
				do  
					if egrep -h -e "type\":\"$MAPPING\"" $TYPEDEFfiles 
					then
						:
					else
						echo "type: $MAPPING   <= type not found in $TYPEDEFfile"
						# INCREMENT_EXCEPTIONS
					fi
				done > $TMP1file
				NECHO "type_definitions.json" "`head -1 $TMP1file`"; echo
				ALL_BUT_FIRST $TMP1file
			fi

			psql -d caspidadb -c '\d threats' 2>/dev/null | grep '|' | cut -d '|' -f 1 > $TMP1file
			
			echo `cat $TMP1file` > $TMP2file
			THREATScols="`cat $TMP2file`"
			
			if ! $uba40feature
			then
				echo
				NECHO "threats fix:" ""
				if test -z "$THREATScols"
				then
					echo "''   <== could not confirm, database is not running"
					INCREMENT_EXCEPTIONS
				else
					if $uba32feature
					then
						reqTHREATScols=$reqTHREATScols_32	# column name change
					fi
					if test "$THREATScols" = "$reqTHREATScols" 
					then
						echo "(not needed; all columns in correct order)"
					else
						echo "now: $THREATScols" | fmt -w 60 > $TMP1file
						echo "`head -1 $TMP1file`   <== need to apply fixThreats.sql"
						INCREMENT_EXCEPTIONS
						ALL_BUT_FIRST $TMP1file
                                        	echo "fix: $reqTHREATScols" | fmt -w 60 | sed -e "s/^/$INDENT/"
					fi
				fi
			else
                        	firstlast=`psql -d caspidadb -c '\d v_threats' 2>/dev/null | egrep "^ firstcomputedtime|^ lastupdatedtime" | wc -l`
                        	if test $firstlast -ne 2
                        	then
                                	NECHO "v_threats:" "firstcomputedtime/lastupdatedtime columns missing   <== recreate view"; echo
                                	INCREMENT_EXCEPTIONS
                                	echo
                        	fi
			fi

			# add some user info

			if $DATABASE_HOST
			then
				echo
				for FILE in User.json Account.json AccountTypes.json
				do
					case $FILE in
						User.json) 	TABLE="hrdatausers";;
						Account.json)	TABLE="hrdataaccounts";;
						*)		TABLE="";;
					esac

					if ! test "$TABLE" = ""
					then
						NECHO "$TABLE columns:" ""
						psql -d caspidadb -c "select * from $TABLE where caspidaid='0';" 2>&1 | head -1
					fi
					echo
					FILEpath=/opt/caspida/conf/attribution/$FILE
					sed -e 's+^//.*$++' -e 's+[^:]//.*$++' -e '/^.\*/d' < $FILEpath |\
					python -m json.tool 2>&1 | fold -s -60 > $TMP1file
					if grep -q "No JSON object could be decoded" $TMP1file
					then
						ERROR="   <== correct '$FILEpath'"
						INCREMENT_EXCEPTIONS
					else
						ERROR=""
					fi
					NECHO "${FILE} (d):" "`head -1 $TMP1file`"; echo "$ERROR"
					ALL_BUT_FIRST $TMP1file
		
					FILEpath=/etc/caspida/local/conf/attribution/$FILE
					if test -f $FILEpath
					then
						sed -e 's+^//.*$++' -e 's+[^:]//.*$++' -e '/^.\*/d' < $FILEpath |\
						python -m json.tool 2>&1 | fold -s -60 > $TMP1file
						if grep -q "No JSON object could be decoded" $TMP1file
						then
							ERROR="   <== correct '$FILEpath'"
							INCREMENT_EXCEPTIONS
						else
							ERROR=""
						fi
						NECHO "${FILE} (l):" "`head -1 $TMP1file`"; echo "$ERROR"
						ALL_BUT_FIRST $TMP1file
					fi
					echo
				done

				echo
				psql -d caspidadb -t -c "select value from hrconfig where key='identity.resolution.hrconfig';" | python -m json.tool > $TMP1file 2>&1 
                		NECHO "hrconfig:" "`head -1 $TMP1file`"; echo
                		ALL_BUT_FIRST $TMP1file

				echo
				HRDATAtype=`psql -d caspidadb -t -c "select name,type,endprocessingtime from datasources where type ilike '%HR%' ORDER BY 3 DESC;" 2>/dev/null | head -1 | tr -d ' ' | tee $TMP2file | cut -d '|' -f 2`
                		if test -z "$HRDATAtype"
                		then
                        		HRDATAtype="HRData"
                		fi
				
				FILEpath=/opt/caspida/conf/attribution/User.json
                		sed -ne "/\"$HRDATAtype\"/,/\"expression\"/p" < $FILEpath |\
				grep expression | sed -r 's/^\s*//' | fold  -s -w 80 > $TMP1file
				if test -s $TMP1file
				then
                			NECHO "${HRDATAtype} (d):" "`head -1 $TMP1file`"; echo
                			ALL_BUT_FIRST $TMP1file
				fi

				FILEpath=/etc/caspida/local/conf/attribution/User.json
				if test -f $FILEpath
				then
                			sed -ne "/\"$HRDATAtype\"/,/\"expression\"/p" < $FILEpath |\
					grep expression | sed -r 's/^\s*//' | fold  -s -w 80 > $TMP1file
                			NECHO "${HRDATAtype} (l):" "`head -1 $TMP1file`"; echo
                			ALL_BUT_FIRST $TMP1file
				fi

                		HRDATAname=`cut -d '|' -f 1 < $TMP2file`
                		if ! test -z "$HRDATAname"
                		then
					HRDATAid=`psql -d caspidadb -t -c "select id  from datasources where name='$HRDATAname';" 2>/dev/null | tr -d ' '`
					if ! test -z "$HRDATid"
					then
                        			DISPLAY_DATASOURCE "$HRDATAid"
					fi
                		fi

				echo
				HRDATAusers=`psql -d caspidadb -c 'select count(*) from hrdatausers' 2>/dev/null | sed -e '3!d' -e 's/ //g'`
				if test -z "$HRDATAusers"
				then
					HRDATAusers=0
				fi

				HRDATAaccounts=`psql -d caspidadb -c 'select count(*) from hrdataaccounts' 2>/dev/null | sed -e '3!d' -e 's/ //g'`
				if test -z "$HRDATAaccounts"
				then
					HRDATAaccounts=0
				fi

				NECHO "hrdata:" "users: $HRDATAusers; accounts: $HRDATAaccounts"; echo

				USERShuman=`psql -d caspidadb -c "select count(*) from users where type='Human'" 2>/dev/null | sed -e '3!d' -e 's/ //g'`
				if test -z "$USERShuman"
				then
					USERShuman=0
				fi

				USERSgenerated=`psql -d caspidadb -c "select count(*) from users where type='Generated'" 2>/dev/null | sed -e '3!d' -e 's/ //g'`
				if test -z "$USERSgenerated"
				then
					USERSgenerated=0
				fi

				NECHO "users:" "human: $USERShuman; generated: $USERSgenerated"; echo
		                echo
                		psql -d caspidadb -c 'select numAccounts,count(*) as numUsers from (select employeeid, count(loginid) as numAccounts from (select u.employeeid,a.loginid from hrdatausers u left join hrdataaccounts a on u.employeeid = a.employeeid ) x1 group by 1) x2 group by 1 order by 1 asc;' 2>/dev/null | egrep -v ' rows*\)' > $TMP1file

                		NECHO "accounts by user:" "`head -1 $TMP1file`"; echo
                		ALL_BUT_FIRST $TMP1file

			else
				NECHO "postgresql:" "   <== may not be installed"; echo
				INCREMENT_EXCEPTIONS
			fi
		fi
	fi
	
	if $KAFKA_BROKER
	then
		if $uba24feature
		then
			echo
			NECHO "kafka queue:" ""
			sudo service kafka-server status 2>/dev/null | cat -v  > $TMP1file
			if grep -q 'running' $TMP1file
			then
				/usr/share/kafka/bin/kafka-topics.sh --zookeeper ${ZOOKEEPER_SERVER_list}:2181 --topic "RuleEventTopic" --describe > $TMP1file 2>/dev/null
				if test -s $TMP1file
				then
					head -1 $TMP1file
					ALL_BUT_FIRST $TMP1file
				else
					echo "RuleEventTopic   <== kafka topic is not defined"
					INCREMENT_EXCEPTIONS
				fi
			else
				echo "RuleEventTopic   <== could not verify; kafka is not running"
				INCREMENT_EXCEPTIONS
			fi
		fi
	fi

	#
	#	show UBA deployment sizing info ... most relevant for multi-node clusters
	#

	> $SIZINGfile	# start with empty sizing info

	if $IMPALA_HOST
	then
		if test -f /etc/default/impala
		then
			VALUE=`grep "be_port" /etc/default/impala | grep mem_limit | sed -e 's/^.*-mem/mem/' -e 's/"//'`
			NECHO "  impala server:" "impala:  $VALUE"
			if test "$EXPECTED_IMPALA_MemLimit" != "tunable"
			then
				case "$VALUE" in
					*$EXPECTED_IMPALA_MemLimit*)	echo;;
							*)	echo "   <= verify deployment value; expected '$EXPECTED_IMPALA_MemLimit' in /etc/default/impala";;
				esac
			else
				echo
			fi
		fi
	fi >> $SIZINGfile
	
	if $CASPIDA_PROPERTIES_EXIST
	then
		
		if $uba42feature
		then
			KAFKAproperties="$CASPIDAdir/conf/kafka/kafka.properties"
		else
			KAFKAproperties="$CASPIDAdir/conf/kafka.properties"
		fi
		if $ZOOKEEPER_SERVER
		then
			egrep '^zookeeper.connect\b' $KAFKAproperties  2>/dev/null > $TMP1file
			ARG=$DEPLOYnode			# either 'localhost' or `hostname`
			VALUE=`egrep -i "\b${ARG}\b" $TMP1file`
			if ! test -z "$VALUE"
			then
				NECHO "  kafka:" "kafka.properties:  $VALUE"; echo
			else
				NECHO "  kafka:" "kafka.properties:  `head -1 $TMP1file`"
				echo "  <== '$ARG' not found in $KAFKAproperties"
				INCREMENT_EXCEPTIONS
				ALL_BUT_FIRST $TMP1file
			fi
		fi

		if $KAFKA_BROKER
		then
			VALUE=`grep "log.retention.hours" $KAFKAproperties`
			NECHO "  kafka:" "kafka.properties:  $VALUE"
			if test "$EXPECTED_KAFKA_log_retention_hours" != "tunable"
			then
				case "$VALUE" in
					*$EXPECTED_KAFKA_log_retention_hours*)	echo;;
							*)		echo "   (expected '$EXPECTED_KAFKA_log_retention_hours' in ${KAFKAproperties})";;
				esac
			else
				echo
			fi

			for PROPERTY in system.messaging.fetchsizeinmb system.messaging.partitions system.realtime.workers system.messaging.flowcontrol.maxtopicsize
			do
				GET_CASPIDA_PROPERTY $PROPERTY	
				NECHO "  kafka:" "$SITE_PROPERTIES:  $PROPERTY=$PROPERTYvalue"
				PROPERTY=`echo $PROPERTY | tr '.' '_'`
				ARGUMENT="\$EXPECTED_KAFKA_${PROPERTY}"
				ARGUMENT=`eval echo "$ARGUMENT"`
				if test "$ARGUMENT" != "tunable"
				then
					case "$PROPERTYvalue" in
						$ARGUMENT)	echo;;
						*)	echo "   <= verify deployment value; expected '$ARGUMENT' in $UBA_SITE_PROPERTIES";;
					esac
				else
					echo
				fi
			done

			if $uba32feature
			then
				ENVsh="/etc/caspida/local/conf/deployment/uba-system-env.sh"
			else
				ENVsh="$CASPIDAdir/bin/CaspidaCommonEnv.sh"
			fi
			ENVsh_base="`basename $ENVsh`"
			VALUE=`grep 'KAFKA_HEAP_OPTS' $ENVsh | tee $TMP1file | egrep -ve '^#'  | cut -d '=' -f 2 | tr -d '"'`

			NECHO "  kafka:" "${ENVsh_base}:  '`head -1 $TMP1file`'" 

			if test "$VALUE" = "$EXPECTED_KAFKA_Heap"
			then
				echo 
			else
				if test "$EXPECTED_KAFKA_Heap" = "tunable"
				then
					echo
				else
					echo "    <= verify deployment value; expected '$EXPECTED_KAFKA_Heap' got '$VALUE' in $ENVsh" 
				fi
			fi
		fi


		if $JOB_AGENT && ! $uba30feature
		then
			for FILE in $CASPIDAdir/conf/jobmgr.yml $CASPIDAdir/conf/quartz.properties
			do
				FILEbase=`basename $FILE`
				if test -f $FILE
				then
					grep 'jdbc:postgresql' $FILE 2>/dev/null > $TMP1file
					DATABASE_HOST_list=`cat $DBfile`
					ARG=$DATABASE_HOST_list
                			VALUE=`egrep "\b${ARG}\b" $TMP1file`
                			if ! test -z "$VALUE"
                			then
                        			NECHO "  job agent:" "${FILEbase}:  $VALUE"; echo
                			else
                        			NECHO "  job agent:" "${FILEbase}:  `head -1 $TMP1file`"
                        			echo "  <== '$ARG' not found in $FILE"
                        			INCREMENT_EXCEPTIONS
                        			ALL_BUT_FIRST $TMP1file
					fi
				else
					NECHO "${FILEbase}:" "(not found)"; echo
				fi
			done
		fi

		if $JOB_AGENT 
		then
			GET_CASPIDA_PROPERTY jobmanager.master.allow.live
			NECHO "  model parallelism:" "$SITE_PROPERTIES:  jobmanager.master.allow.live=$PROPERTYvalue"
			if test "$EXPECTED_MODEL_jobmanager_master_allow_live" != "tunable"
			then
				case "$PROPERTYvalue" in
					$EXPECTED_MODEL_jobmanager_master_allow_live)	echo;;
						*)	echo "   <= verify deployment value; expected '$EXPECTED_MODEL_jobmanager_master_allow_live' in $UBA_SITE_PROPERTIES";;
				esac
			else
				echo
			fi
		fi

		if $ANALYTICS_HOST
		then
			ARGS="analytics.store.data.retention analytics.messaging.read.maxmsgs "
			if ! $uba41feature
			then
				ARGS="$ARGS analytics.messaging.read.interval identity.session.cache.limit"
			fi
			if $uba30feature
			then
				ARGS="$ARGS analytics.store.max.connections.active"
			fi
			if $uba301feature
			then
				ARGS="$ARGS analytics.store.maxfiles analytics.store.block.size analytics.store.hdfs.block.size"
			fi
			for PROPERTY in $ARGS
			do
				GET_CASPIDA_PROPERTY $PROPERTY
				NECHO "  analytics:" "$SITE_PROPERTIES:  $PROPERTY=$PROPERTYvalue";
				if $uba301feature
				then
					if test "$PROPERTY" = "analytics.messaging.read.maxmsgs" || test "$PROPERTY" = "analytics.store.maxfiles" ||\
						 test "$PROPERTY" = "analytics.store.block.size" || test "$PROPERTY" = "analytics.store.hdfs.block.size"
					then
						if test -n "$PROPERTYvalue"
						then
							echo "   <== value not expected in $UBA_PROPERTIES and must be removed"
							INCREMENT_EXCEPTIONS
						else
							echo
						fi
						continue
					fi
				fi
				PROPERTY=`echo $PROPERTY | tr '.' '_'`
				ARGUMENT="\$EXPECTED_ANALYTICS_${PROPERTY}"
				ARGUMENT=`eval echo "$ARGUMENT"`
				if test "$ARGUMENT" != "tunable"
				then
					case "$PROPERTYvalue" in
						$ARGUMENT)	echo;;
						*)	echo "   <= verify deployment value; expected '$ARGUMENT' in $UBA_SITE_PROPERTIES";;
					esac
				else
					echo
				fi
			done
			
			if $uba32feature
			then
				CASPIDAenvironment="/etc/caspida/local/conf/deployment/uba-system-env.sh"
				OPTS="ANALYTICS_JVM_OPTS="
			else
				CASPIDAenvironment="/opt/caspida/bin/CaspidaCommonEnv.sh"
				OPTS="JVM_OPTS="
				if ! grep -q "$ARG" $CASPIDAenvironment 2>/dev/null
				then
					CASPIDAenvironment="/opt/caspida/bin/CaspidaAnalyticServer"
				fi
			fi

			if test -f $CASPIDAenvironment
			then
				VALUE=`egrep "$OPTS" $CASPIDAenvironment`
				NECHO "  analytics server:" "`basename $CASPIDAenvironment`:  $VALUE"
				if test "$EXPECTED_ANALYTICS_MaxMemory" != "tunable"
				then
					case "$VALUE" in
						*$EXPECTED_ANALYTICS_MaxMemory*);;
							 	*)	echo -n "   <= verify memory sizing value; expected '$EXPECTED_ANALYTICS_MaxMemory' in $CASPIDAenvironment";;
					esac
					case "$VALUE" in
						*$EXPECTED_ANALYTICS_Threads*)	echo;;
								*)	echo "   <= verify sizing threads value; expected '$EXPECTED_ANALYTICS_Threads' in $CASPIDAenvironment";;
					esac
				else
					echo
				fi
			fi
			if $uba33feature
			then
				for PROPERTY in models.threats.highconfidence 
				do
					GET_CASPIDA_PROPERTY $PROPERTY
					NECHO "  threats:" "$SITE_PROPERTIES:  $PROPERTY=$PROPERTYvalue"

					PROPERTY=`echo $PROPERTY | tr '.' '_'`
					ARGUMENT="\$EXPECTED_THREAT_${PROPERTY}"
					ARGUMENT=`eval echo "$ARGUMENT"`
					if test "ARGUMENT" != "tunable"
					then
						case "$PROPERTYvalue" in
							$ARGUMENT)	echo;;
							*)	echo "   <= verify deployment value; expected '$ARGUMENT' in $UBA_SITE_PROPERTIES"; 
						esac
					else
						echo
					fi
				done

				for PROPERTY in models.threats.goBackDays
				do
					GET_CASPIDA_PROPERTY $PROPERTY
					NECHO "  threats:" "$SITE_PROPERTIES:  $PROPERTY=$PROPERTYvalue"; echo
				done
				
				OFFLINEregistry="/opt/caspida/content/Splunk-Standard-Security/modelregistry/offlineworkflow/ModelRegistry.json"
				MODELmemory=`sed -n "/Threat Computation Task/,/memory/p" < $OFFLINEregistry | grep memory | sed -e "s/.*: //" -e "s/,$//"`
				NECHO "  threat comp task:" "\"memory\":${MODELmemory}"
				echo
			#	if test "$MODELmemory" = '"0mo 14d 0h 0min"'
			#	then
			#		echo
			#	else
			#		echo "   <= 14d may be preferred; modify $OFFLINEregistry"
			#	fi
			fi
		fi

		if $DATABASE_HOST
		then
			FILE="$PGconf"
			FILEbase=`basename "$PGconf"`
			sudo egrep -e "shared_buffers\s+=|max_connections\s*=" $FILE 2>/dev/null |\
			sed -e 's/[ \t][ \t]*/ /g' > $TMP2file
	
			BUFFERS=`egrep "^shared_buffers" $TMP2file | cut -d ' ' -f 3`
			
			if test -z "$BUFFERS"
			then
				BUFFERS=0
			fi

			NECHO "  postgresql:" "$FILEbase:  shared_buffers=$BUFFERS"
			if test "$EXPECTED_POSTGRESQL_shared_buffers" != "tunable"
			then
				if test "$BUFFERS" != "$EXPECTED_POSTGRESQL_shared_buffers"
				then
					echo "   <= verify deployment value; expected '$EXPECTED_POSTGRESQL_shared_buffers' in $FILE"
				else
					echo
				fi
			else
				echo
			fi
	
			CONNECTIONS=`egrep -e "^max_connections" $TMP2file | cut -d ' ' -f 3`
			
			if test -z "$CONNECTIONS"
			then
				CONNECTIONS=0
			fi

			NECHO "  postgresql:" "$FILEbase:  max_connections=$CONNECTIONS"
			if test "$EXPECTED_POSTGRESQL_max_connections" != "tunable"
			then
				if test "$CONNECTIONS" -ne "$EXPECTED_POSTGRESQL_max_connections"
				then
					echo "   <= verify deployment value; expected '$EXPECTED_POSTGRESQL_max_connections' in $FILE"
				else
					echo
				fi
			else
				echo
			fi
		fi

		if $PERSISTENCE_DATASTORE_TSDB && $uba32feature
		then
			RETENTIONpolicy=`influx -execute "show retention policies on caspida;"| egrep "^default" | sed -e "s/\s\s*/ /g" | cut -d ' ' -f 2`
			NECHO "  influx:" "caspida default retention policy: $RETENTIONpolicy"
			if test "$RETENTIONpolicy" = "2160h0m0s"
			then
				echo
			else
				echo "   <= expecting retention policy of 90 days (2160 hours)"
			fi
		fi

		if $JOB_AGENT || $JOB_MANAGER
		then
			for PROPERTY in system.realtime.workers.modelcache.memory system.realtime.workers.maxmodelcache
			do
				GET_CASPIDA_PROPERTY $PROPERTY
				NECHO "  model mem cache:" "$SITE_PROPERTIES:  $PROPERTY=$PROPERTYvalue"
				PROPERTY=`echo $PROPERTY | tr '.' '_'`
				ARGUMENT="\$EXPECTED_MODELMEM_${PROPERTY}"
				ARGUMENT=`eval echo $ARGUMENT`
				if test "$ARGUMENT" != "tunable"
				then
					case "$PROPERTYvalue" in
						$ARGUMENT)	echo;;
						*)	echo "   <= verify deployment value; expected '$ARGUMENT' in $UBA_SITE_PROPERTIES";;
					esac
				else
					echo
				fi
			done
		fi

		if $STORM_SUPERVISOR
		then
			FILE="/etc/storm/conf/storm.yaml"
                	FILEbase=`basename $FILE`
			if test -f $FILE
			then
				VALUE=`egrep -v '^#' $FILE | sed -r 's/^(\w)/\n\1/' | sed -n '/^storm.zookeeper.servers/,/^$/p' | sed -e 's/^ *-//' | sed -e :a -e '$!N; s/\n//; ta'`
				NECHO "  storm:" "$FILEbase:  $VALUE"
				case "$VALUE" in
					*$ZOOKEEPER_SERVERS_list*)	echo;;
							*)	echo "   <= verify deployment value; expected '$ZOOKEEPER_SERVERS_list'";;
				esac
				VALUE=`egrep -v '^#' $FILE | sed -r 's/^(\w)/\n\1/' | sed -n '/^nimbus.host/,/^$/p' | sed -e 's/^ *-//' | sed -e :a -e '$!N; s/\n//; ta'`
				NECHO "  storm:" "$FILEbase:  $VALUE"
				case "$VALUE" in
					*$STORM_NIMBUS_HOST_list*)	echo;;
							*)	echo "   <= verify deployment value; expected '$STORM_NIMBUS_HOST_list'";;
				esac
			fi
		fi

		if $STORM_SUPERVISOR
		then
			if test -f /etc/storm/conf/storm.yaml
			then
				ERROR=""
				VALUE=`egrep -v '^#' /etc/storm/conf/storm.yaml | sed -r 's/^(\w)/\n\1/' | sed -n '/^supervisor.slots.ports/,/^$/p' | sed -e 's/^ *-//' | sed -e :a -e '$!N; s/\n//; ta'`
				NECHO "  storm:" "storm.yaml:  $VALUE"
				if test "$EXPECTED_STORM_supervisor_slots_ports" != "tunable"
				then
					case "$VALUE" in
						*$EXPECTED_STORM_supervisor_slots_ports)	echo;;
							*)	echo "   <= verify deployment value; expected '$EXPECTED_STORM_supervisor_slots_ports'";;
					esac
				else
					echo
				fi
				VALUE=`egrep -e '^worker.childopts' /etc/storm/conf/storm.yaml | sed -e 's/^worker.*-Xmx/Xmx/' -e 's/ -XX.*$//'`
				NECHO "  storm:" "worker.childopts=-$VALUE"
				if test "$VALUE" = "$EXPECTED_STORM_MaxMem"
				then
					ERROR=""
				else
					lowerExpected=`echo "$EXPECTED_STORM_MaxMem" | tr '[:upper:]' '[:lower:]'`
					lowerValue=`echo "$VALUE" | tr '[:upper:]' '[:lower:]'`
					if test "$lowerValue" = "$lowerExpected"
					then
						ERROR=""
					else
						numValue=`echo "$lowerValue" | sed -e 's/[^0-9]//g'`
						unitValue=`echo "$lowerValue" | sed -e 's/xmx//' -e 's/[0-9]//g'`
						numExpected=`echo "$lowerExpected" | sed -e 's/[^0-9]//g'`
						unitExpected=`echo "$lowerExpected" | sed -e 's/xmx//' -e 's/[0-9]//g'`
						
						if test "$unitValue" = "$unitExpected"
						then
							ERROR="values differed"
						else				
							if test "$unitValue" = "g"
							then
								numValue=`expr $numValue \* 1024 2>/dev/null`	# g * 1024 = m	
								lowerValue="xmx${numValue}m"	
								if test "$lowerValue" = "$lowerExpected"
                                        			then
                                                			ERROR=""
                                        			else
									ERROR="failed conversion ($lowerValue != $lowerExpected)"
								fi
							else
								ERROR="unexpected unit '$unitValue' $numValue"
							fi

						fi
					fi
				fi
				if ! test -z "$ERROR"
				then
					ERROR="   <= verify deployment value string; expected '$EXPECTED_STORM_MaxMem' in /etc/storm/conf/storm.yaml ($ERROR)"
				fi
				echo "$ERROR"			# could be null
				ERROR=""			# reset for later use
			fi

			if ! $uba32feature
			then
				for WORKFLOW in User Partition Domain Device AnomalyEvent
				do
					FILE="$CASPIDAdir/conf/workflows/${WORKFLOW}Workflow.json"
					FILEbase=`basename "$FILE"`
					if test -f $FILE
					then
						VALUE=`grep 'workersHint' $FILE | tr -d ' ,'`
						NECHO "  storm:" "$FILEbase:  $VALUE"
						ARGUMENT="\$EXPECTED_STORM_${WORKFLOW}_workersHint"
						ARGUMENT=`eval echo $ARGUMENT`
						case "$VALUE" in
							*$ARGUMENT)	echo;;
									*)	echo "   <= verify deployment value; expected '$ARGUMENT' in $CASPIDAdir/conf/workflows/${WORKFLOW}Workflow.json";;
						esac
					fi
				done
			fi
		fi

		if $MULTInode
		then
			if $ZOOKEEPER_SERVER
			then
				egrep '^server\.' /etc/zookeeper/conf/zoo.cfg 2>/dev/null > $TMP1file
				VALUE=`egrep -i "\b${THISnode}\b" $TMP1file`
				if ! test -z "$VALUE"
				then
					NECHO "  zookeeper:" "zoo.cfg: $VALUE"; echo
				else
					NECHO "  zookeeper:" "zoo.cfg: `head -1 $TMP1file`"
					echo "  <== '$ARG' not found in /etc/zookeeper/conf/zoo.cfg"
					INCREMENT_EXCEPTIONS
					ALL_BUT_FIRST $TMP1file
				fi
				if test -r /var/lib/zookeeper/data/myid
				then
					MYID=`cat /var/lib/zookeeper/data/myid`
					NECHO "  zookeeper:" "myid:  $MYID"
					
					lower_ZOOKEEPER_SERVER_list=`echo $ZOOKEEPER_SERVER_list | tr '[:upper:]' '[:lower:]'`
					lower_THISnode=`echo $THISnode | tr '[:upper:]' '[:lower:]'`
					case "$lower_ZOOKEEPER_SERVER_list" in
						*,$lower_THISnode) 	EXPECTid=3;;
						*,$lower_THISnode,*)	EXPECTid=2;;
						*$lower_THISnode,*)	EXPECTid=1;;
						*)		EXPECTid="";;
					esac
					if test "$MYID" = "$EXPECTid"
					then
						echo
					else
						echo "   <== expected '$EXPECTid' in /var/lib/zookeeper/data/myid"
						INCREMENT_EXCEPTIONS
					fi
				fi
			fi
		fi

		if $HADOOP_NAMENODE
		then
			sudo service hadoop-hdfs-namenode status 2>/dev/null | cat -v > $TMP1file
                        if grep -q 'running' $TMP1file
                        then
				sudo -u hdfs hdfs dfsadmin -report 2>/dev/null |  egrep 'datanodes|^Hostname|Status' |  sed -e :a -e '$!N;s/\nD/\tD/;ta' -e 'P;D' > $TMP1file
				FIRSTline=`head -1 $TMP1file```	
				LIVEcount=`echo $FIRSTline | sed -e 's/^.*(//' -e 's/).*$//'`
				EXPECTEDcount=`echo $HADOOP_DATANODE_list | tr ',' ' ' | wc -w`
				NECHO "  hadoop:" "$FIRSTline"
				if test "$LIVEcount" = "$EXPECTEDcount"
				then
					echo
				else
					echo "   <== expected '$EXPECTEDcount', $HADOOP_DATANODE_list; check all hadoop logs and services"
					INCREMENT_EXCEPTIONS
				fi
				ALL_BUT_FIRST $TMP1file
			fi
		fi

		if ($HADOOP_NAMENODE || $HADOOP_SNAMENODE || $HADOOP_DATANODE)
		then
			FILE="/etc/hadoop/conf.empty/core-site.xml"
			FILEbase=`basename $FILE`
			sed -n '/<name/,/\/value>/p' < $FILE | sed -e 'N;s/\n//' | tr -d ' ' > $TMP1file	# name/value on single line
			grep 'fs.defaultFS' $TMP1file  2>/dev/null > $TMP2file
			ARG=$HADOOP_MASTER_list
                	VALUE=`egrep "\b${ARG}\b" $TMP2file`
                	if ! test -z "$VALUE"
                	then
                		NECHO "  hadoop:" "${FILEbase}:  $VALUE"; echo
                	else
                        	NECHO "  hadoop:" "${FILEbase}:  `head -1 $TMP2file`"
                        	echo "  <== '$ARG' not found in $FILE"
                        	INCREMENT_EXCEPTIONS
                        	ALL_BUT_FIRST $TMP2file
                	fi
		fi

		if $HBASE_MASTER
		then
			FILE="/etc/hbase/conf/hbase-site.xml"
			FILEbase=`basename $FILE`
			sed -n '/<name/,/\/value>/p' < $FILE | sed -e 'N;s/\n//' | tr -d ' ' > $TMP1file	# name/value on single line
			grep 'hbase.rootdir' $TMP1file  2>/dev/null > $TMP2file
			ARG=$HBASE_MASTER_list
                	VALUE=`egrep "\b${ARG}\b" $TMP2file`
                	if ! test -z "$VALUE"
                	then
                		NECHO "  hbase:" "${FILEbase}:  $VALUE"; echo
                	else
                        	NECHO "  hbase:" "${FILEbase}:  `head -1 $TMP2file`"
                        	echo "  <== '$ARG' not found in $FILE"
                        	INCREMENT_EXCEPTIONS
                        	ALL_BUT_FIRST $TMP2file
                	fi
			grep 'hbase.zookeeper.quorum' $TMP1file  2>/dev/null > $TMP2file
			ARG=$ZOOKEEPER_SERVER_list
                	VALUE=`egrep "\b${ARG}\b" $TMP2file`
                	if ! test -z "$VALUE"
                	then
                		NECHO "  hbase:" "${FILEbase}:  $VALUE"; echo
                	else
                        	NECHO "  hbase:" "${FILEbase}:  `head -1 $TMP2file`"
                        	echo "  <== '$ARG' not found in $FILE"
                        	INCREMENT_EXCEPTIONS
                        	ALL_BUT_FIRST $TMP2file
               	 	fi
				
		fi

		if ($SPARK_MASTER || $SPARK_WORKER)
		then
                	FILE="/var/vcap/packages/spark/conf/spark-env.sh"
                	FILEbase=`basename $FILE`
			NECHO "  spark:" "$FILEbase: "
			if test -f $FILE
			then
				if $uba32feature
				then
					ARG="SPARK_MASTER_HOST"
				else
					ARG="SPARK_MASTER_IP"
				fi
				if egrep -ie "^${ARG}\b" $FILE > $TMP1file 2>/dev/null
				then
					echo -n "`cat $TMP1file`"
					ARG=`echo "$SPARK_MASTER_list" | sed -e 's/:[0-9]*\b//g'` # strip off port number
					if egrep -qe "\b${ARG}\b" $TMP1file
					then
						echo
					else
						if egrep -qie "\b${ARG}\b" $TMP1file
						then
							echo "   <= '$ARG' not found in $FILE; case inconsistency"
						else
							echo "   <== '$ARG' not found in $FILE"
							INCREMENT_EXCEPTIONS
						fi
					fi
				else
					echo "   <== '$ARG' not found in $FILE"
					INCREMENT_EXCEPTIONS
				fi
			else
				echo "   <== '$SPARK_MASTER_list' not present; $FILE  does not exist"
				INCREMENT_EXCEPTIONS
			fi

			if $MULTInode
			then
				if ! test -z "$SPARK_WORKER_list"
				then
					FILE="/var/vcap/packages/spark/conf/slaves"
                			FILEbase=`basename $FILE`
					NECHO "  spark:" "$FILEbase:  "
					if test -f $FILE
					then
						WORKERS=`echo "$SPARK_WORKER_list" | sed -e 's/,/ /g'`
						for WORKER in $WORKERS
						do
							if grep -q "$WORKER" $FILE
							then
								continue
							else
								if grep -iq "$WORKER" $FILE
								then
									ERROR="   <= '$SPARK_WORKER_list' missing; not found in $FILE"
									continue
								else
									ERROR="   <== '$SPARK_WORKER_list' missing; not found in $FILE"
									INCREMENT_EXCEPTIONS
								fi
							fi
						done
						echo "`head -1 $FILE`$ERROR"
						ERROR=""		# reset for next use
						ALL_BUT_FIRST $FILE
					else
						echo "''   <== '$SPARK_WORKER_list' not present; $FILE does not exist"
						INCREMENT_EXCEPTIONS
					fi
				fi
			fi
		fi

		if $SPARK_MASTER
		then
			GET_CASPIDA_PROPERTY offline.workflow.cores
			NECHO "  spark:" "$SITE_PROPERTIES:  offline.workflow.cores=$PROPERTYvalue"
			if test "$EXPECTED_SPARK_offline_workflow_cores" != "tunable"
			then
				case "$PROPERTYvalue" in
					$EXPECTED_SPARK_offline_workflow_cores)	echo;;
							*)	echo "   <= verify deployment value; expected '$EXPECTED_SPARK_offline_workflow_cores' in $UBA_SITE_PROPERTIES";;
				esac
			else
				echo
			fi
			
			ps -elf | egrep -e "[s]park.deploy.master.Master" > $TMP2file		# is spark running on this node
			if test -s $SIZINGfile
			then
				if $uba42feature
				then
					URL="https://localhost:8080"
					TOKEN=`grep '^\s*jobmanager.restServer.auth.user.token=' /opt/caspida/conf/uba-default.properties | cut -d'=' -f2`
					AUTH="-Ssk -H 'Authorization: Bearer $TOKEN'"
				else
                			URL="http://localhost:8080"
					AUTH=""
				fi
				if curl --connect-timeout 30 -o $TMP2file $AUTH $URL 2>/dev/null
				then
					TOTALcores=`grep "Cores.*:" $TMP2file | sed -e 's/^.*> //' -e 's/ Total.*$//'`
					NECHO "  spark total cores:" "'$TOTALcores'"
					if test "$EXPECTED_SPARK_offline_workflow_cores" != "tunable"
					then
						if test "$TOTALcores" -ge  "$EXPECTED_SPARK_offline_workflow_cores"
						then
							echo
						else
							echo "   <== expecting $EXPECTED_SPARK_offline_workflow_cores"
							INCREMENT_EXCEPTIONS
						fi
					else
						echo
					fi
				fi
			fi
		fi

		if $SPARK_WORKER
		then
			WORKERcores=`grep "^SPARK_WORKER_CORES" /var/vcap/packages/spark/conf/spark-env.sh 2>/dev/null | cut -d '=' -f 2` 
			NECHO "  spark worker:" "spark-env.sh:  SPARK_WORKER_CORES=$WORKERcores"
			if test "$EXPECTED_SPARK_offline_worker_cores" != "tunable"
			then
				if test "$WORKERcores" =  "$EXPECTED_SPARK_offline_worker_cores"
				then
					echo
				else
					if test "$WORKERcores" \>  "$EXPECTED_SPARK_offline_worker_cores"
					then
						echo
					else
						echo "   <== expected $EXPECTED_SPARK_offline_worker_cores for '$EXPECTED_SPARK_offline_workers' workers; verify value in /var/vcap/packages/spark/conf/spark-env.sh"
						INCREMENT_EXCEPTIONS
					fi
				fi
			else
				echo
			fi
		fi


	fi >> $SIZINGfile

	if test -s $SIZINGfile
	then
		echo
		NECHO "deployment ..." ""
		if $MEMBERnode
		then
			if test -s $MD5compare
			then
				md5sum -c $MD5compare 2>/dev/null |\
				grep -v 'OK$' > $TMP2file
			else
				> $TMP2file	# there was noting to compare
			fi
			if test -s $TMP2file
			then
				echo "   <== sizing may be incorrect because one (or more) 'conf' files do not match leader"
				INCREMENT_EXCEPTIONS
			else	
				echo	 
			fi
		else
			echo		# no comment on leader
		fi

		cat $SIZINGfile
	fi

	if $CASPIDA_PROPERTIES_EXIST
	then
		if $LEADERnode
		then
			cd $CASPIDAdir/conf
			if $uba30feature
			then
				ODBCini=""
			else
				ODBCini="odbc-impala.ini"
			fi
			FILElist="$UBA_SITE_PROPERTIES $ODBCini"
			if ! $uba42feature
			then
				FILElist="$FILElist jobmgr.yml quartz.properties"
			fi
			for FILE in $FILElist
			do
				if test -f $FILE
				then
					if grep -q "<%=" $FILE 2>/dev/null
					then
						echo
						NECHO "`basename ${FILE}`:" "'not modified'"
						echo "   <== verify that  Caspida replace_properties  was run"
						INCREMENT_EXCEPTIONS
						break
					fi
				else
					NECHO "${FILE}:" "'not found'"
					echo "   <== verify that  Caspida replace_properties  was run"
					INCREMENT_EXCEPTIONS
					break
				fi
			done

		cd "$PWD"

			TIMESTAMP_CaspidaSecurityJar=`stat --format='%Y' $CASPIDAdir/lib/CaspidaSecurity.jar 2>/dev/null`
			if test -z "$TIMESTAMP_CaspidaSecurityJar"
			then
				TIMESTAMP_CaspidaSecurityJar=0
			fi 
			if test "$TIMESTAMP_CaspidaSecurityJar" -lt "$TIMESTAMP_OptCaspida"
			then
				HUMANTIME_CaspidaSecurityJar=`date --date="@${TIMESTAMP_CaspidaSecurityJar}" '+%b %e %H:%M' 2>/dev/null`	# convert to readable
				HUMANTIME_OptCaspida=`date --date="@${TIMESTAMP_OptCaspida}" '+%b %e %H:%M' 2>/dev/null`	# convert to readable
				echo
				NECHO "CaspidaSecurity.jar:" "modified -  $HUMANTIME_CaspidaSecurityJar"
				if ! $uba40feature
				then
					echo "   <== verify that CreateCaspidaSecurityJar.py was run after $HUMANTIME_OptCaspida"
					INCREMENT_EXCEPTIONS
				else
					echo
				fi
			fi
		fi
	fi

	echo
	
	if $ANALYTICS_HOST
	then
		if $CASPIDA_PROPERTIES_EXIST
        	then
			if ! $uba30feature
			then
				GET_CASPIDA_PROPERTY analytics.store.sql.connector
				SQLconnector=$PROPERTYvalue

				NECHO "SQLconnector:" "$SQLconnector"
	
				if test "$SQLconnector" = "jdbc"
				then
					echo
				else
					if $RHELrelease && ! $uba30feature
					then
						echo "   <== must be 'jdbc' for RHEL; modify casida-site.properties"
						INCREMENT_EXCEPTIONS
					else
						echo
					fi
				fi
			fi
		fi
	fi

	if $RHELrelease
	then
		if $CASPIDA_PROPERTIES_EXIST
		then
			GET_CASPIDA_PROPERTY system.network.interface
			SITEinterface=$PROPERTYvalue

			if test -z "$SITEinterface"
			then
				SITEinterface="eth0"			# this is the default
			fi

			NECHO "interface:" "'$SITEinterface'"
			if test "$SITEinterface" != "$INTERFACE"
			then
				echo "   <== system.network.interface value in $UBA_SITE_PROPERTIES does not match '$INTERFACE'"
			else
				echo
			fi
		fi
	fi

	#
	#	TLS versions should match between node and Splunk servers
	#
	
	> $TMP2file		# used for protocol check
	
	GET_CASPIDA_PROPERTY system.splunk.drilldown.searchhead
	SEARCHhead=$PROPERTYvalue

	if $UISERVER_HOST
	then
		if $CASPIDA_PROPERTIES_EXIST
		then
			NECHO "searchhead:" "'$SEARCHhead'"
			if ( echo "$SEARCHhead" | grep -qi "http" )
			then
				if test "$SEARCHhead" = "$defSEARCHhead"
				then
					if $uba41feature
					then
						echo "   (ignored with UBA 4.1)"
					else
						echo "   <== system.splunk.drilldown.searchhead should be updated for proper Splunk> search head in $UBA_SITE_PROPERTIES"
						INCREMENT_EXCEPTIONS
					fi
				else
					if $uba41feature
					then
						echo "   <= ignored with UBA 4.1"
					else
						echo
						SEARCHhead=`echo "$SEARCHhead" | sed -e 's+^.*://++' -e 's/:.*$//' -e 's+/*$+:8089+'`
						echo "$SEARCHhead" > $TMP2file		# may be used later for SSL protocol check
					fi
				fi
			else
				if $uba41feature
				then
					echo
				else
					echo "   <== not a valid URL; $UBA_SITE_PROPERTIES may need update"
					INCREMENT_EXCEPTIONS
				fi
			fi
		fi
		if $uba41feature
		then
			SEARCHheads="/etc/caspida/local/conf/splunk_search_head.json"
			if test -f $SEARCHheads
			then
				if python -m json.tool < $SEARCHheads > $TMP1file 2>&1
				then
					NECHO "splunk_search_head.json:" "`head -1 $TMP1file`"; echo
					ALL_BUT_FIRST $TMP1file
				else
					NECHO "splunk_search_head.json:" "`head -1 $SEARCHheads`"; echo "   <== JSON decoding of '$SEARCHheads' failed"
					ALL_BUT_FIRST $SEARCHheads
					INCREMENT_EXCEPTIONS
				fi
			fi
		fi
	fi

	GET_CASPIDA_PROPERTY uiServer.host
	UIserver=$PROPERTYvalue

	if $DATABASE_HOST
	then
		OUTPUTconnectors=`psql -d caspidadb -t -c "select count(*) from outputconnectors where type='SplunkES'" 2>/dev/null | head -1 | sed -e 's/ //g'`
               	if test -z "$OUTPUTconnectors"
               	then
               		OUTPUTconnectors=0
                fi	
		if test $OUTPUTconnectors -gt 0
		then
			NECHO "uiserver:" "'$UIserver'"
			
			if test -z "$UIserver"
			then
				echo "   <== uiServer.host not set in $UBA_SITE_PROPERTIES; needed for output connectors"
				INCREMENT_EXCEPTIONS
			else
				if echo "$UIserver" | egrep -qe ':|/'
				then
					echo "   <== IP or hostname expected"
					INCREMENT_EXCEPTIONS 
				else
					UIip=`getent ahostsv4 "$UIserver" 2>/dev/null | cut -d " " -f 1 | sort -u | head -1`
					HOSTip=`getent ahostsv4 "$UISERVER_HOST_list" 2>/dev/null | cut -d " " -f 1 | sort -u | head -1`
					if test "$UIip" = "$HOSTip"
					then
						echo
					else
						case "$UIip" in
							10.*)		EXTERNAL="";;
							192.168*)	EXTERNAL="";;
							172.16.*)	EXTERNAL="";;
							*)		EXTERNAL=", might be public IP";;
						esac
						echo "   <= does not resolve to ${HOSTip}${EXTERNAL}; uiServer.host must be set properly in $UBA_SITE_PROPERTIES"
					fi
				fi
			fi
		fi
	
		if $CASPIDA_PROPERTIES_EXIST
        	then
			GET_CASPIDA_PROPERTY connector.splunk.protocol
			PROTOCOLuba=$PROPERTYvalue
			NECHO "uba protocol:" "$PROTOCOLuba"; echo
		fi

		psql -d caspidadb -c "select id from datasources where type='Splunk' order by type desc, name" 2>/dev/null | egrep -v '^[ \t]*id\b|^---|\(.*rows\)|^$' > $TMP1file
		if test -s $TMP1file
		then
			DATAids=`cat $TMP1file`
			for DATAid in $DATAids
			do
				# ENDpoint=`DISPLAY_DATASOURCE_ENDPOINT $DATAsource`
				ENDpoint=`DISPLAY_DATASOURCE "$DATAid" | grep '"endpoint":' | sed -e 's/^.* "h/h/' -e 's/",*$//'`
				if ! test -z "$ENDpoint"
				then
					echo "$ENDpoint" | sed -e 's+https*://++' >> $TMP2file		# may already contain the search head
				fi		
			done 
		fi

		if test -s $TMP2file
		then
			sort -u < $TMP2file|\
                	while read ENDpoint
                	do
                		PROTO_CHECK "$ENDpoint"
                	done
		fi
	fi

	
	#
	#	check http connectivity of uiserver
	#

	if jps | egrep -q -e "Caspida|?orker"			# IFF caspida has been started
	then
		env | grep -i "_proxy=" > $TMP1file
		if test -s $TMP1file
		then
                       	if $uba41release
                        then
                                IPcontainers=",10.96.0.0/12,10.244.0.0/16"
                        else
                                IPcontainers=""
                        fi
			NOPROXYlist="localhost,127.0.0.1${IPcontainers},$CLUSTER_NODES_list"
			if test -z "$no_proxy"
                	then
                        	NECHO "no_proxy:" "'$no_proxy'   <== not set;  no_proxy value must include '$NOPROXYlist'"; echo
                        	INCREMENT_EXCEPTIONS
			else
				NOPROXYentries=`echo "$NOPROXYlist" | tr ',' ' '`	
				for ENTRY in $NOPROXYentries
				do
					if echo "$no_proxy" | egrep -qie "(^|,)$ENTRY(,|\$)"
					then
						continue
					else
						NECHO "no_proxy:" "'$no_proxy'   <== incomplete, missing $ENTRY;  no_proxy value must include '$NOPROXYlist'"; echo
                                		INCREMENT_EXCEPTIONS
						break
					fi
				done
                	fi
		fi

		for SERVER in $JOB_MANAGER_host
		do
			echo
			if $uba42feature
			then
				URL="https://$SERVER:9002/remote/agents"
				TOKEN=`grep '^\s*jobmanager.restServer.auth.user.token=' /opt/caspida/conf/uba-default.properties | cut -d'=' -f2`
				AUTH="-Ssk -H 'Authorization: Bearer $TOKEN'"
				HEADER="--header='Authorization: Bearer $TOKEN'"
			else
                		URL="http://$SERVER:9002/remote/agents"
				AUTH=""
				HEADER=""
			fi
			NECHO "http connect (wget):" "$THISnode to $SERVER"
			wget --no-check-certificate --spider --tries=1 --timeout=4 $HEADER $URL > $TMP1file 2>&1
			if grep -q connected $TMP1file
			then
				echo "   (successful)"
			else
				echo "$?   <== failed to connect"
				INCREMENT_EXCEPTIONS
			fi
			NECHO "http connect (curl):" "$THISnode to $SERVER"
			if curl --connect-timeout 10 $AUTH $URL > /dev/null 2>&1
			then
				echo "   (successful)"
			else
				NOPROXY='--noproxy'
				if curl --connect-timeout 10 $NOPROXY '*' $AUTH $URL > /dev/null 2>&1
				then
					echo "   <= issue with no_proxy is suspected"
				else
					echo "   <== failed to connect"
					INCREMENT_EXCEPTIONS
				fi
			fi
			if test "$SERVER" = "$THISnode"
			then
				if $uba42feature
				then
					URL="https://localhost:9002/remote/agents"
				else
                			URL="http://localhost:9002/remote/agents"
				fi
                		NECHO "http connect (curl):" "$THISnode to localhost"
                		if curl --connect-timeout 10 $AUTH $URL > /dev/null 2>&1
                		then
                       	 		echo "   (successful)"
                		else
					NOPROXY='--noproxy'
                                	if curl --connect-timeout 10 $NOPROXY '*' $AUTH $URL > /dev/null 2>&1
                                	then
                                        	echo "   <= issue with no_proxy is suspected"
                                	else
                                        	echo "   <== failed to connect"
                                		INCREMENT_EXCEPTIONS
                                	fi
                		fi
			fi
			echo
		done
	fi

	#
	#	analytics.store.data.partition should be checked
	#

	if $ANALYTICS_HOST
	then
		if $CASPIDA_PROPERTIES_EXIST
        	then
			GET_CASPIDA_PROPERTY analytics.store.data.partition
			FREQ=$PROPERTYvalue
			NECHO "analytics:" "analytics.store.data.partition=${FREQ}"
			if test "$FREQ" = "Daily"
			then
				echo
			else
				echo "   <== not default value 'Daily'; may need to update $UBA_SITE_PROPERTIES" 
				INCREMENT_EXCEPTIONS
			fi
		fi
	fi

	#
	#	identity.resolution.hrdata.filter.unknown should be checked in 2.4 or later
	#

	if $JOB_AGENT || $JOB_MANAGER
	then
		if $uba24feature
		then
			if $CASPIDA_PROPERTIES_EXIST
                	then
				GET_CASPIDA_PROPERTY identity.resolution.hrdata.filter.unknown
				NECHO "hrdata.filter:" "identity.resolution.hrdata.filter.unknown:"
				echo -n "$PROPERTYvalue"
				case "$PROPERTYvalue" in
					true)	echo;;
					false) 	echo "   <== not default value 'true'; may need to update $UBA_SITE_PROPERTIES"
						INCREMENT_EXCEPTIONS;;
					*)	echo "   <== value not set in $UBA_SITE_PROPERTIES"
						INCREMENT_EXCEPTIONS;;
				esac

				if ! $uba42feature
				then
					if $uba30feature
					then
						recDBthreads=10		# 10 is the proper number
					fi
					NECHO "hrdata.threads:" "attribution.hrdata.db.threads:"
					GET_CASPIDA_PROPERTY attribution.hrdata.db.threads
					echo -n "$PROPERTYvalue"
					case "$PROPERTYvalue" in
						$recDBthreads)	echo;;
						[1-9][0-9]*) 	echo "   <=  '$recDBthreads' may be advised in $UBA_SITE_PROPERTIES";;
						*)	echo "   <== value not set in $UBA_SITE_PROPERTIES"
							INCREMENT_EXCEPTIONS;;
					esac
				fi
			
				if $uba32feature
				then
					> $TMP1file
					NORMALIZEdevices=false
					NORMALIZEauto=false
					DEVICEproperties="identity.resolution.device.domains.normalize identity.resolution.device.domains.list"
					if $uba41feature
					then
						DEVICEproperties="identity.resolution.device.auto.normalize $DEVICEproperties"
					fi
					if $uba40feature
					then
						DEVICEproperties="$DEVICEproperties parser.device.internal.domains"	
					fi
					DEVICEproperties="uba.unresolveddevices.retentiondays $DEVICEproperties"
					for DEVICEspec in $DEVICEproperties
					do
						GET_CASPIDA_PROPERTY $DEVICEspec
						echo -n "${DEVICEspec}=$PROPERTYvalue"
						if ! test -z "$PROPERTYvalue"
						then
							if test "$DEVICEspec" = "parser.device.internal.domains"
							then
								ERROR=""
							

								echo " $PROPERTYvalue " | tr "," " " > $TMP2file
								COUNTdomain=`wc -w < $TMP2file`
								if ! egrep -qe " local " $TMP2file
								then
									ERROR="'local' missing"
									COUNTdomain=`expr $COUNTdomain - 1`
								fi
								if ! egrep -qe " localdomain " $TMP2file
								then
									ERROR="$ERROR 'localdomain' missing"
									COUNTdomain=`expr $COUNTdomain - 1`
								fi

								if ! test -z "$ERROR"
								then
									ERROR=": $ERROR"
								fi

								if test $COUNTdomain -lt 3
								then
									ERROR="$ERROR "
								fi

								if test -z "$ERROR"
								then
									echo
								else
									echo "   <= verify domain scoping is correct${ERROR}"
								fi 
							fi
							if $uba41feature
							then
								if test "$DEVICEspec" = "identity.resolution.device.auto.normalize"
								then
									if test "$PROPERTYvalue" = "true"
                                                                	then
										NORMALIZEauto=true
									else
										echo -n "   <= 'true' is the expected value"
									fi
								fi
							fi
							if ! $NORMALIZEauto
							then
								if test "$DEVICEspec" = "identity.resolution.device.domains.normalize"
								then
									if test "$PROPERTYvalue" = "true"
									then
										NORMALIZEdevices=true
									else
										echo -n "   <== 'true' is expected value in /etc/caspida/local/conf/uba-site.properties"
										INCREMENT_EXCEPTIONS
									fi
								fi
								if test "$DEVICEspec" = "identity.resolution.device.domains.list"
								then
									if test "$PROPERTYvalue" = ""
									then
										if $NORMALIZEdevices
										then
											echo -n "   <== comma-separated list of internal domains is required in /etc/caspida/local/conf/uba-site.properties"
											INCREMENT_EXCEPTIONS
										fi
									fi
								fi
							fi
						else
							if ! $NORMALIZEauto
							then
								if test "$DEVICEspec" = "identity.resolution.device.domains.normalize"
								then
									echo "   <== 'true' is expected value in /etc/caspida/local/conf/uba-site.properties"
									INCREMENT_EXCEPTIONS
								fi
								if test "$DEVICEspec" = "identity.resolution.device.domains.list"
								then
									if $NORMALIZEdevices
                                                                	then
                                                                        	echo -n "   <== comma-separated list of internal domains is required in /etc/caspida/local/conf/uba-site.properties"
                                                                        	INCREMENT_EXCEPTIONS
                                                                	fi
								fi
							fi
						fi
						echo
					done >> $TMP1file

					if test -s $TMP1file
					then
						NECHO "device settings:" "`head -1 $TMP1file`"; echo
						ALL_BUT_FIRST $TMP1file
					fi
				fi
			fi
		fi
	
	#
	#	the list of 'internal' domains is needed to identify 'external' ...
	#
	
		if $CASPIDA_PROPERTIES_EXIST
        	then
			GET_CASPIDA_PROPERTY identity.resolution.domains.list
			DOMAINSlist=$PROPERTYvalue
	
			if test -z "$DOMAINSlist"
			then
				NECHO "domains.list (users):" "''"; echo
			else
				echo "$DOMAINSlist" | sed -e 's/,/, /g' | fmt -w 60 | sed -e 's/, /,/g' > $TMP1file
	        		NECHO "domains.list:" "`head -1 $TMP1file`"
				if (echo "$DOMAINSlist" | egrep -qe "['\"]"  )
				then
					echo "   <== values must not be quoted"		# easy mistake to make and software doesn't catch it
					INCREMENT_EXCEPTIONS
				else
					echo "   <== should be empty; update 'identity.resolution.domains.list' in $UBA_SITE_PROPERTIES"
					INCREMENT_EXCEPTIONS
				fi
				ALL_BUT_FIRST $TMP1file
			fi
		fi
	
		if $uba24feature
		then
			for FILE in User.json Account.json
			do
				if test -f $CASPIDAdir/conf/attribution/$FILE
				then
					NECHO "$FILE:" ""
					grep '"isComputed"' $CASPIDAdir/conf/attribution/$FILE 2>/dev/null > $TMP1file
					if ! test -s $TMP1file
					then
						echo "   <== isComputed missing from $CASPIDAdir/conf/attribution/$FILE"
						INCREMENT_EXCEPTIONS
					else
						sed -e 's/^[ \t]*//' < $TMP1file > $TMP2file
						head -1 $TMP2file
						ALL_BUT_FIRST $TMP2file
					fi
				fi
			done
		fi

		if test -f $CASPIDAdir/conf/attribution/Account.json
		then
			if egrep -q -e "'adm_%'|'%_adm'|'%_a'|'a_%'" $CASPIDAdir/conf/attribution/Account.json
			then
				NECHO "Account.json:" "'adm_%'|'%_adm'|'%_a'|'a_%'"
				echo "   <== verify strings properly escaped in $CASPIDAdir/conf/attribution/Account.json"
				INCREMENT_EXCEPTIONS
			fi
		fi

		for FILE in /opt/caspida/conf/etl/configuration/EntityValidations.json /etc/caspida/local/conf/etl/configuration/EntityValidations.json
		do
			if test -f $FILE
			then
				case $FILE in
					/opt/*)	LABEL="internalIPRange (d):";;
					*)	LABEL="internalIPRange (l):";;
				esac

				NECHO "$LABEL" "$FILE"
				sed -e 's+^//.*$++' -e 's+[^:]//.*$++' -e '/^.\*/d' < $FILE > $TMP1file
				if test -s $TMP1file
				then
					if ! python -m json.tool < $TMP1file > $TMP2file 2>/dev/null
					then
						echo "   <== invalid JSON syntax; needs to be corrected"
						INCREMENT_EXCEPTIONS
						continue		# can't proceed with this file
					else
						echo "   (okay)"
					fi
				else
					echo "   (empty)"
					continue
				fi
				sed -n '/"internalIPRange"/,/]/p' < $TMP2file | tr -d ' 	][}' > $TMP1file 
				
				if egrep -qe '"internalIPRange":.*,' $TMP1file
				then
					IPlist=""	# is empty
				else
					echo `cat $TMP1file` > $TMP2file 
					IPlist=`cut -d ':' -f 2 < $TMP2file`
				fi
	
				if test -z "$IPlist"
				then
					NECHO "$LABEL" "[]"
					echo " empty   <= update $FILE internal, routable, non-private IP ranges for proper device scoping"
				else
					echo $FILE > $TMP1file
					echo "$IPlist" | sed -e 's/,/, /g' | fmt -w 60 | sed -e 's/, /,/g' >> $TMP1file
					if egrep -qe '/[0-7]"' $TMP1file
					then
						ERROR="   <== contains CIDR suffix lower than 8"
						INCREMENT_EXCEPTIONS
					else
						ERROR=""
					fi
					NECHO "$LABEL" "`head -1 $TMP1file`"; echo "$ERROR"
					ALL_BUT_FIRST $TMP1file
				fi
			fi
		done

		NECHO "etl timezones ..." ""; echo
		INPUT_TZ=""
		OUTPUT_TZ=""
		if $uba32feature
		then
			GET_CASPIDA_PROPERTY parser.global.input_timezone
			INPUT_TZ="$PROPERTYvalue"	
			NECHO "  input (default):" "'$INPUT_TZ'"
			echo -n "   (parser.global.input_timezone setting in /etc/caspida/local/conf/uba-site.properties)"
			if test -f /usr/share/zoneinfo/$INPUT_TZ
			then
				echo
			else
				echo "   <== /usr/share/zoneinfo/$INPUT_TZ not not exist"
				INCREMENT_EXCEPTIONS 
			fi
			if test -z "$INPUT_TZ"
			then
				GLOBALconf="/etc/caspida/local/conf/etl/morphlines/include/global.conf"
				if test -r $GLOBALconf
				then
					INPUT_TZ=`grep input_timezone $GLOBALconf | cut -d '"' -f 2`
					OUTPUT_TZ=`grep "output_timezone :" $GLOBALconf | cut -d '"' -f 2`
					NECHO "  input (default):" "'$INPUT_TZ'"
					echo -n "   input_timezone setting in $GLOBALconf"
					if test -f /usr/share/zoneinfo/$INPUT_TZ
					then
						echo
					else
						echo "   <== /usr/share/zoneinfo/$INPUT_TZ not not exist"
						INCREMENT_EXCEPTIONS 
					fi
				fi
			fi
		fi

		if test -z "$INPUT_TZ"
		then
			GLOBALconf="/opt/caspida/conf/etl/morphlines/include/global.conf"
			if test -r $GLOBALconf
			then
				INPUT_TZ=`grep input_timezone $GLOBALconf | cut -d '"' -f 2`
				NECHO "  input (default):" "'$INPUT_TZ'"
				echo -n "   input_timezone setting in $GLOBALconf"
				if test -f /usr/share/zoneinfo/$INPUT_TZ
				then
					echo
				else
					echo "   <== /usr/share/zoneinfo/$INPUT_TZ not not exist"
					INCREMENT_EXCEPTIONS 
				fi
			fi
		fi
			
		if test -z "$OUTPUT_TZ"
		then
			GLOBALconf="/opt/caspida/conf/etl/morphlines/include/global.conf"
			OUTPUT_TZ=`grep "output_timezone :" $GLOBALconf 2>/dev/null | cut -d '"' -f 2`
		fi

		if ! $uba33feature
		then
			NECHO "  output:" "'$OUTPUT_TZ'   output_timezone setting in $GLOBALconf"
			if test "$OUTPUT_TZ" = "UTC"
			then
				echo
			else
				echo "   <== value must be UTC"
				INCREMENT_EXCEPTIONS
			fi
		fi
		
		if $uba32feature
		then
			CONFdir=/etc/caspida/local/conf
		else
			CONFdir=/opt/caspida/conf
		fi

		if test -f $CONFdir/competitorDomains.txt
		then
        		LINEcount=`wc -l < $CONFdir/competitorDomains.txt`
        		FIRSTcompetitor=`head -1 $CONFdir/competitorDomains.txt`
        		NECHO "competitors:" "$FIRSTcompetitor"
        		if test $LINEcount -gt 1
        		then
                		echo
                		ALL_BUT_FIRST $CONFdir/competitorDomains.txt
        		else
                		if test "$FIRSTcompetitor" = "hp.com"
                		then
                        		echo "   <== verify $CONFdir/competitorDomains.txt contains relevant list of competitor email domains"
                        		INCREMENT_EXCEPTIONS
                		else
                        		echo
                		fi
			fi
		else
			NECHO "competitors:" "not found   <= $CONFdir/competitorDomains.txt with relevant list of competitor email domains might be desired"; echo
        	fi
	fi

	if $DATABASE_HOST
	then
		if $uba32feature
		then
			GET_CASPIDA_PROPERTY ui.internalGeoLocation
			GEOfile="$UBA_SITE_PROPERTIES"
			GEOinfo="$PROPERTYvalue"
		else
			GEOfile="$CASPIDAdir/web/caspida-ui/server/config/uiConfig.js"
 			sed -ne '/DEFAULT_UI_SERVER_CONFIG/,/}/p' < $GEOfile |\
			sed -ne '/"ui.internalGeoLocation"/,/}./p' |\
			tr '\r\n\t' ' ' |\
			sed -e 's/  */ /g' -e 's/.*{/{/' > $TMP1file
			GEOinfo=`cat $TMP1file`
		fi
		if $uba40feature
		then
			psql -d caspidadb -t -c "select value from appconfig where name='settings'" 2>/dev/null >$TMP1file
                	if test -s $TMP1file
                	then
                        	if ! python -m json.tool < $TMP1file >$TMP2file 2>&1
                        	then
                                	NECHO "ui settings:" "`head -1 $TMP2file`"
                                	echo "   <== UI will not start with invalid settings"
                                	INCREMENT_EXCEPTIONS
                                	ALL_BUT_FIRST $TMP2file
                        	fi
                	fi
			GEOui=`psql -d caspidadb -t -c "select value::json->>'ui.internalGeoLocation' as internalGeoLocation from appconfig where name='settings';" 2>/dev/null | sed -e 's/^ //' -e '/^$/d'`
			if ! test -z "$GEOui"
			then
				GEOinfo="$GEOui"
			fi
		fi

		if test -n "$GEOinfo"
		then	
			NECHO "GeoLocation:" "$GEOinfo"
			GEOlat="`echo "$GEOinfo" | tr -d ' "' | sed -e 's/^.*latitude://' -e 's/,.*$//'`"
			GEOlong="`echo "$GEOinfo" | tr -d ' "' | sed -e 's/^.*longitude://' -e 's/,.*$//'`"
			if test "$GEOlat" = "$defLATITUDE" && test "$GEOlong" = "$defLONGITUDE"
			then
				echo "   <= relevant coordinates desired in $GEOfile"
			else
				echo
			fi
		else
			NECHO "GeoLocation:" "'$GEOinfo'"
			echo "   <= verify geo location information has been set" 
		fi

		if test "$TIMEzone" != "Etc/UTC (UTC, +0000)"
		then
			if ! (test -z "$GEOlat" && test -z "$GEOlong")
			then
				GEOlat=`echo "$GEOlat" | sed -e 's/\(^-*[0-9][0-9]*\.[0-9]\).*$/\1/'`
				GEOlong=`echo "$GEOlong" | sed -e 's/\(^-*[0-9][0-9]*\.[0-9]\).*$/\1/'`
				curl --connect-timeout 10 "http://api.geonames.org/timezone?lat=${GEOlat}&lng=${GEOlong}&username=splunkuba" 2>/dev/null > $TMP1file
				GEOtimezone=`grep timezoneId $TMP1file | sed -e 's/^<timezoneId>//' -e 's/<.*$//'`	
				if ! test -z "$GEOtimezone"
				then	
					ERROR=""
					GEOnow=`TZ="$GEOtimezone" date "--date=@$NOWsecs" "+%F_%R:%S"`
					if test "$NOW" != "$GEOnow"
					then
						if test "$TIMEzone" != "Etc/UTC (UTC, +0000)"
						then
							ERROR="   <= verify that server timezone ($TIMEzone) is set correctly; does not match GeoLocation"
						fi
					fi
	
					NECHO "Geo TimeZone:" "'$GEOtimezone'	($GEOlat, $GEOlong)"; echo "$ERROR"
				fi
			fi
		fi

		if $CASPIDA_PROPERTIES_EXIST
		then
			>$TMP1file
			for MONITOR in ubaMonitor.pipeline.connectorETLEvents.pollperiod ubaMonitor.pipeline.connectorETLEvents.constant.polls ubaMonitor.pipeline.connectorETLEvents.increase.polls
			do
				GET_CASPIDA_PROPERTY $MONITOR
				if ! test -z "$PROPERTYvalue"
				then
					echo "${MONITOR}=$PROPERTYvalue" >> $TMP1file
				fi
			done
			if test -s $TMPfile
			then
				echo
				NECHO "ubaMonitor settings:" "`head -1 $TMP1file`"; echo
				ALL_BUT_FIRST $TMP1file
			fi
	
			> $TMP1file
			for ALERT in alert.email.eps.alert.watermark alert.email.eps.alert.enable alert.email.eps.alert.frequency
			do
				GET_CASPIDA_PROPERTY $ALERT
				if ! test -z "$PROPERTYvalue"
                                then
                                        echo "${ALERT}=$PROPERTYvalue" >> $TMP1file
                                fi
			done
                       	if test -s $TMPfile
                        then
				echo
                                NECHO "alert email settings:" "`head -1 $TMP1file`"; echo
                                ALL_BUT_FIRST $TMP1file
                        fi

			GET_CASPIDA_PROPERTY alert.email.lists
			EMAILalert=$PROPERTYvalue
	
			if test -z "$EMAILalert"
			then
				NECHO "alert.email.lists:" ""
				echo "''   <= update $UBA_SITE_PROPERTIES with alert.email.lists for email alerts"
			else
				TRANSLATE "$EMAILalert" | sed -e 's/,/, /g' | fmt -w 60 | sed -e 's/, /,/g' > $TMP1file
	        		NECHO "alert.email *:" "`head -1 $TMP1file`"; echo
				ALL_BUT_FIRST $TMP1file

				NECHO "EMail Output:" ""
               			 psql -d caspidadb -c "select type,creationtime from outputconnectors where type='Email';" 2>/dev/null >$TMP1file
                		if egrep -q '\bEmail\b' $TMP1file
                		then
                        		echo "connector exists"
                		else
                        		echo "connector does not exist"
                		fi
			fi
		fi
	
		echo
		if test -f $CASPIDAdir/web/zplex/web/views/geoMapView.js
		then
			NECHO "Google key:" ""
			if ! grep -q "${reqMAPkey}" $CASPIDAdir/web/zplex/web/views/geoMapView.js 2>/dev/null
			then
				echo "   <== required key '$reqMAPkey' is missing in $CASPIDAdir/web/zplex/web/views/geoMapView.js"; echo
				INCREMENT_EXCEPTIONS
			else
				echo "key is present"
			fi
		fi

		# check external connectivity

		echo
		for URL in https://maps.googleapis.com https://www.virustotal.com 
		do
			FQDN=`basename $URL`
			NECHO "${FQDN}:" ""
			if ! which wget >/dev/null 2>/dev/null
                	then
                        	echo "   <= could not verify server connectivity; required program 'wget' is not available"
                	else
                        	if $TIMEOUT wget --no-check-certificate --spider --tries=1 --timeout=4 "$URL" > $TMP1file 2>&1
                        	then
                                	echo -n "server connectivity verified"
					if test "$FQDN" = "geolite.maxmind.com"
					then
						echo
					else
                                		echo "; client should be checked"
					fi
                        	else
                                	echo -n "   <= server could not connect to '$URL'"
					if test "$FQDN" = "geolite.maxmind.com" -o "$FQDN" = "www.virustotal.com"
					then
                                		echo "; check firewall"
					else
						echo
					fi
                        	fi
                	fi
		done
	fi

	#if $uba24feature
	#then
		#if $SPARK_MASTER
		#then
			#if test -d /opt/caspida/conf/spark
			#then
				#cd /opt/caspida/conf/spark
				#md5sum * > $TMP1file
				#if test -d /var/vcap/packages/spark/conf
				#then
					#cd /var/vcap/packages/spark/conf
					#md5sum -c $TMP1file 2>/dev/null | grep "FAILED" > $TMP2file
					#if test -s $TMP2file
					#then
						#NECHO "spark conf:" "`head -1 $TMP2file`"
						#echo "   <== copy /opt/caspida/conf/spark/* to /var/vcap/packages/spark/conf after stopping spark"
						#ALL_BUT_FIRST $TMP2file
						#INCREMENT_EXCEPTIONS
					#fi
				#fi
			#fi
		#fi
	#fi

	if ! $LEADERnode
	then
		echo
		NECHO "compare files:" ""
		if test -s $MD5compare
		then
			md5sum -c $MD5compare 2>/dev/null |\
			grep -v 'OK$' > $TMP1file
			if test -s $TMP1file
			then
				if $uba32feature
				then
					SUGGEST="(sync-cluster advised)"
				else
					SUGGEST=""
				fi
				sed -e "s/$/   <== re-copy file from $FIRSTnode $SUGGEST/"  < $TMP1file > $TMP2file
				echo "`head -1 $TMP2file`"
				FAILEDcount=`wc -l < $TMP2file`
				INCREMENT_EXCEPTIONS $FAILEDcount
				ALL_BUT_FIRST $TMP2file
			else
				echo "   (files comparison successful)"
			fi
		else
			echo "   <== could not compare files from leader"
			INCREMENT_EXCEPTIONS	
		fi
	fi

	if test -f $CASPIDAdir/conf/version.properties
	then
		NEWERref="$CASPIDAdir/conf/version.properties"
		if test -d "$CASPIDAdir/conf/attribution"
		then
			if test "$CASPIDAdir/conf/attribution" -nt "$CASPIDAdir/conf/version.properties"
			then
				NEWERref="$CASPIDAdir/conf/attribution"
			fi
		fi
		CONFdirs="$CASPIDAdir/conf"
		if test -d /etc/caspida/local/conf/
		then
			CONFdirs="$CONFdirs /etc/caspida/local/conf"
		fi
		
		find $CONFdirs -type f -newer $NEWERref -exec ls -l {} \; |\
		sort -k 9 | sed -e 's/\(^.* \)\([JFMASOND].*$\)/\2/' |\
		egrep -iv '[_.]bak.*$|[_.-]orig$|/conf/analytics/|/conf/analytics.stage/|/conf/deployment/templates/' > $TMP1file		# modified 'conf' files
		if test -s $TMP1file
                then
                        echo
			NEWERdate=`stat --format='%Y' $NEWERref`
			SINCE=`date --date="@${NEWERdate}" '+%b %e %H:%M'`
                        NECHO "modified conf files:" "since $SINCE $NEWERref ..."; echo
			sort $TMP1file |  uniq -cw 12 | egrep -e '^ +1 ' | sed -e "s/^ \{1,\}1 /$INDENT/" 

			egrep -i '\.json$' $TMP1file |\
			sed -e 's+^[^/]*++' |\
			while read FILE
			do
				if sed -e 's+^//.*$++' -e 's+:////*+://+' -e 's+[^:]//.*$++'  -e '/^\s*\*/d' -e '/^\s*\/\*/d' < $FILE | python -m json.tool > /dev/null 2>&1 
				then
					continue
				else
					echo "$FILE   <== incorrect JSON syntax"
				fi
			done > $TMP2file
			if test -s $TMP2file
			then
				echo
				NECHO "failed JSON syntax:" "`head -1 $TMP2file`"; echo
				ALL_BUT_FIRST $TMP2file
				INCREMENT_EXCEPTIONS
			fi
		fi
	fi
	
	#
	#	if this is multi-node cluster, most of the same requirements should be applied to other nodes
	#
	#	While there is no attempt to use 'blueprint' to reduce software requirements on member nodes,
	#	second node has specific checks
	#
	
	if $LEADERnode
	then

		# FIRSTnode is responsible for checking other nodes
                if test -s "$MEMBERwait"
                then
			echo
			ITERcount=0
			while ps `cat $BGjobs` > $TMP1file 2>/dev/null
			do
				if test $ITERcount -lt 24
                                then
                                        JOBcount=`wc -l < $TMP1file`
                                        JOBcount=`expr $JOBcount - 1`
                                        NECHO "waiting:" "$JOBcount other check scripts still executing"; echo
                                        ITERcount=`expr $ITERcount + 1`
                                        sleep 30                                    # wait for all members to complete
                                else
					if test "$JOBcount" = "1"
					then
						ARROW="<="
					else
						ARROW="<=="
						INCREMENT_EXCEPTIONS
					fi
                                        NECHO "gave up:" "timeout   $ARROW $JOBcount other check scripts were executing"; echo
                                        break
                                fi
                        done
				echo
			
			OScwleaderrelease=`cat $OSfile`
			UBAleader=`cat $UBAfile`
			CONTENTleader=`cat $CONTENTfile`
			
                        while read MEMBERoutfile
                        do
                                MEMBERname=`echo $MEMBERoutfile | sed -e "s+^$MEMBERout++" -e 's/.txt$//'`

                                echo "Checking $MEMBERname ..."

                                if ! test -s $MEMBERoutfile
                                then
                                        NECHO "missing output:" "$MEMBERoutfile   <== missing or empty"; echo
                                        INCREMENT_EXCEPTIONS 1 
                                        continue
                                fi

				if egrep -e "^  *current os:" $MEMBERoutfile > $TMP1file
				then
					if ! grep -q "$OSleader" $TMP1file
					then
						sed -i -e '/^  *current os:/s/\r*$/   <== does not match leader OS/' $MEMBERoutfile	
						INCREMENT_EXCEPTIONS
					fi
				fi

				if egrep -e "^  *current uba:" $MEMBERoutfile > $TMP1file
				then
					if ! grep -q "$UBAleader" $TMP1file
					then
						sed -i -e '/^  *current uba:/s/\r*$/   <== does not match leader UBA/' $MEMBERoutfile	
						INCREMENT_EXCEPTIONS
					fi
				fi

				if egrep -e "^  *last content:" $MEMBERoutfile > $TMP1file
				then
					if ! grep -q "$CONTENTleader" $TMP1file
					then
						sed -i -e '/^  *last content:/s/\r*$/   <== does not match leader CONTENT/' $MEMBERoutfile	
						INCREMENT_EXCEPTIONS
					fi
				fi

				if ! grep -q '(check completed)' $MEMBERoutfile
				then
					NECHO "partial output:" "review $MEMBERname   <== check did not complete"; echo
					INCREMENT_EXCEPTIONS
				fi

                                head -n -4  $MEMBERoutfile | tr -d "\r"              # all but the last few lines

				tail -4 $MEMBERoutfile | grep exceptions > $TMP2file

				echo
                                NODEexceptions=`sed -e 's/^.*remediate //' -e 's/ exceptions.*$//' < $TMP2file`
				echo

				NECHO "$MEMBERname:" "'$NODEexceptions' exceptions"; echo
				
                                if expr "$NODEexceptions" + 0 > /dev/null 2>&1
                                then
                                        INCREMENT_EXCEPTIONS $NODEexceptions
                                fi
                        done < $MEMBERwait
                fi
	else
		echo; NECHO "${THISnode}:" "(check completed)"; echo; echo; echo; echo; echo 
	fi
        echo

	if $LEADERnode
	then
		if test "$LISTENpid" != ""
		then
			if ps $LISTENpid >/dev/null 2>&1
			then
				for I in $NODElist
				do
					killall $LISTENprog >/dev/null 2>&1     # kill all of the remaining listeners
				done
				sleep 2
				kill $LISTENpid >/dev/null 2>&1                # should be dead already
			fi
		fi
	fi

        if test -s $EXCfile
        then
                EXCEPTIONS=`cat $EXCfile`
        else
                EXCEPTIONS=0
        fi

	echo
        if test "$EXCEPTIONS" != "0"
        then
                echo "Please review and remediate $EXCEPTIONS exceptions"
        else
                echo "No exceptions detected"
        fi
	echo
	echo
) | tee -a $OUTfile

if $LEADERnode
then
	egrep -e "   <= |^Checking" $OUTfile > $OBLfile
	egrep -e " <== |^Checking" $OUTfile > $EXLfile
	if test -s $EXLfile
	then
		echo
                NECHO "note:" "'<='  is an observation that may never be an exception; could be due to customization"; echo
                NECHO "" "'<==' is an exception that must be reviewed; it may not be relevant in all deployments"; echo
                NECHO "" "'*'   PII has been obfuscated"; echo
		echo
		echo
		echo
		NECHO "exception summary:" "`head -1 $EXLfile`"; echo
		ALL_BUT_FIRST $EXLfile
		echo
		echo
		echo
	fi | tee -a $OUTfile

	if test -s $OBLfile
	then
		echo
		echo
		NECHO "observation summary:" "`head -1 $OBLfile`"; echo
		ALL_BUT_FIRST $OBLfile
		echo
		echo
	fi | tee -a $OUTfile

	(
		echo
		OSrelease="`cat $OSfile`"
		NECHO "current os:" "'$OSrelease'"; echo
        	UBAversion=`grep release-version $CASPIDAdir/conf/version.properties 2>/dev/null | cut -d "=" -f 2`
		NECHO "current uba:" "'$UBAversion'"; echo
        	CONTENT=`egrep -e '"version"' /opt/caspida/content/*/content-descriptor.json 2>/dev/null | sort | tail -1 | cut -d '"' -f 4`
                NECHO "current content:" "'$CONTENT'"; echo
		DATABASE_HOST_list=`cat $DBfile`
		if test "$DATABASE_HOST_list" = ""
		then
			DATABASE_HOST_list="localhost"
		fi
		DBversion=`ssh $DATABASE_HOST_list "psql -d caspidadb -t -c 'select currentversion from dbinfo;' 2>/dev/null" 2>/dev/null | tr -d ' '`
		NECHO "current db:" "'$DBversion'"; echo
		echo
                NECHO "${BASEname}:" "'$VERSion'"; echo
                echo
	) | tee -a $OUTfile

        STARTsecs=$NOWsecs                              # that was then
        NOW=`date '+%s'`                                # this is now

	ELAPSEDsecs=`expr $NOW - $STARTsecs`
	ELAPSEDmins=`expr $ELAPSEDsecs / 60`
	ELAPSEDsecs=`expr $ELAPSEDsecs \% 60`

        (echo "${PROGname}: duration $ELAPSEDmins minutes and $ELAPSEDsecs seconds"; echo) | tee -a $OUTfile

	if $CASPIDAuser
	then
		if test -d /var/log/caspida
		then
			SAVEdir=/var/log/caspida/check
		else
			SAVEdir=$CASPIDAhome/check
		fi
	else
		SAVEdir=/tmp
	fi

 	case $BASEname in
 		uba_health_check)	SAVEname="uhc";;
 		uba_pre_check)		SAVEname="upc";;
 		*)			SAVEname=$BASEname;;
 	esac

	if mkdir -p $SAVEdir >/dev/null 2>&1
	then
		SAVEfile="${SAVEname}_${THISnode}_${OUTnow}_v${VERSion}.txt"
		if cp $OUTfile $SAVEdir/$SAVEfile 2>/dev/null
		then
			echo "${PROGname}: output saved as    $SAVEdir/$SAVEfile" | tee -a $OUTfile
			if ln -sf $SAVEdir/$SAVEfile $SAVEdir/${BASEname}.txt 2>/dev/null
        		then
                		echo "${PROGname}: output linked to $SAVEdir/${BASEname}.txt"
			else
				echo "${PROGname}: could not link to $SAVEdir/${BASEname}.txt"
        		fi
		else
			echo "${PROGname}: could not copy output to $SAVEdir/$SAVEfile"
		fi
	else
		echo "${PROGname}: could not create $SAVEdir"
		echo "${PROGname}: could not save output"
	fi
	
	echo
	echo
fi 

if ! $LEADERnode
then	
	cp $OUTfile $LASTfile		# save output on member nodes
	sleep 5
	:
fi

